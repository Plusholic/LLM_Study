from typing import Set
import time
from backend.core import run_llm
import streamlit as st
from streamlit_chat import message


# >> streamlit run app.py
# ì—¬ê¸°ì— page ì¶”ê°€í•˜ê³ ì‹¶ìŒ. -> done
# sidebarë¡œ Notion, Papers ë“± ì„ íƒí•  ìˆ˜ ìˆê²Œ ë§Œë“¤ê¸° -> done
# Notion API ë¡œ ëŒ€ë‹µí•˜ëŠ” ì±—ë´‡ ë§Œë“¤ê¸° -> done
# Excel Baseë¡œ ëŒ€ë‹µí•˜ëŠ” ì±—ë´‡ ë§Œë“¤ê¸° 
# PPT Baseë¡œ ëŒ€ë‹µí•˜ëŠ” ì±—ë´‡ ë§Œë“¤ê¸° 
# ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê²Œ í•˜ë ¤ë©´, run_llm ì—ì„œ queryì— ë­”ê°€ë¥¼ ì¶”ê°€í•´ì¤˜ì•¼ í•˜ë‚˜..?
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€. -> done

def create_sources_string(source_urls: Set[str]) -> str:
    if not source_urls:
        return ""
    sources_list = list(source_urls)
    sources_list.sort()
    sources_string = "\n\n reference sources:\n"
    # sources_page = "page:\n"
    
    for i, source in enumerate(sources_list):
        sources_string += f"{i+1}. Path : {source[0]}\n Page : {source[1]}\n"
        # sources_string += "{num} [link]({link}) \n Page : {page}\n".format(num = i+1, link=source[0], page=source[1])
        # sources_string += "[link]({link})".format(link=source[0])
        
    return sources_string
    # return source[0], source[1]


st.header("Plusholic Knowledge Database")
st.markdown("---")

if "user_prompt_history" not in st.session_state:
    st.session_state["user_prompt_history"] = []

if "chat_answers_history" not in st.session_state:
    st.session_state["chat_answers_history"] = []
    
if "db_type" not in st.session_state:
    st.session_state.db_type = "PDF papers"  # ì´ˆê¸° ë°ì´í„°ë² ì´ìŠ¤ ìœ í˜•
if "k" not in st.session_state:
    st.session_state.k = 4  # ì´ˆê¸° ìŠ¬ë¼ì´ë” ê°’ (k)
if "threshold" not in st.session_state:
    st.session_state.threshold = 0.5  # ì´ˆê¸° ìŠ¬ë¼ì´ë” ê°’ (threshold)



with st.sidebar:
    st.title('ğŸ¤—ğŸ’¬ Plusholic Data Chat')
    
    # st.checkbox("Disable selectbox widget", key="disabled")
    db_type = st.radio(
                "Select Database Type ğŸ‘‰",
                key="visibility",
                options=["PDF papers", "Notion", "Excel", "PPT"],
                )
    if db_type == "PDF papers":
        # k = st.slider('How many references', 1, 20, 4)
        # threshold = st.slider('How much scores', value=0.5, min_value=0.0, max_value=1.0, step=0.05)
        st.session_state.k = st.slider('How many references', 1, 20, 4)
        st.session_state.threshold = st.slider('How much scores', value=0.5, min_value=0.0, max_value=1.0, step=0.05)


       # ì±„íŒ… ë‚´ì—­ ìˆìœ¼ë©´ ë³´ì´ê²Œ í•´ì•¼í•¨
       
    # Database Typeì„ PDF Papersë¡œ í–ˆë‹¤ë©´,
if db_type == "PDF papers":
    source_dict = {}
    st.title("Chat With Graph Neural Network References")
    
    # k = st.slider('How many references', 1, 50, 1)
    if "messages" not in st.session_state.keys(): # Initialize the chat message history
        st.session_state.messages = []
        st.session_state.messages.append({"role": "assistant", "content": "Ask me a question about Graph Neural Network!"})
        
        
        with st.chat_message(st.session_state.messages[0]["role"]):
            # st.write(st.session_state.messages[0]["content"])
            st.markdown(st.session_state.messages[0]["content"])
            # ì—¬ê¸°ì— ì¶”ê°€
            
    # ë§Œì•½ í”„ë¡¬í”„íŠ¸ê°€ ë“¤ì–´ì˜¤ë©´ ë‹¤ìŒ ë‚´ìš©ì„ ì‹¤í–‰.
    if prompt := st.chat_input("Enter your question"):

        # ì´ì „ ë©”ì„¸ì§€ë“¤ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])
                # st.markdown(message["content"])

        # ë‚´ê°€ ì…ë ¥í•œ ë©”ì„¸ì§€ê°€ ë¨¼ì € ë‚˜ì˜¤ë„ë¡ ì„¸íŒ…
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message(st.session_state.messages[-1]["role"]):
            # st.write(st.session_state.messages[-1]["content"])
            st.markdown(st.session_state.messages[-1]["content"])
        # ë‹µë³€ì„ ìƒì„±í• ë•Œ ìŠ¤í”¼ë„ˆê°€ ëŒì•„ê°€ë„ë¡ ì„¸íŒ…
        with st.spinner("Thinking..."):
            try:
                generated_response = run_llm(query=prompt,
                                             k=st.session_state.k,
                                             threshold=st.session_state.threshold)
            except:
                generated_response = {"source_documents" : "None",
                                      "answer" : "I dont know"}
            
        # ë©”ì„¸ì§€ê°€ ì±—ë´‡ìœ¼ë¡œë¶€í„° ì˜¨ê²Œ ì•„ë‹ˆë¼ë©´ -> ìœ ì €ë¡œë¶€í„° ì˜¨ê±°ë¼ë©´, ì‘ë‹µì„ ìƒì„±í•¨
        if st.session_state.messages[-1]["role"] != "assistant":
            
            # with chat_message() -> ì±„íŒ…ì°½ì—ì„œ streamlitì˜ ë´‡ í”„ë¡œí•„ê°™ì€ê±° ë‚˜ì˜¤ê²Œí•¨
            with st.chat_message("assistant"):
                    # ë‹µë³€ ìƒì„±
                    try:
                        sources = set([(doc.metadata['source'], doc.metadata['page']) for doc in generated_response["source_documents"]])
                    except:
                        sources = [("Not exists", "")]
                    # formatted_response = (f"{generated_response['result']} \n {create_sources_string(sources)}")
                    # formatted_response = (f"{generated_response['answer']} \n {create_sources_string(sources)}")

                    # formatted response ë¼ëŠ” ë§ë„ í•„ìš” ì—†ëŠ”ê±°ê°™ì€ë°? ì´ê±° ë°”ê¾¸ì
                    formatted_response = (f"{generated_response['answer']}")
                    
                    # st.write(formatted_response)
                    
                    full_response = ""
                    message_placeholder = st.empty()
                    
                    # ë‹µë³€ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° -> OpenAIì—ì„œ streaming=True ì¼ ë•Œ ì–´ë–»ê²Œ ì“°ëŠ”ì§€ ë‚˜ì¤‘ì— ì¶”ê°€
                    for idx, chunk in enumerate(formatted_response.split()):  
                        full_response += chunk + " "
                        time.sleep(0.05)
                        message_placeholder.markdown(full_response + "â–Œ")
                        prev_chunk = chunk
                        
                    full_response += create_sources_string(sources)
                    # full_response += formatted_response
                    # source_path, source_page = create_sources_string(sources)
                    # source_dict[source_page].append(source_page)
                    
                    # ì´ê±° ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë°”ê¾¸ì
                    from streamlit_file_browser import st_file_browser
                    # print(create_sources_string(sources)[4:-6])
                    # ì´ê±° ì–´ë–»ê²Œ ìˆ˜ì •í•˜ì§€..
                    event = st_file_browser(
                        key='S',
                        path='/Users/jeonjunhwi/á„†á…®á†«á„‰á…¥/Projects/GNN_Covid/refference/GNNë…¼ë¬¸/Data-Driven',
                        show_choose_file=True,
                        show_download_file=False)
                    st.write(event)
                    
                    message_placeholder.markdown(full_response)
                    # st.markdown(create_sources_string(sources), unsafe_allow_html=True)
                    st.session_state.messages.append({"role": "assistant", "content": full_response}) # Add response to message history


        # session_stateì— ì§ˆë¬¸ê³¼ ë‹µë³€ ì €ì¥
        st.session_state["user_prompt_history"].append(prompt)
        st.session_state["chat_answers_history"].append(formatted_response)

elif db_type == "Notion":
    
    st.title("Chat With Graph Neural Network References")
    st.write('You Must select PDF papers')
    st.write('You current selected:', db_type)
    
    link='check out this [link](https://retailscope.africa/)'
    st.markdown(link,unsafe_allow_html=True)
    
elif db_type == "Excel":

    st.title("Chat With Graph Neural Network References")
    st.write('You Must select PDF papers')
    st.write('You current selected:', db_type)
    
elif db_type == "PPT":

    st.title("Chat With Graph Neural Network References")
    st.write('You Must select PDF papers')
    st.write('You current selected:', db_type)