{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 5.08k/5.08k [00:00<00:00, 10.7MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 712M/712M [01:45<00:00, 6.73MB/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 625/625 [00:00<00:00, 388kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 127kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 1.23MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.96M/1.96M [00:01<00:00, 1.94MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 환경변수 준비\n",
    "\n",
    "# pip install torch transformers python-pptx Pillow\n",
    "# pip install pypdf\n",
    "import os\n",
    "import openai\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "import logging\n",
    "import sys\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "\n",
    "from llama_index.node_parser.extractors.metadata_extractors import (\n",
    "    MetadataExtractor,\n",
    "    EntityExtractor,\n",
    ")\n",
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "\n",
    "entity_extractor = EntityExtractor(\n",
    "    prediction_threshold=0.5,\n",
    "    label_entities=False,  # include the entity label in the metadata (can be erroneous)\n",
    "    device=\"cpu\",  # set to \"cuda\" if you have a GPU\n",
    ")\n",
    "\n",
    "metadata_extractor = MetadataExtractor(extractors=[entity_extractor])\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(metadata_extractor=metadata_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x587b8 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x58cb2 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x58e64 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x59116 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x593cd for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5979f for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x59ad2 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x59f54 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5a137 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5a2ea for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5a475 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5a618 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5a803 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5ab12 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5adfc for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5b2c6 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5b6c1 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5ba97 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5bd83 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5c042 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5c30a for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5c5e4 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5c8a3 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5cb98 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5ce60 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5d11f for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x5d360 for key /MediaBox\n",
      "/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "Invalid parent xref., rebuild xref\n",
      "incorrect startxref pointer(1)\n"
     ]
    }
   ],
   "source": [
    "# 로그 레벨 설정\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)\n",
    "# 문서 로드(data 폴더에 문서를 넣어 두세요)\n",
    "documents = SimpleDirectoryReader(\"/Users/jeonjunhwi/문서/Projects/GNN_Covid/refference/GNN논문\").load_data()\n",
    "# \n",
    "# 인덱스 생성\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'The impact of social isolation and changes in work patterns on ongoing thought during the first covid-19 lockdown in the united kingdom.pdf', 'entities': {'United Kingdom'}}\n",
      "{'page_label': '78', 'file_name': 'Spatiotemporal dynamic network for regional maritime vessel flow prediction amid COVID-19.pdf', 'entities': {'China cSchool of International Trade and Economics', 'Changchun Yangc', 'Min Zuoa', 'University of International Business and Economics', 'China', 'Lipo Moa', 'Xin Lia', 'Chuan Zhaoa', 'Beijing Technology and Business University', 'Beijing', 'Elsevier Ltd.'}}\n",
      "{'page_label': '928', 'file_name': 'Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting.pdf'}\n",
      "{'page_label': '3', 'file_name': 'EvolveGCN, Evolving Graph Convolutional Networks for Dynamic Graphs.pdf'}\n",
      "{'page_label': '13', 'file_name': 'Forecasting of COVID19 per regions using ARIMA models and polynomial functions.pdf', 'entities': {'Applied Soft Computing Journal', 'Afghanistan', 'Oman', 'Malaysia', 'Uzbekistan', 'Kuwait', 'H. Fujita', 'Iran', 'Armenia', 'T. Hayashi', 'A. Hernandez-Matamoros', 'Iraq'}}\n"
     ]
    }
   ],
   "source": [
    "# from random import random\n",
    "import random\n",
    "samples = random.sample(nodes, 5)\n",
    "for node in samples:\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex(nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "I'm sorry, but I can't answer that question based on the provided context information.\n",
      "source_nodes:\n",
      "[NodeWithScore(node=TextNode(id_='00466d00-c4b0-42fc-942e-71d533ef5ced', embedding=None, metadata={'page_label': '3', 'file_name': 'stan spatio temporal attention network for pandemic prediction using real world evidence.pdf', 'entities': {'GRU', 'GAT'}}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='47292acd-77ba-42d1-aaa0-a4c754d328ac', node_type=None, metadata={'page_label': '3', 'file_name': 'stan spatio temporal attention network for pandemic prediction using real world evidence.pdf'}, hash='96665c1d469ef9870fc334f13869d1690c9256cbaec9a0541a4942b8d510eaa1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5a6d8a0b-3fc3-4559-a473-bd61e0ed2e7d', node_type=None, metadata={'page_label': '3', 'file_name': 'stan spatio temporal attention network for pandemic prediction using real world evidence.pdf'}, hash='31a2484d4292b1a65ffea11f4385a2adafcf2db029690baa9da1a9e90ceb0947')}, hash='848b7be959cf01845c0644554620cd7d910e05a3eb2c7c00d6d8d9d853d0b8f6', text='Wk\\na2R1/C22Fzrepresents the attention computation ma-\\ntrix for the k-th head, and /C1j/C1ðÞdenotes the vector concatenation.r\\nis the nonlinear activation function, and here we use the leaky recti-fied linear function (LeakyReLU):\\nrxðÞ ¼x;if x/C210\\n0:01x;otherwise(\\nwhich is the same as the original GAT model.15\\nNext, we use the softmax function to calculate the attention\\nscore:\\nak\\nij¼softmax ek\\nij/C16/C17\\n¼exp ek\\nij/C16/C17\\nPN\\nn¼1expek\\nin/C0/C1\\nFigure 1.The STAN model: We construct the location graph using location-wise dynamic and static features as nodes and geographic proximity as edges.The\\ngraph is fed into graph attention network to extract spatio-temporal features and learn the graph embedding for the target location.Then the graph em bedding is\\nfed into the GRU to extract temporal relationships.The hidden states of GRU will be used to predict future number of infected and recovered cases.We us e an ad-\\nditional transmission dynamics loss based on pandemic transmission dynamics to optimize the model.Journal of the American Medical Informatics Association , 2021, Vol.28, No.4 735Downloaded from https://academic.oup.com/jamia/article/28/4/733/6118380 by guest on 12 October 2022', start_char_idx=None, end_char_idx=None, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8573956275054573), NodeWithScore(node=TextNode(id_='970f142a-7ef1-4cde-8a4f-37f4ef69ce5c', embedding=None, metadata={'page_label': '6', 'file_name': 'EXPLAINABLE GRAPH PYRAMID AUTOFORMER FOR LONG-TERM TRAFFIC FORECASTING.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0e250dda-625b-45ec-80f8-7e8c3fcee71c', node_type=None, metadata={'page_label': '6', 'file_name': 'EXPLAINABLE GRAPH PYRAMID AUTOFORMER FOR LONG-TERM TRAFFIC FORECASTING.pdf'}, hash='c49c22cd727d8ec8a1bea6ee26717b40be7f3fe3145ef53b6b58b10baa31db78')}, hash='c49c22cd727d8ec8a1bea6ee26717b40be7f3fe3145ef53b6b58b10baa31db78', text='arXiv Template A P REPRINT\\nFigure 2: Framework of graph pyramid autoformer.\\nneural network model parameterized by θandexpis the exponential operation to ensure that the weights are positive.\\nThe hidden feature of the predicted trafﬁc data is calculated by vt+j=∑\\ni,jwi,jχ′gci,pj.\\n4 Explanation extraction\\nThe attention mechanism is a popular approach to explain the deep learning model predictions. In a traditional self-\\nattention layer, the output is considered as a linear combination of input features. To increase the explainability of\\nthe model, we develop two variants of the single-headed attention mechanism by either removing the value mapping\\nor removing values mapping and query mapping simultaneously, as shown in Figure 3. In the ﬁrst variant, we deﬁne\\nvalue mapping as identical mapping FV(x) =x. In the second variant, we adopt the same function as our query\\nmapping and key mapping FQ=FKwhile using the identical mapping as the value mapping at the same time. When\\nusing these two attention mechanisms, even though we stack multiple graph attention layers and temporal attention\\nlayers, the ﬁnal output becomes a linear combination of the original input. In our model, we tested three different\\nkinds of attention mechanism and adopted the most explainable one.\\nFigure 3: Description of different attention mechanisms.\\nRecall that in the graph attention layer we deﬁne αklas the importance of node lto nodek, and in the temporal\\nattention layer we deﬁne Sqas the attention score with respect to time delay τq. We deﬁne GCas the operation of the\\ngraph attention layer and deﬁne TCas the operation of the temporal attention layer.\\nThe input of each attention layer is deﬁned as I∈RN×T1×D, and the output is deﬁned as O∈RN×T2×D, whereN\\nis the number of nodes, T1,T2is the number of time steps, and Dis the number of features. Hence we can derive the\\nrelationship between the input and the output of multiple graph attention layers as follows:\\nOk,:,:=GC(∑\\nl∈NkαklIl,:,:).\\nSimilarly, the relationship between the input and the output of multiple temporal attention layers is\\nO:,p,:=TC(∑\\nqSq∗I:,p+τq,:).\\n6', start_char_idx=None, end_char_idx=None, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8533970001533386)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type set is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_nodes:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39msource_nodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# # 인덱스 저장\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/storage/storage_context.py:116\u001b[0m, in \u001b[0;36mStorageContext.persist\u001b[0;34m(self, persist_dir, docstore_fname, index_store_fname, vector_store_fname, graph_store_fname, fs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     vector_store_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path(persist_dir) \u001b[39m/\u001b[39m vector_store_fname)\n\u001b[1;32m    114\u001b[0m     graph_store_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path(persist_dir) \u001b[39m/\u001b[39m graph_store_fname)\n\u001b[0;32m--> 116\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocstore\u001b[39m.\u001b[39;49mpersist(persist_path\u001b[39m=\u001b[39;49mdocstore_path, fs\u001b[39m=\u001b[39;49mfs)\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_store\u001b[39m.\u001b[39mpersist(persist_path\u001b[39m=\u001b[39mindex_store_path, fs\u001b[39m=\u001b[39mfs)\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_store\u001b[39m.\u001b[39mpersist(persist_path\u001b[39m=\u001b[39mvector_store_path, fs\u001b[39m=\u001b[39mfs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/storage/docstore/simple_docstore.py:85\u001b[0m, in \u001b[0;36mSimpleDocumentStore.persist\u001b[0;34m(self, persist_path, fs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Persist the store.\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kvstore, BaseInMemoryKVStore):\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_kvstore\u001b[39m.\u001b[39;49mpersist(persist_path, fs\u001b[39m=\u001b[39;49mfs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/storage/kvstore/simple_kvstore.py:66\u001b[0m, in \u001b[0;36mSimpleKVStore.persist\u001b[0;34m(self, persist_path, fs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     fs\u001b[39m.\u001b[39mmakedirs(dirpath)\n\u001b[1;32m     65\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39mopen(persist_path, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 66\u001b[0m     f\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39;49mdumps(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m skipkeys \u001b[39mand\u001b[39;00m ensure_ascii \u001b[39mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[39mand\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m indent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m separators \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort_keys \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_encoder\u001b[39m.\u001b[39;49mencode(obj)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type set is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# 인덱스 로드\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "# index = load_index_from_storage(storage_context)\n",
    "\n",
    "# 쿼리 엔진 생성\n",
    "query_engine = index.as_query_engine(\n",
    ")\n",
    "\n",
    "# 질의응답\n",
    "response = query_engine.query(\"Please to explain Graph Attention Network(GAT)\")\n",
    "# streaming_response.print_response_stream()\n",
    "print(f\"response:\\n{response}\")\n",
    "print(f\"source_nodes:\\n{response.source_nodes}\")\n",
    "# # 인덱스 저장\n",
    "index.storage_context.persist()\n",
    "\n",
    "# docid로 원래 데이터 찾는법..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleNodeParser\n\u001b[1;32m      3\u001b[0m parser \u001b[38;5;241m=\u001b[39m SimpleNodeParser()\n\u001b[0;32m----> 4\u001b[0m nodes \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mget_nodes_from_documents(\u001b[43mdocuments\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser.\n",
    "\n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import QueryResponseEvaluator\n",
    "\n",
    "# # build service context\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "# # build index\n",
    "# ...\n",
    "\n",
    "# define evaluator\n",
    "evaluator = QueryResponseEvaluator(service_context=service_context)\n",
    "\n",
    "# query index\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\n",
    "eval_result = evaluator.evaluate_source_nodes(response)\n",
    "print(str(eval_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index\n",
    "llama_index.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "from backend.core import run_llm\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "st.header(\"PDF retreival Question Answering\")\n",
    "\n",
    "\n",
    "prompt = st.text_input(\"Prompt\", placeholder=\"Enter your prompt here..\")\n",
    "\n",
    "if \"user_prompt_history\" not in st.session_state:\n",
    "    st.session_state[\"user_prompt_history\"] = []\n",
    "\n",
    "if \"chat_answers_history\" not in st.session_state:\n",
    "    st.session_state[\"chat_answers_history\"] = []\n",
    "\n",
    "\n",
    "def create_sources_string(source_urls: Set[str]) -> str:\n",
    "    if not source_urls:\n",
    "        return \"\"\n",
    "    sources_list = list(source_urls)\n",
    "    sources_list.sort()\n",
    "    sources_string = \"sources:\\n\"\n",
    "    for i, source in enumerate(sources_list):\n",
    "        sources_string += f\"{i+1}. {source}\\n\"\n",
    "    return sources_string\n",
    "\n",
    "\n",
    "if prompt:\n",
    "    with st.spinner(\"Generating response..\"): # 로딩중에 돌아가는 바퀴같은거\n",
    "        generated_response = run_llm(query=prompt) \n",
    "        sources = set(\n",
    "            [doc.metadata[\"source\"] for doc in generated_response[\"source_documents\"]]\n",
    "        )\n",
    "\n",
    "        formatted_response = (\n",
    "            f\"{generated_response['result']} \\n\\n {create_sources_string(sources)}\"\n",
    "        )\n",
    "\n",
    "        st.session_state[\"user_prompt_history\"].append(prompt)\n",
    "        st.session_state[\"chat_answers_history\"].append(formatted_response)\n",
    "\n",
    "if st.session_state[\"chat_answers_history\"]:\n",
    "    for generated_response, user_query in zip(\n",
    "        st.session_state[\"chat_answers_history\"],\n",
    "        st.session_state[\"user_prompt_history\"],\n",
    "    ):\n",
    "        message(user_query, is_user=True)\n",
    "        message(generated_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('LLM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a3a50cfa768e55079b834244adbc23d62a4b24bbd2f8b443e30bd8c79d3f7ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
