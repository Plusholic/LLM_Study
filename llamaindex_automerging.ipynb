{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_hub.file.pdf.base import PDFReader\n",
    "# from llama_hub.file.pymu_pdf.base import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyMuPDFReader()\n",
    "# loader = PDFReader()\n",
    "# docs0 = loader.load_data(file=Path(\"./data/llama2.pdf\"))\n",
    "# docs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"./data/\", \n",
    "    required_exts=[\".pdf\"],\n",
    "    # file_extractor={\".md\": MarkdownDocsReader()},\n",
    "    recursive=True\n",
    ")\n",
    "docs0 = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "docs = [Document(text=doc_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.node_parser import (\n",
    "    HierarchicalNodeParser,\n",
    "    SentenceSplitter,\n",
    ")\n",
    "node_parser = HierarchicalNodeParser.from_defaults()\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.node_parser import get_leaf_nodes, get_root_nodes\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "len(leaf_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_nodes = get_root_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define storage context\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "# insert nodes into docstore\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "# define storage context (will include vector store by default too)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    ")\n",
    "## Load index into vector index\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "base_index = VectorStoreIndex(\n",
    "    leaf_nodes,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 5 nodes into parent node.\n",
      "> Parent node id: e3e542f0-e6ed-4451-aee7-7c01007a3d45.\n",
      "> Parent node text: Wethenusethehuman\n",
      "preference data to train a safety reward model (see Section 3.2.2), and also re...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.retrievers.auto_merging_retriever import AutoMergingRetriever\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
    "# query_str = \"What were some lessons learned from red-teaming?\"\n",
    "# query_str = \"Can you tell me about the key concepts for safety finetuning\"\n",
    "query_str = (\n",
    "    \"What could be the potential outcomes of adjusting the amount of safety\"\n",
    "    \" data used in the RLHF stage?\"\n",
    ")\n",
    "\n",
    "nodes = retriever.retrieve(query_str)\n",
    "base_nodes = base_retriever.retrieve(query_str)\n",
    "# > Merging 4 nodes into parent node.\n",
    "# > Parent node id: caf5f81c-842f-46a4-b679-6be584bd6aff.\n",
    "# > Parent node text: We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: an...\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c13c3ee5-bd54-4121-b2fa-9ac61bb37261<br>**Similarity:** 0.862531014045397<br>**Text:** We also list two\n",
       "qualitative examples where safety and helpfulness reward models don’t agree with each other in Table 35.\n",
       "A.4.2 Qualitative Results on Safety Data Scaling\n",
       "In Section 4.2.3, we study the impact of adding more safety data into model RLHF in a quantitative manner.\n",
       "Hereweshowcaseafewsamplestoqualitativelyexaminetheevolutionofmodelbehaviorwhenwescale\n",
       "safetydatainTables36,37,and38.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e3e542f0-e6ed-4451-aee7-7c01007a3d45<br>**Similarity:** 0.8572897957134463<br>**Text:** Wethenusethehuman\n",
       "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
       "sample from the model during the RLHF stage.\n",
       "BetterLong-TailSafetyRobustnesswithoutHurtingHelpfulness Safetyisinherentlyalong-tailproblem,\n",
       "wherethe challengecomesfrom asmallnumber ofveryspecific cases. Weinvestigatetheimpact ofSafety\n",
       "RLHFbytakingtwointermediate Llama 2-Chat checkpoints—onewithoutadversarialpromptsintheRLHF\n",
       "stageandonewiththem—andscoretheirresponsesonourtestsetsusingoursafetyandhelpfulnessreward\n",
       "models. In Figure 14, we plot the score distribution shift of the safety RM on the safety test set (left) and that\n",
       "of the helpfulness RM on the helpfulness test set (right). In the left hand side of the figure, we observe that\n",
       "thedistributionofsafetyRMscoresonthesafetysetshiftstohigherrewardscoresaftersafetytuningwith\n",
       "RLHF,andthatthelongtailofthedistributionnearzerothinsout. Aclearclusterappearsonthetop-left\n",
       "corner suggesting the improvements of model safety. On the right side, we do not observe any gathering\n",
       "patternbelowthe y=xlineontherighthandsideofFigure14,whichindicatesthatthehelpfulnessscore\n",
       "distributionispreservedaftersafetytuningwithRLHF.Putanotherway,givensufficienthelpfulnesstraining\n",
       "data, the addition of an additional stage of safety mitigation does not negatively impact model performance\n",
       "on helpfulness to any notable degradation. A qualitative example is shown in Table 12.\n",
       "ImpactofSafetyDataScaling. AtensionbetweenhelpfulnessandsafetyofLLMshasbeenobservedin\n",
       "previous studies (Bai et al., 2022a). To better understand how the addition of safety training data affects\n",
       "general model performance, especially helpfulness, we investigate the trends in safety data scaling by\n",
       "adjustingtheamountofsafetydatausedintheRLHFstage. Inthisablationexperiment,wekeeptheamount\n",
       "of helpfulness training data unchanged ( ∼0.9M samples) and gradually increase the amount of safety data\n",
       "used in model tuning, ranging from 0% to 100% ( ∼0.1M samples).<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 5 nodes into parent node.\n",
      "> Parent node id: e3e542f0-e6ed-4451-aee7-7c01007a3d45.\n",
      "> Parent node text: Wethenusethehuman\n",
      "preference data to train a safety reward model (see Section 3.2.2), and also re...\n",
      "\n",
      "Adjusting the amount of safety data used in the RLHF stage could potentially have the following outcomes:\n",
      "1. Improved safety: Increasing the amount of safety data used in model tuning may lead to a shift in the score distribution of the safety reward model towards higher reward scores. This suggests that the model's safety performance improves, as indicated by a clearer cluster appearing in the top-left corner of the score distribution plot.\n",
      "2. Preserved helpfulness: Adjusting the amount of safety data does not negatively impact the model's performance on helpfulness. The distribution of helpfulness scores is preserved after safety tuning with RLHF, as indicated by the absence of any gathering pattern below the y=x line in the score distribution plot.\n",
      "3. Long-tail safety robustness: Safety is inherently a long-tail problem, meaning that the challenge arises from a small number of very specific cases. Increasing the amount of safety data may help thin out the long tail of the safety reward model's score distribution near zero, indicating improvements in model safety.\n",
      "4. Potential tension between helpfulness and safety: Previous studies have observed a tension between helpfulness and safety in language models. Adjusting the amount of safety data used in the RLHF stage allows for investigating the impact on general model performance, particularly on helpfulness. This suggests that there may be a trade-off between safety and helpfulness, and finding the right balance is important.\n",
      "Adjusting the amount of safety data used in the RLHF stage could potentially have an impact on the general model performance, particularly in terms of helpfulness. It is possible that increasing the amount of safety data could lead to improvements in model safety, as indicated by a clear cluster appearing on the top-left corner of the score distribution. However, it is also suggested that the addition of safety data does not negatively impact model performance on helpfulness, provided that there is sufficient helpfulness training data. Therefore, adjusting the amount of safety data used in the RLHF stage could potentially lead to improvements in model safety without notable degradation in model performance on helpfulness.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "base_query_engine = RetrieverQueryEngine.from_args(base_retriever)\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))\n",
    "base_response = base_query_engine.query(query_str)\n",
    "print(str(base_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    QueryResponseDataset,\n",
    ")\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# NOTE: run this if the dataset isn't already saved\n",
    "# Note: we only generate from the first 20 nodes, since the rest are references\n",
    "eval_service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-4\"))\n",
    "dataset_generator = DatasetGenerator(\n",
    "    root_nodes[:20],\n",
    "    service_context=eval_service_context,\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=3,\n",
    ")\n",
    "eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60)\n",
    "eval_dataset.save_json(\"data/llama2_eval_qr_dataset.json\")\n",
    "# optional\n",
    "eval_dataset = QueryResponseDataset.from_json(\n",
    "    \"data/llama2_eval_qr_dataset.json\"\n",
    ")\n",
    "결과 비교\n",
    "우리는 정확성, 의미론적 유사성, 관련성, 충실도 등 각 검색기에 대한 평가를 실행합니다.\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from llama_index.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    PairwiseComparisonEvaluator,\n",
    ")\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# NOTE: can uncomment other evaluators\n",
    "evaluator_c = CorrectnessEvaluator(service_context=eval_service_context)\n",
    "evaluator_s = SemanticSimilarityEvaluator(service_context=eval_service_context)\n",
    "evaluator_r = RelevancyEvaluator(service_context=eval_service_context)\n",
    "evaluator_f = FaithfulnessEvaluator(service_context=eval_service_context)\n",
    "# pairwise_evaluator = PairwiseComparisonEvaluator(service_context=eval_service_context)\n",
    "from llama_index.evaluation.eval_utils import get_responses, get_results_df\n",
    "from llama_index.evaluation import BatchEvalRunner\n",
    "eval_qs = eval_dataset.questions\n",
    "qr_pairs = eval_dataset.qr_pairs\n",
    "ref_response_strs = [r for (_, r) in qr_pairs]\n",
    "pred_responses = get_responses(eval_qs, query_engine, show_progress=True)\n",
    "base_pred_responses = get_responses(\n",
    "    eval_qs, base_query_engine, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred_response_strs = [str(p) for p in pred_responses]\n",
    "base_pred_response_strs = [str(p) for p in base_pred_responses]\n",
    "evaluator_dict = {\n",
    "    \"correctness\": evaluator_c,\n",
    "    \"faithfulness\": evaluator_f,\n",
    "    \"relevancy\": evaluator_r,\n",
    "    \"semantic_similarity\": evaluator_s,\n",
    "}\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)\n",
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    eval_qs, responses=pred_responses, reference=ref_response_strs\n",
    ")\n",
    "base_eval_results = await batch_runner.aevaluate_responses(\n",
    "    eval_qs, responses=base_pred_responses, reference=ref_response_strs\n",
    ")\n",
    "results_df = get_results_df(\n",
    "    [eval_results, base_eval_results],\n",
    "    [\"Auto Merging Retriever\", \"Base Retriever\"],\n",
    "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n",
    ")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_runner = BatchEvalRunner(\n",
    "    {\"pairwise\": pairwise_evaluator}, workers=10, show_progress=True\n",
    ")\n",
    "pairwise_eval_results = await batch_runner.aevaluate_response_strs(\n",
    "    eval_qs,\n",
    "    response_strs=pred_response_strs,\n",
    "    reference=base_pred_response_strs,\n",
    ")\n",
    "pairwise_score = np.array(\n",
    "    [r.score for r in pairwise_eval_results[\"pairwise\"]]\n",
    ").mean()\n",
    "pairwise_score\n",
    "0.525"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('llamaindex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f13b51dec9893a665284acd155bab776a7e14c668dd25de27de89f99a4571a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
