{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Use local embeddings + gpt-3.5-turbo-16k\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-16k\", max_tokens=512, temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-base-en\"\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, Document\n",
    "from llama_index.node_parser import HierarchicalNodeParser, SimpleNodeParser, get_leaf_nodes\n",
    "from llama_index.schema import MetadataMode\n",
    "# from llama_docs_bot.markdown_docs_reader import MarkdownDocsReader\n",
    "\n",
    "\n",
    "\"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "filepath = './data/'\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=filepath, \n",
    "    required_exts=[\".pdf\"],\n",
    "    # file_extractor={\".md\": MarkdownDocsReader()},\n",
    "    recursive=True\n",
    ")\n",
    "hierarchical = True\n",
    "documents = loader.load_data()\n",
    "\n",
    "if hierarchical:\n",
    "    # combine all documents into one\n",
    "    # documents = [\n",
    "    #     Document(text=\"\\n\\n\".join(\n",
    "    #             document.get_content(metadata_mode=MetadataMode.ALL) \n",
    "    #             for document in documents\n",
    "    #         )\n",
    "    #     )\n",
    "    # ]\n",
    "\n",
    "    # chunk into 3 levels\n",
    "    # majority means 2/3 are retrieved before using the parent\n",
    "    large_chunk_size = 1536\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(\n",
    "        chunk_sizes=[\n",
    "            1024, \n",
    "            512,\n",
    "            128\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    # return nodes, get_leaf_nodes(nodes)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    # nodes, get_leaf_nodes 확인\n",
    "# else:\n",
    "#     node_parser = SimpleNodeParser.from_defaults()\n",
    "#     nodes = node_parser.get_nodes_from_documents(documents)\n",
    "#     return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2533, 1865)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.__len__(), get_leaf_nodes(nodes).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('afa8ec28-8b5e-48f9-b4a5-a42f3012bf6d',\n",
       " '673547a3-927d-4370-94ff-f8dcb830445f',\n",
       " None,\n",
       " [RelatedNodeInfo(node_id='661e105e-65ba-4f3a-adbe-469f6717fa68', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='0b5c1c4363f0b9282b21e888107f244c3f978e41aa9a63bfe0341ff5c422b174'),\n",
       "  RelatedNodeInfo(node_id='979f262e-335a-407d-915e-0c3b32cec2bd', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='15cab383be0eea103016ce032ec460475440319dfe9a8d1aa22b6c066349c0af')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].ref_doc_id, nodes[0].node_id, nodes[0].parent_node, nodes[0].child_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research;‡University College London;⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [ 47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [ 51,52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4  [cs.CL]  12 Apr 2021'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('We explore\\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models.',\n",
       " RelatedNodeInfo(node_id='ff6c50b6-c8b0-4df3-8ea0-5ab46a6b578a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='18249784e643e06d1bb7f023af1263ec93da056d5e05cf7741376c694dcd19f2'),\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[83].text, nodes[83].parent_node, nodes[83].child_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[296].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is worth noting that RAG’s\\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[85].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\\nencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[86].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " RelatedNodeInfo(node_id='9442703e-c67a-44cd-9c0f-e8ef121d1d17', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '4', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='6bd50c649fa0c18f78d81573f13a5ae84074f55934354c880c773309c59b38f7'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[83].child_nodes, nodes[59].parent_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='61bb9b59-8512-4596-9379-b1a2d81b8d9c', embedding=None, metadata={'page_label': '17', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='99a75de0-06bc-4841-882b-5204bfdda56b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '17', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='0c568b76c6b741a5e782f0bd780704b8308b41a46208dc8042901d2bd73a3f57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='08dfffe6-78b0-4a33-9c59-41dcd581d76c', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '17', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='24591bc10c4d0872f0913e2938781b57470b34148dbb26a4eceb20ff6fc4074d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f7126b89-bd3a-47bb-b77d-f02ee6752c9d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7c72525c0ac22d840399d98e7dc3b785db099cd5abf101a6a872efb0c560caa1'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='99a75de0-06bc-4841-882b-5204bfdda56b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '17', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='0c568b76c6b741a5e782f0bd780704b8308b41a46208dc8042901d2bd73a3f57')}, hash='ae9e3062f1aeacd6b784cd06ee042fa89d2ee7b326a775949f5f22d30fc6835f', text='We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.', start_char_idx=353, end_char_idx=761, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c0fd20be-2a8d-412b-8454-47f3bd715452', embedding=None, metadata={'page_label': '5', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ff6c50b6-c8b0-4df3-8ea0-5ab46a6b578a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='18249784e643e06d1bb7f023af1263ec93da056d5e05cf7741376c694dcd19f2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3d8b9808-a294-4faa-9181-d54d075ea118', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='51e04f39d449d3f8d801a21eded2c4737686f003446328fe05c6375393c19130'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='49e71175-7c20-443c-aa1c-aae5155becb4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='530cc80e23dc2f90fe685dec0795222d59fe6507192627d4a86017b4246d598b'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='ff6c50b6-c8b0-4df3-8ea0-5ab46a6b578a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': '2005.11401.pdf', 'file_path': 'data/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2023-12-17', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2023-12-17'}, hash='18249784e643e06d1bb7f023af1263ec93da056d5e05cf7741376c694dcd19f2')}, hash='4949c297fd3707877eb26b9302dc784026c21be7aa9077120553bcb7854efe59', text='We explore\\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models.', start_char_idx=1120, end_char_idx=1442, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_leaf_nodes([nodes[0], nodes[296], nodes[83]])\n",
    "# leaf nodes는 nodes[296] 을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nodes.__len__()):\n",
    "    if nodes[i].node_id == '5955f000-41f9-4402-915a-78df02b12e9a':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 2533/2533 [18:02<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index import VectorStoreIndex,StorageContext, load_index_from_storage\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import AutoMergingRetriever\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "\n",
    "# 주의할 점. 여기서 경로를 안 지우고 저장하면, 기존에 있던 저장소 위에 덮어씌워짐. 파일명을 계속 바꾸던지, 아니면 저장소를 바꿔야 함\n",
    "# create client and a new collection\n",
    "# chroma_client = chromadb.EphemeralClient()\n",
    "chroma_client = chromadb.PersistentClient(path=f\"./Retrieval_Database/LLM_chroma_openai\")\n",
    "chroma_collection = chroma_client.create_collection(\"LLM_folder\", get_or_create=True)\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# service_context = ServiceContext.from_defaults()#embed_model=embed_model)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    store_nodes_override=True,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = AutoMergingRetriever(\n",
    "    index.as_retriever(similarity_top_k=30), \n",
    "    storage_context=storage_context,\n",
    "    simple_ratio_thresh=0.1\n",
    ")\n",
    "# threshold의 default는 0.5임. 이는, 3개의 계층으로 나눴을 때 3개 3개 중 2개가 선택된다면, 비율이 0.66 > 0.5 이므로 최상위 노드를 반환함.\n",
    "# 하지만 한 개만 선택된다면 0.33 < 0.5 이기 때문에 최상위 노드를 반환하지 않음.\n",
    "# 이걸 낮게 주면, 한 개만 선택되어도 무조건 최상위 노드를 리턴하게 됨.\n",
    "# res = retriever.retrieve(\"What papers did the authors reference in LLaMA2 and how do they relate to the content of LLaMA2?\")\n",
    "# res.__len__()\n",
    "res = retriever.retrieve(\"What is DropEdge?\")\n",
    "res.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[RelatedNodeInfo(node_id='9205633f-fb28-448a-9b6e-42f775d5b468', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '59', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4f964c2fda5e02a175df60bf1d8a84bbdc9840c27d80311e939de1928d8cbd21'), RelatedNodeInfo(node_id='df528c0b-6d88-40a3-816d-af57502852de', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '59', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='d2e7380a48bc5c837ed7fcc7c17229307670def6dcf83a6864d4d1fcc6d981b0')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='920f84a4-9f88-4e36-82b1-66543a8f024e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='de6539373e0af4f3e653151394e63db4fbd9be33347ac9bc045d2fc3048f9f9c'), RelatedNodeInfo(node_id='f7499bf1-1a02-4b67-bb38-d489a7cc0c94', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='e0b3320013019376c7f6e2877e6b2fd2c36e5f6c9b930325c83f91a5d32869eb'), RelatedNodeInfo(node_id='1731f05a-90a6-484b-8f58-b0fb8dc4d375', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='808affe4d3fcfe5914365395ed5eef9d88dfe33414759ef1611ed69e2e02de3e')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='842cd27d-867e-493a-91ce-658139666f5e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '34', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4b1478dd8fe5a4329f2c017da2a4a453f5eea7591bfc0b4794805d2e3e217dd3'), RelatedNodeInfo(node_id='56f44bbe-95ed-4644-8b44-acf222f7c1c2', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '34', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='65d9d952e3208d2eb0b18a83e12ea75377e15cf9115c62de9bacc2e9390a8301')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='920f84a4-9f88-4e36-82b1-66543a8f024e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='de6539373e0af4f3e653151394e63db4fbd9be33347ac9bc045d2fc3048f9f9c'), RelatedNodeInfo(node_id='f7499bf1-1a02-4b67-bb38-d489a7cc0c94', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='e0b3320013019376c7f6e2877e6b2fd2c36e5f6c9b930325c83f91a5d32869eb'), RelatedNodeInfo(node_id='1731f05a-90a6-484b-8f58-b0fb8dc4d375', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='808affe4d3fcfe5914365395ed5eef9d88dfe33414759ef1611ed69e2e02de3e')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='63ddb7b4-1017-42ad-866a-a6077b81ba63', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='a535e6fc27fe3077deb4205c7e10fc5af464e9dcbfa5f2caeddaadf1686542f4'), RelatedNodeInfo(node_id='69462421-7294-43f8-9894-07c87488a688', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='7c851c478467e5fcbd7d12cacc522f35f3a55813b5f0ee9ab16c2b2a57161dea'), RelatedNodeInfo(node_id='91b2c03a-b981-41a8-9a5a-bdb34f209326', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='83e05fe5c64747e1a4b93f987f54d2d907fa2e2e41589dee892de5d41cb0adfd')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='63ddb7b4-1017-42ad-866a-a6077b81ba63', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='a535e6fc27fe3077deb4205c7e10fc5af464e9dcbfa5f2caeddaadf1686542f4'), RelatedNodeInfo(node_id='69462421-7294-43f8-9894-07c87488a688', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='7c851c478467e5fcbd7d12cacc522f35f3a55813b5f0ee9ab16c2b2a57161dea'), RelatedNodeInfo(node_id='91b2c03a-b981-41a8-9a5a-bdb34f209326', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='83e05fe5c64747e1a4b93f987f54d2d907fa2e2e41589dee892de5d41cb0adfd')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='842cd27d-867e-493a-91ce-658139666f5e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '34', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4b1478dd8fe5a4329f2c017da2a4a453f5eea7591bfc0b4794805d2e3e217dd3'), RelatedNodeInfo(node_id='56f44bbe-95ed-4644-8b44-acf222f7c1c2', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '34', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='65d9d952e3208d2eb0b18a83e12ea75377e15cf9115c62de9bacc2e9390a8301')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='9205633f-fb28-448a-9b6e-42f775d5b468', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '59', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4f964c2fda5e02a175df60bf1d8a84bbdc9840c27d80311e939de1928d8cbd21'), RelatedNodeInfo(node_id='df528c0b-6d88-40a3-816d-af57502852de', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '59', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='d2e7380a48bc5c837ed7fcc7c17229307670def6dcf83a6864d4d1fcc6d981b0')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='6512e87b-2fd0-4bd8-b626-7460a2595800', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '28', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='ff751285f2a2d22993874e263d1c66db786ca6c6197ffc54114207a165bfc910'), RelatedNodeInfo(node_id='af672946-2858-4193-8315-5ff4648f164f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '28', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='3f4ca59ec19d2e9bff3ab38a606696e6f54571523596120f12adc97a22d5c382')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='2c15c969-d0f1-4e12-8926-5973e322d038', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='c4bea9eaf1eb38f88bf19bd8495502f86515a1199bf13a60bd757404b3ec5ceb'), RelatedNodeInfo(node_id='84c4d4df-4baa-4783-898f-b6992d205c6f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4acc4bf7a8872451622a1ee0b43c5eea2324223c6d2743ec0a6f86e5185997a2'), RelatedNodeInfo(node_id='3abdb7b5-877e-42e7-8853-c69048038299', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='b6801f0846dca6cf876a044553fee48485b21e38a6fe7323a2c96a491f8267ec')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='4504fb57-9121-44bf-97a2-a9197923cd88', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '29', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='e37b6d594d50d13eb065e32248387a5867a1deb0b5d611cbd9b1171f0ae6a649'), RelatedNodeInfo(node_id='8f0ecc28-1bdf-47e9-ad0c-6961f731a7f7', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '29', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='0be6bc230af786aafc7d63209bc78cad348a2651a573ffb8e49edc83c1ab3596'), RelatedNodeInfo(node_id='0b8828a8-277c-4e91-8566-4a6abd38c399', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '29', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='71d2d4a4959d013dca43d822383fbfba51c203a95786124a7d5b3b35f885136f')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='4504fb57-9121-44bf-97a2-a9197923cd88', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '29', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='e37b6d594d50d13eb065e32248387a5867a1deb0b5d611cbd9b1171f0ae6a649'), RelatedNodeInfo(node_id='8f0ecc28-1bdf-47e9-ad0c-6961f731a7f7', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '29', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='0be6bc230af786aafc7d63209bc78cad348a2651a573ffb8e49edc83c1ab3596'), RelatedNodeInfo(node_id='0b8828a8-277c-4e91-8566-4a6abd38c399', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '29', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='71d2d4a4959d013dca43d822383fbfba51c203a95786124a7d5b3b35f885136f')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='2c15c969-d0f1-4e12-8926-5973e322d038', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='c4bea9eaf1eb38f88bf19bd8495502f86515a1199bf13a60bd757404b3ec5ceb'), RelatedNodeInfo(node_id='84c4d4df-4baa-4783-898f-b6992d205c6f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4acc4bf7a8872451622a1ee0b43c5eea2324223c6d2743ec0a6f86e5185997a2'), RelatedNodeInfo(node_id='3abdb7b5-877e-42e7-8853-c69048038299', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='b6801f0846dca6cf876a044553fee48485b21e38a6fe7323a2c96a491f8267ec')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='920f84a4-9f88-4e36-82b1-66543a8f024e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='de6539373e0af4f3e653151394e63db4fbd9be33347ac9bc045d2fc3048f9f9c'), RelatedNodeInfo(node_id='f7499bf1-1a02-4b67-bb38-d489a7cc0c94', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='e0b3320013019376c7f6e2877e6b2fd2c36e5f6c9b930325c83f91a5d32869eb'), RelatedNodeInfo(node_id='1731f05a-90a6-484b-8f58-b0fb8dc4d375', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='808affe4d3fcfe5914365395ed5eef9d88dfe33414759ef1611ed69e2e02de3e')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='a2913099-e622-4f5f-8437-e15827fcc254', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '46', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='ef83850e47c437a60a049aca565334b3784f90d7181d21c5d345b781a417eb49'), RelatedNodeInfo(node_id='98d97a70-e34b-4ea9-bc44-5053ec84b7b5', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '46', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='562d0c60be20db378cf2c55dd42a5bfa4dd4b167630960b8e6390c6dcc7e3a38'), RelatedNodeInfo(node_id='bb02fd66-04fd-4fce-880c-cbce7fce1b1c', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '46', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='ed388a70d7aa1ddaf581e53143a1151b2e63935f1d3af43e03ea50c641466f90')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='842cd27d-867e-493a-91ce-658139666f5e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '34', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4b1478dd8fe5a4329f2c017da2a4a453f5eea7591bfc0b4794805d2e3e217dd3'), RelatedNodeInfo(node_id='56f44bbe-95ed-4644-8b44-acf222f7c1c2', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '34', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='65d9d952e3208d2eb0b18a83e12ea75377e15cf9115c62de9bacc2e9390a8301')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='53a072aa-a713-4848-91f9-0283e69c7ac4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '30', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='f9ce2ba52b34a4da7e01c58823960faeed476ba8e653fdae2f3400cae264270e'), RelatedNodeInfo(node_id='53a8bc8a-5174-4d63-9978-094494bd0a6a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '30', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='be13a7c1affe7d67fd125ecbd4eb1b6a0a0afe4d4a984e6898d966250bdf082b')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='63ddb7b4-1017-42ad-866a-a6077b81ba63', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='a535e6fc27fe3077deb4205c7e10fc5af464e9dcbfa5f2caeddaadf1686542f4'), RelatedNodeInfo(node_id='69462421-7294-43f8-9894-07c87488a688', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='7c851c478467e5fcbd7d12cacc522f35f3a55813b5f0ee9ab16c2b2a57161dea'), RelatedNodeInfo(node_id='91b2c03a-b981-41a8-9a5a-bdb34f209326', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '24', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='83e05fe5c64747e1a4b93f987f54d2d907fa2e2e41589dee892de5d41cb0adfd')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='2e20d104-1dba-4138-bc07-6c45ff9a2e77', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '70', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='ad032271689990bc7810f0a65dfcadfca7f1a5b8ff7e01da96fad9735f093a13'), RelatedNodeInfo(node_id='16c7649c-5cd9-41fa-89eb-51578886fa8f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '70', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='6daa43dcca8940a5c01f86396e160dc3f343c07c18179c9cf5fb83597b443b1f'), RelatedNodeInfo(node_id='6c62cb8f-fff8-46ee-86ab-319ce38a29ed', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '70', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='42bf0b75ed5adcc32363b746082d7d71b94e566b5224808c73b342189eadb970')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='572e2756-bdaf-409e-9015-4a4b2740097b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '23', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='6c52ac0dd5e3aa7396fb8fa27229538dd6edc1528520525e8c1bccdf1fb75444'), RelatedNodeInfo(node_id='036b1f4f-3d71-41ee-925a-7dc5a3e098fe', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '23', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='d8fe2ddb5d16e7942b67d0118f9bf8ff36f709d07bc3c0799641854f390c56ee')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='22bf12a7-0407-443f-b452-c55dba6f1828', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '22', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='441c292ec0922da0018517c10383aceee2c9377fdc41fd74225adc3b9a54b52b'), RelatedNodeInfo(node_id='7b7145be-3ff2-45af-995a-7a3664fc2652', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '22', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='82468a990e5b8dbc3c36b8e2af3bc67cc1e88fb0b4757dad90584e2153bfb4de'), RelatedNodeInfo(node_id='815f83a3-cee5-4e69-9af0-445aa7cc8e68', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '22', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='c09f0d0e2e7f93680e9ee7bd642e6d9b9e7bc1f996fa51c0f2ab9de075819192')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='a2913099-e622-4f5f-8437-e15827fcc254', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '46', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='ef83850e47c437a60a049aca565334b3784f90d7181d21c5d345b781a417eb49'), RelatedNodeInfo(node_id='98d97a70-e34b-4ea9-bc44-5053ec84b7b5', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '46', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='562d0c60be20db378cf2c55dd42a5bfa4dd4b167630960b8e6390c6dcc7e3a38'), RelatedNodeInfo(node_id='bb02fd66-04fd-4fce-880c-cbce7fce1b1c', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '46', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='ed388a70d7aa1ddaf581e53143a1151b2e63935f1d3af43e03ea50c641466f90')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='d5b6e21b-ebe6-4219-b66f-73f82026d901', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '31', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='1ab38a34969b90c8dd3282be97847405d4bed97daa3ffa02886f924ccc696b2e'), RelatedNodeInfo(node_id='b7b39c72-6de6-4687-bc80-aeb15a471679', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '31', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='3a229cf4f488802f8f67a78c6fd332b940ff6e3da5011f5dba9a22997aa07573')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='2c15c969-d0f1-4e12-8926-5973e322d038', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='c4bea9eaf1eb38f88bf19bd8495502f86515a1199bf13a60bd757404b3ec5ceb'), RelatedNodeInfo(node_id='84c4d4df-4baa-4783-898f-b6992d205c6f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4acc4bf7a8872451622a1ee0b43c5eea2324223c6d2743ec0a6f86e5185997a2'), RelatedNodeInfo(node_id='3abdb7b5-877e-42e7-8853-c69048038299', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='b6801f0846dca6cf876a044553fee48485b21e38a6fe7323a2c96a491f8267ec')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='1b38a563-dcec-4a20-b4d1-fc9e2934d0b7', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '36', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='6dd2a214f298d451147df32f53c2e3e20e256c968cdfe5aa2d8f82363bed0df8'), RelatedNodeInfo(node_id='387423ca-10f1-4888-a3f3-54b1a5a115bc', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '36', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='6630bff7582fb8e612240ccb81bb39f008c6daf65e07ca1a20fe63450aee5864')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='22bf12a7-0407-443f-b452-c55dba6f1828', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '22', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='441c292ec0922da0018517c10383aceee2c9377fdc41fd74225adc3b9a54b52b'), RelatedNodeInfo(node_id='7b7145be-3ff2-45af-995a-7a3664fc2652', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '22', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='82468a990e5b8dbc3c36b8e2af3bc67cc1e88fb0b4757dad90584e2153bfb4de'), RelatedNodeInfo(node_id='815f83a3-cee5-4e69-9af0-445aa7cc8e68', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '22', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='c09f0d0e2e7f93680e9ee7bd642e6d9b9e7bc1f996fa51c0f2ab9de075819192')]\n",
      "------------\n",
      "None\n",
      "[RelatedNodeInfo(node_id='8c2b8dd2-f519-4d83-9ea1-ad4113b69b89', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='4e76262120858e40d896ad019875e600efc7e1a32ae934d76d00e949821e2c85'), RelatedNodeInfo(node_id='1fc56914-179a-45ce-bd6b-30607e296126', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'llama2.pdf', 'file_path': 'data/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2023-12-16', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2023-12-16'}, hash='0e17ffeb41d526723c16860fd6d317135043339cbf8beafb8753c9b81ed4cd38')]\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(res)):\n",
    "    print(res[i].node.parent_node)\n",
    "    print(res[i].node.child_nodes)\n",
    "    print('------------')\n",
    "    \n",
    "    \n",
    "# 여기서 file name 같은거 딕셔너리로 하나로 모으고\n",
    "# 거기서 내용만 참조하도록 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_retriever = index.as_retriever(similarity_top_k=10)\n",
    "index_res = index_retriever.retrieve('What is LLaMA2?')\n",
    "index_res.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold = 0.5 일 때\n",
    "\n",
    "f8bd3e1c-3950-4900-8f85-440d7ea2362d\n",
    "\n",
    "343e4825-c62e-4a9e-9789-d7e4d47bfb7f (343의 Parent node는 9dd)\n",
    "f4fd1722-520c-41b2-be9e-b2d500488a71 (343의 Child node임.)\n",
    "\n",
    "04078385-fcb9-4da8-928d-4b42ae5d90ee\n",
    "2aacdfbd-b137-41ec-a5f4-ed5244c44802\n",
    "f4d1e035-4858-4bb5-a765-8683af09f811\n",
    "812d3faf-f297-45c3-a6e5-c89993a50aba\n",
    "5690ed2e-29ac-431f-be00-85c6112c2a8d\n",
    "f74ed3d4-8edc-48b7-963c-877508506d39\n",
    "5a731c2b-1100-4424-901b-bdb5d8d23557\n",
    "\n",
    "automerging은 Child node, parent node가 다 있어야 완전한 parent node로 대체되는거네.\n",
    "\n",
    "---\n",
    "\n",
    "f8bd3e1c-3950-4900-8f85-440d7ea2362d\n",
    "\n",
    "9dddb0e6-ec03-4d44-b88d-3ebf3964233c\n",
    "\n",
    "04078385-fcb9-4da8-928d-4b42ae5d90ee\n",
    "2aacdfbd-b137-41ec-a5f4-ed5244c44802\n",
    "f4d1e035-4858-4bb5-a765-8683af09f811\n",
    "812d3faf-f297-45c3-a6e5-c89993a50aba\n",
    "5690ed2e-29ac-431f-be00-85c6112c2a8d\n",
    "f74ed3d4-8edc-48b7-963c-877508506d39\n",
    "5a731c2b-1100-4424-901b-bdb5d8d23557\n",
    "\n",
    "---\n",
    "\n",
    "ab38a223-9e5c-4d91-9fbe-a3215227c90c #child 2개 'ed280e47-194c-47df-b2d7-48c9971eca35' '4e40d025-ee38-4aa8-99d3-e1ef9e3ca01e'\n",
    "\n",
    "9dddb0e6-ec03-4d44-b88d-3ebf3964233c\n",
    "9dddb0e6-ec03-4d44-b88d-3ebf3964233c\n",
    "\n",
    "0893d9ce-ee9c-4f77-9da4-56009760ddb8\n",
    "d8bc367d-58b9-48fd-8010-a33ff8c2ba69\n",
    "dd7469f8-b8e8-4caf-9994-52e09f633496\n",
    "7d3ca43f-7dea-44c0-bad8-9d9f6110cad8\n",
    "d4cae969-98c4-41a4-b8af-cd46f48077ba\n",
    "c7f22b05-d380-4797-a986-acf8a4e6d227\n",
    "a7869589-c73e-45e9-a505-2af628faba0f\n",
    "ce73bafb-3960-4f93-a3b2-3aec78cd1293\n",
    "2d9402ea-f188-4420-8798-791a10480030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res[i].node.parent_node.node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever,\n",
    "        # node_postprocessors=postprocessors or [],\n",
    "    )\n",
    "\n",
    "query_engine = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name='./Retrieval_Database/llama2_chroma_openai',\n",
    "        description='llama2 paper'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16420 tokens (15908 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mWhat papers did the authors reference in LLaMA2 and how do they relate to the content of LLaMA2?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mPlease list the names and authors of the paper you referenced.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQueryBundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/core/base_query_engine.py:30\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     29\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:171\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    168\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    169\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m    170\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 171\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    172\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    173\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py:146\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    143\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    145\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 146\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    147\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    148\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    149\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    150\u001b[0m         ],\n\u001b[1;32m    151\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    154\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     39\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m     40\u001b[0m     text_chunks\u001b[39m=\u001b[39;49mnew_texts,\n\u001b[1;32m     41\u001b[0m     prev_response\u001b[39m=\u001b[39;49mprev_response,\n\u001b[1;32m     42\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:127\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m prev_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    128\u001b[0m             query_str, text_chunk, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    133\u001b[0m             prev_response, query_str, text_chunk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kwargs\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:182\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    179\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    181\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 182\u001b[0m             program(\n\u001b[1;32m    183\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m    184\u001b[0m                 output_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_cls,\n\u001b[1;32m    185\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    186\u001b[0m             ),\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    189\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:53\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructuredRefineResponse:\n\u001b[0;32m---> 53\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt,\n\u001b[1;32m     55\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[39m=\u001b[39manswer, query_satisfied\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:219\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, output_cls, **prompt_args)\u001b[0m\n\u001b[1;32m    217\u001b[0m     messages \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat_messages(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[1;32m    218\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_messages(messages)\n\u001b[0;32m--> 219\u001b[0m     chat_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mchat(messages)\n\u001b[1;32m    220\u001b[0m     output \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llms/base.py:187\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m    179\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    180\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    181\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         },\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    190\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llms/openai.py:200\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     chat_fn \u001b[39m=\u001b[39m completion_to_chat_decorator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_complete)\n\u001b[0;32m--> 200\u001b[0m \u001b[39mreturn\u001b[39;00m chat_fn(messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llms/openai.py:254\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chat\u001b[39m(\u001b[39mself\u001b[39m, messages: Sequence[ChatMessage], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponse:\n\u001b[1;32m    253\u001b[0m     message_dicts \u001b[39m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 254\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    255\u001b[0m         messages\u001b[39m=\u001b[39;49mmessage_dicts,\n\u001b[1;32m    256\u001b[0m         stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    257\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_model_kwargs(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    259\u001b[0m     openai_message \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\n\u001b[1;32m    260\u001b[0m     message \u001b[39m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    843\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    844\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    845\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    846\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    847\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    848\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16420 tokens (15908 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "What papers did the authors reference in LLaMA2 and how do they relate to the content of LLaMA2?\"\n",
    "Please list the names and authors of the paper you referenced.\n",
    "\"\"\"\n",
    "\n",
    "res = query_engine.query_engine.query(QueryBundle(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The papers referenced in LLaMA2 are not provided in the given context information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "display_response(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = query_engine.query_engine.retrieve(\"What papers did the authors reference in LLaMA2 and how do they relate to the content of LLaMA2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16399 tokens (15887 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m      7\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mWhat papers did the authors reference in LLaMA2 and how do they relate to the content of LLaMA2?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mPlease list the names and authors of the paper you referenced.\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mquestion : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/core/base_query_engine.py:30\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     29\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:171\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    168\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    169\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m    170\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 171\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    172\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    173\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py:146\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    143\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    145\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 146\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    147\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    148\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    149\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    150\u001b[0m         ],\n\u001b[1;32m    151\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    154\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     39\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m     40\u001b[0m     text_chunks\u001b[39m=\u001b[39;49mnew_texts,\n\u001b[1;32m     41\u001b[0m     prev_response\u001b[39m=\u001b[39;49mprev_response,\n\u001b[1;32m     42\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:127\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m prev_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    128\u001b[0m             query_str, text_chunk, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    133\u001b[0m             prev_response, query_str, text_chunk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kwargs\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:182\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    179\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    181\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 182\u001b[0m             program(\n\u001b[1;32m    183\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m    184\u001b[0m                 output_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_cls,\n\u001b[1;32m    185\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    186\u001b[0m             ),\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    189\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:53\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructuredRefineResponse:\n\u001b[0;32m---> 53\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt,\n\u001b[1;32m     55\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[39m=\u001b[39manswer, query_satisfied\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:219\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, output_cls, **prompt_args)\u001b[0m\n\u001b[1;32m    217\u001b[0m     messages \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat_messages(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[1;32m    218\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_messages(messages)\n\u001b[0;32m--> 219\u001b[0m     chat_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mchat(messages)\n\u001b[1;32m    220\u001b[0m     output \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llms/base.py:187\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m    179\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    180\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    181\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         },\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    190\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llms/openai.py:200\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     chat_fn \u001b[39m=\u001b[39m completion_to_chat_decorator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_complete)\n\u001b[0;32m--> 200\u001b[0m \u001b[39mreturn\u001b[39;00m chat_fn(messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/llms/openai.py:254\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chat\u001b[39m(\u001b[39mself\u001b[39m, messages: Sequence[ChatMessage], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponse:\n\u001b[1;32m    253\u001b[0m     message_dicts \u001b[39m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 254\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    255\u001b[0m         messages\u001b[39m=\u001b[39;49mmessage_dicts,\n\u001b[1;32m    256\u001b[0m         stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    257\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_model_kwargs(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    259\u001b[0m     openai_message \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\n\u001b[1;32m    260\u001b[0m     message \u001b[39m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    843\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    844\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    845\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    846\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    847\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    848\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16399 tokens (15887 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "context = ''\n",
    "for i in range(len(res)):\n",
    "    context += '\\n' + res[i].text\n",
    "    if i == 7:\n",
    "        break\n",
    "    \n",
    "question = \"\"\"\n",
    "What papers did the authors reference in LLaMA2 and how do they relate to the content of LLaMA2?\"\n",
    "Please list the names and authors of the paper you referenced.\n",
    "\"\"\"\n",
    "query = f\"\"\"\n",
    "\n",
    "{context}\n",
    "\n",
    "Answer the questions below by referring to the context above.\n",
    "question : {question}\n",
    "\"\"\"\n",
    "\n",
    "res = query_engine.query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_response(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.storage.docstore import SimpleDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Use local embeddings + gpt-3.5-turbo-16k\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-16k\", max_tokens=512, temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-base-en\"\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "# loader = PDFReader()\n",
    "# docs = loader.load_data(file=Path(\"/home/inbodyai/문서/Junhwi/test_pdf_data/LLaMA2.pdf\"))\n",
    "# \n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"/home/inbodyai/문서/Junhwi/test_pdf_data/\",\n",
    "    required_exts=['.pdf'],\n",
    "    recursive=True,\n",
    ")\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from collections import defaultdict\n",
    "\n",
    "prev_page = 0\n",
    "cnt = 0\n",
    "text = '\\n'\n",
    "documents_dict = defaultdict(str)\n",
    "for doc in docs:\n",
    "    documents_dict[doc.metadata['file_name']] += doc.get_content()\n",
    "    \n",
    "documents = [\n",
    "    Document(text=documents_dict[key],\n",
    "             metadata={'file_name' : key})\n",
    "             for key in documents_dict\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = HierarchicalNodeParser.from_default(\n",
    "    chunk_sizes = [\n",
    "        512,\n",
    "        256,\n",
    "        256\n",
    "    ]\n",
    ")\n",
    "nodes = node_parser.get_nodes_from_documents(final_doc_list),\n",
    "leaf_nodes = get_leaf_nodes(nodes[0])\n",
    "\n",
    "from llama_index.embeddings import resolve_embed_model\n",
    "\n",
    "embed_model = resolve_embed_model('local:BAAI/bge-small-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node embeddings\n",
    "for i in range(nodes[0].__len__()):\n",
    "    nodes[0][i].embedding = embed_model.get_text_embedding(nodes[0][i].text)\n",
    "    \n",
    "for i in range(leaf_nodes.__len__()):\n",
    "    leaf_nodes[i].embedding = embed_model.get_text_embedding(leaf_nodes[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make qdrant vectorstore\n",
    "client = qdrant_client.QdrantClient(\n",
    "    path = \"./test_Qdrant\"\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client,\n",
    "                                 enable_hybrid=True,\n",
    "                                 collection_name=\"papers\")\n",
    "vector_store.add(nodes[0])\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore,\n",
    "    vector_store=vector_store,\n",
    "    )\n",
    "\n",
    "core_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    # store_nodes_override=True\n",
    ")\n",
    "docstore.persist()\n",
    "core_index.storage_context.persist(persist_dir='./test_Qdrant')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir='/home/inbodyai/문서/Junhwi/InBody_RAG_v3/test_Qdrant')\n",
    "from llama_index.indices.loading import load_index_from_storage\n",
    "new_index = load_index_from_storage(\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import RecursiveRetriever, AutoMergingRetriever\n",
    "retriever_auto = AutoMergingRetriever(\n",
    "    core_index.as_retriever(similarity_top_k=10, sparse_top_k=3), \n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "res = retriever_auto.retrieve('llama2?')\n",
    "\n",
    "for i in range(res.__len__()):\n",
    "    print(len(res[i].text), res[i].node.child_nodes)\n",
    "\n",
    "print()\n",
    "for i in range(res.__len__()):\n",
    "    print(len(res[i].text), res[i].node.parent_node)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('LLM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a3a50cfa768e55079b834244adbc23d62a4b24bbd2f8b443e30bd8c79d3f7ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
