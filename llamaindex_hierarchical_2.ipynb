{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.storage.docstore import SimpleDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/llamaindex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Use local embeddings + gpt-3.5-turbo-16k\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-16k\", max_tokens=512, temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-base-en\"\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "from llama_index.embeddings import resolve_embed_model\n",
    "\n",
    "embed_model = resolve_embed_model('local:BAAI/bge-small-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"./data\",\n",
    "    required_exts=['.pdf'],\n",
    "    recursive=True,\n",
    ")\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, Document\n",
    "from llama_index.node_parser import HierarchicalNodeParser, SimpleNodeParser, get_leaf_nodes\n",
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes = [\n",
    "        512,\n",
    "        256\n",
    "    ]\n",
    ")\n",
    "nodes = node_parser.get_nodes_from_documents(docs),\n",
    "leaf_nodes = get_leaf_nodes(nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node embeddings\n",
    "for i in range(nodes[0].__len__()):\n",
    "    nodes[0][i].embedding = embed_model.get_text_embedding(nodes[0][i].text)\n",
    "    \n",
    "for i in range(leaf_nodes.__len__()):\n",
    "    leaf_nodes[i].embedding = embed_model.get_text_embedding(leaf_nodes[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qdrant_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# make qdrant vectorstore\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mqdrant_client\u001b[49m\u001b[38;5;241m.\u001b[39mQdrantClient(\n\u001b[1;32m      4\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./test_Qdrant_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m QdrantVectorStore(client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m      8\u001b[0m                                  enable_hybrid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m                                  collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpapers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m vector_store\u001b[38;5;241m.\u001b[39madd(nodes[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qdrant_client' is not defined"
     ]
    }
   ],
   "source": [
    "# save\n",
    "# make qdrant vectorstore\n",
    "client = qdrant_client.QdrantClient(\n",
    "    path = \"./test_Qdrant_2\"\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client,\n",
    "                                 enable_hybrid=True,\n",
    "                                 collection_name=\"papers\")\n",
    "vector_store.add(nodes[0])\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes[0])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore,\n",
    "    vector_store=vector_store,\n",
    "    )\n",
    "\n",
    "\n",
    "# # docs가 아니라 nodes를 document로 바꿔서 실행해봐야함. dosc는 청킹되기 전의 문서임.\n",
    "core_index = VectorStoreIndex(\n",
    "    nodes[0],\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    # store_nodes_override=True\n",
    ")\n",
    "core_index.storage_context.persist(persist_dir='./test_Qdrant_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load\n",
    "# client = qdrant_client.QdrantClient(\n",
    "#     path = \"./test_Qdrant\"\n",
    "# )\n",
    "\n",
    "# vector_store = QdrantVectorStore(client=client,\n",
    "#                                  enable_hybrid=True,\n",
    "#                                  collection_name=\"papers\")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "# storage_context = StorageContext.from_defaults(persist_dir='./test_Qdrant')\n",
    "# core_index = VectorStoreIndex.from_vector_store(\n",
    "#     vector_store,\n",
    "#     storage_context=storage_context,\n",
    "#     service_context=service_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id='f663e72f-1fef-4880-bc41-1621ab85a49c' node_type=<ObjectType.TEXT: '1'> metadata={'page_label': '6', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'} hash='df35631e082b009af3ffce83004415564c55db89dcb215c9e9c41a39b093bc86' None\n",
      "\n",
      "node_id='991b787d-69aa-4d6e-928e-9123e52ad3f8' node_type=<ObjectType.TEXT: '1'> metadata={'page_label': '2', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'} hash='99da98d3e220c8daf3e0c285849acff53e6a100d92e1bd69ddb0df0c85931fc2' None\n",
      "\n",
      "None [RelatedNodeInfo(node_id='679815fc-227f-45a8-99d0-b0e03fc52f2f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='878705f0fd10152a81d041310c12ac6a1ace839e9599e7f8e58722432f2d0a3e'), RelatedNodeInfo(node_id='427652ad-51a9-4682-bc4a-7a2898a80f91', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='3e288ca4dccbf2a5be8f1baa4fa474921e4987c9f15db852ad5b795f6106885e'), RelatedNodeInfo(node_id='76f399ea-e4ed-4414-8917-87dffda7847e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='28da5621c1ff1fa2e100ae26a7232526752f50941e3a78038dacb35322aa9e26')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = core_index.as_retriever(similarity_top_k=7, sparse_top_k=3).retrieve('what is llama2?')\n",
    "for i in range(len(res)):    \n",
    "    print(res[i].node.parent_node, res[i].node.child_nodes)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046 [RelatedNodeInfo(node_id='55ed51e7-22b8-4e72-91eb-d6edd2662f58', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='2e5d55dc1d3e465933a3e2f691da9904220dc52ff1a0d1ecce92f9a7aab910fa'), RelatedNodeInfo(node_id='9ffd4149-7228-4f89-a5a9-765a3225a2cc', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='81b7102bf7aead116e872fead2a8d213825e40d41ee5bb19c16afc74bdf81dd2')]\n",
      "1691 [RelatedNodeInfo(node_id='69a41549-436d-4273-8fdb-bed82a4ddb41', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='68d68ab6b4ab91e134eb770bfe6b62d7c99fba8c7614ebb131c2e05aa29d3dec'), RelatedNodeInfo(node_id='257dc603-7b94-44dd-a173-4eac38144dc8', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='517bb571ab694a7088c1513a1e0ad70833490c936091398b4daff57737b49ec9'), RelatedNodeInfo(node_id='6cc39b60-7117-475b-9e03-20f55f7addac', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='a07cc7542b386506209c723804d4e4a96e076e2f60f3f8566365caaceac10e0f')]\n",
      "945 [RelatedNodeInfo(node_id='cbb57568-ed87-4f80-8c7f-d5bd52dcde9f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='f92a65095cfdd0683d0f38d645f3bf12b8ea25af0808cdf3dad147b4b266af23'), RelatedNodeInfo(node_id='f307531c-8d4b-40d5-a316-bdfc682ec108', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='94ddc6a810425390937376c13cbf41c194c545af85decfe68f1c03242b2d6846')]\n",
      "1180 [RelatedNodeInfo(node_id='679815fc-227f-45a8-99d0-b0e03fc52f2f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='878705f0fd10152a81d041310c12ac6a1ace839e9599e7f8e58722432f2d0a3e'), RelatedNodeInfo(node_id='427652ad-51a9-4682-bc4a-7a2898a80f91', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='3e288ca4dccbf2a5be8f1baa4fa474921e4987c9f15db852ad5b795f6106885e'), RelatedNodeInfo(node_id='76f399ea-e4ed-4414-8917-87dffda7847e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='28da5621c1ff1fa2e100ae26a7232526752f50941e3a78038dacb35322aa9e26')]\n",
      "1895 [RelatedNodeInfo(node_id='85332810-d041-4ea6-9924-07512be3b03a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '7', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='78789deff69b8b3a6783a2d8751d050bbf9415a646ad5b3a821d9d2498df4d01'), RelatedNodeInfo(node_id='fbd67563-80ce-4133-b8c8-5e892e450716', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '7', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='cb064a43a202f34424ab4785cd2beb1abbe7264e2f0b3760004a980c30fdff6a'), RelatedNodeInfo(node_id='e23b6136-3b4c-4dc1-958f-ffc0d3b99620', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '7', 'file_name': '2005.11401.pdf', 'file_path': 'data/test/2005.11401.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2024-02-03', 'last_modified_date': '2023-12-17', 'last_accessed_date': '2024-02-03'}, hash='cd64b06395055a70f50b759bf3fbef65aa1c35a95ee6a780321851cf9908b6f7')]\n",
      "\n",
      "1046 None\n",
      "1691 None\n",
      "945 None\n",
      "1180 None\n",
      "1895 None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers import RecursiveRetriever, AutoMergingRetriever\n",
    "retriever_auto = AutoMergingRetriever(\n",
    "    core_index.as_retriever(similarity_top_k=7, sparse_top_k=3), \n",
    "    storage_context=storage_context,\n",
    "    simple_ratio_thresh= 0.1\n",
    ")\n",
    "\n",
    "res = retriever_auto.retrieve('llama2?')\n",
    "\n",
    "for i in range(res.__len__()):\n",
    "    print(len(res[i].text), res[i].node.child_nodes)\n",
    "\n",
    "print()\n",
    "for i in range(res.__len__()):\n",
    "    print(len(res[i].text), res[i].node.parent_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.postprocessor import BaseNodePostprocessor\n",
    "from llama_index.schema import NodeWithScore\n",
    "from collections import defaultdict\n",
    "from typing import List, Optional \n",
    "from llama_index.schema import TextNode\n",
    "\n",
    "class DupNodePostprocessor(BaseNodePostprocessor):\n",
    "    def _postprocess_nodes(\n",
    "        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n",
    "    ) -> List[NodeWithScore]:\n",
    "        # subtracts 1 from the score\n",
    "        node_dict = defaultdict(str)\n",
    "        score_dict = defaultdict(int)\n",
    "        cnt_dict = defaultdict(int)\n",
    "        \n",
    "        for n in nodes:\n",
    "            node_dict[n.metadata['file_name']] += n.get_content()\n",
    "            score_dict[n.metadata['file_name']] += n.score\n",
    "            cnt_dict[n.metadata['file_name']] += 1\n",
    "\n",
    "        node_list = []\n",
    "        # score update\n",
    "        for key in node_dict:\n",
    "            score_dict[key] /= cnt_dict[key]\n",
    "        \n",
    "            node_list.append(NodeWithScore(node=TextNode(\n",
    "                text=node_dict[key],\n",
    "                metadata={'file_name' : key},\n",
    "                score=score_dict[key]\n",
    "                )))\n",
    "        \n",
    "        return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='I\\'m sorry, but I cannot answer the query as there is no information provided in the context about \"llama2\".', source_nodes=[NodeWithScore(node=TextNode(id_='8df2b93a-bd1c-40be-8c8e-b61cad5cfdea', embedding=None, metadata={'file_name': '2005.11401.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBookT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBookREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/ 68.0 45.2 52.2Table 2: Generation and classiﬁcation Test Scores.\\nMS-MARCO SotA is [ 4], FEVER-3 is [ 68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8*49.9*76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.For FEVER [ 56] fact veriﬁcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequence xto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pη(z|x)with parameters ηthat returns (top-K truncated)\\ndistributions over text passages given a query xand (ii) a generator pθ(yi|x,z,y 1:i−1)parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/ . An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2As FEVER is a classiﬁcation task, both RAG models are equivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.475.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.472.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.474.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\\nbetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiﬁcant differences in performance between them.The\\tDivine\\nComedy\\t(x) qQuery\\nEncoder\\nq(x)\\nMIPS p θGenerator \\xa0pθ\\n(Parametric)\\nMargin-\\nalize\\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t (y)End-to-End Backprop through q and\\xa0 p θ\\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii. (x)\\nFact V eriﬁcation: Fact Querysupports \\t(y)\\nQuestion GenerationFact V eriﬁcation:\\nLabel GenerationDocument\\nIndexDefine\\t\"middle\\tear\" (x)\\nQuestion Answering:\\nQuestion QueryThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t (y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη\\n(Non-Parametric)\\nz 4\\nz3\\nz2\\nz 1d(z)\\nJeopardy Question\\nGeneration:\\nAnswer QueryFigure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document\\nIndex ) with a pre-trained seq2seq model ( Generator ) and ﬁne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we\\ntreatzas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'8df2b93a-bd1c-40be-8c8e-b61cad5cfdea': {'file_name': '2005.11401.pdf'}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "d_processor = DupNodePostprocessor(nodes = res)\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever_auto,\n",
    "    node_postprocessors=[d_processor],\n",
    "    # response_synthesizer=response_synthesize\n",
    ")\n",
    "\n",
    "final_res = query_engine.query('what is llama2?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('llamaindex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f13b51dec9893a665284acd155bab776a7e14c668dd25de27de89f99a4571a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
