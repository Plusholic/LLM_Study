{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.chat_engine\n",
    "llama_2_docs = SimpleDirectoryReader(input_files=['./assets/llama2.pdf'], filename_as_id=True).load_data()\n",
    "index = VectorStoreIndex.from_documents(llama_2_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 채팅 컨덴스 모드\n",
    "# openai : OpenAIAgent는 openai chat mode 사용. openai는 아마 대부분 구현되어있어서 따로 뺀듯?\n",
    "# react : 에이전트 기반 채팅 모드(에이전트가 행동할 수 있도록, Tool 사용 등..)\n",
    "# context : 지식 기반 및 일반적인 상호 작용과 직접적으로 관련된 질문에 효과적입니다.\n",
    "# condense_question : 항상 지식 기반에 쿼리하기 때문에 \"내가 전에 무엇을 물어봤나요?\"와 같은 질문에 답변이 어려움\n",
    "# condense_plus_context : 지식 기반 및 일반적인 상호 작용과 직접적으로 관련된 질문에 효과적입니다.\n",
    "\n",
    "# chat_engine = index.as_chat_engine(chat_mode=\"condense_question\") # 성능이 좋지 않네.. 간단한 대화만 가능\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"context\") # 확실히 컨텍스트 기반이므로 효과적임\n",
    "# chat_engine = index.as_chat_engine(chat_mode=\"condense_plus_context\") # 이거 또는 context 사용해야 할 듯\n",
    "# chat_engine = index.as_chat_engine(chat_mode=\"react\") # agent가 없는 상황에서 성능이 좋지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed by a team of researchers. These models range in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs in Llama 2, called Llama 2-Chat, are specifically optimized for dialogue use cases. The goal of Llama 2 is to provide high-performing chat models that can be used as an alternative to closed-source models. The researchers have conducted evaluations and found that Llama 2-Chat outperforms open-source chat models on various benchmarks. The paper also provides detailed information about the approach to fine-tuning and safety improvements in Llama 2-Chat, with the aim of enabling the community to build on their work and contribute to the responsible development of LLMs.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"What is Llama2?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Llama 2 is a project that focuses on developing large language models (LLMs) for chat-based conversational AI. The researchers behind Llama 2 have created a collection of pretrained and fine-tuned LLMs, with sizes ranging from 7 billion to 70 billion parameters.\n",
      "\n",
      "The main objective of Llama 2 is to provide high-quality and safe chat models that can be used as an alternative to closed-source models. The researchers have fine-tuned the LLMs specifically for dialogue-based tasks, resulting in models called Llama 2-Chat.\n",
      "\n",
      "To evaluate the performance of Llama 2-Chat, the researchers conducted human evaluations. Annotators were presented with pairs of model responses and asked to determine which response was better in terms of being helpful, safe, and honest. The evaluations showed that Llama 2-Chat outperformed open-source chat models on various metrics.\n",
      "\n",
      "The Llama 2-Chat models also incorporate safety improvements. The researchers have made efforts to mitigate harmful and untruthful outputs by using a combination of rule-based filtering, reinforcement learning from human feedback, and a Moderation API. These safety measures aim to ensure that the models provide helpful and responsible responses.\n",
      "\n",
      "The Llama 2 project aims to contribute to the responsible development of LLMs by providing a strong baseline for chat models and sharing insights into fine-tuning techniques and safety improvements. The researchers hope that the community can build upon their work and continue to advance the field of conversational AI in a responsible manner.\n",
      "\n",
      "If you have any specific questions or would like more details about any aspect of Llama 2, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Can you tell me more?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before asking for more information about Llama 2, you asked, \"What is Llama2?\"\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"What did I ask you right before?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리셋 하면 앞의 대화내용이 다 사라짐\n",
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! In the given context, the prompt is about a woman with strong opinions about pizza. She believes that Chicago-style pizza is the best and that pizza should never be folded. She also thinks that pineapples on pizza are an abomination. The prompt suggests going to grab a slice of pizza after work and asks if the user wants to join.\n",
      "\n",
      "The safety data provided shows different responses generated by the model at different percentages of safety. The responses vary in terms of expressing the woman's strong opinions about pizza, her preference for Chicago-style pizza, her dislike for folding pizza, and her disdain for pineapples on pizza. The responses also convey her willingness to go out for pizza and her desire to find a place that serves real Chicago-style pizza.\n",
      "\n",
      "The responses aim to engage the user in a conversation about pizza preferences and potentially plan a pizza outing. The model provides different variations of the response while maintaining the woman's strong opinions and preferences.\n"
     ]
    }
   ],
   "source": [
    "# 환각이 생김..\n",
    "response = chat_engine.chat(\"Can you tell me more?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('llamaindex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f13b51dec9893a665284acd155bab776a7e14c668dd25de27de89f99a4571a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
