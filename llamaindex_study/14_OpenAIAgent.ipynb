{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader, VectorStoreIndex, load_index_from_storage\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "PDFReader = download_loader('PDFReader')\n",
    "loader = PDFReader()\n",
    "\n",
    "class PaperTitle:\n",
    "    def __init__(self, name, about, file, key):\n",
    "        self.name = name\n",
    "        self.about = about\n",
    "        self.file = file\n",
    "        self.key = key\n",
    "        \n",
    "papers_titles = [\n",
    "    PaperTitle('llama2', 'llama2 the name of large language models by metaAI', 'llama2.pdf', 'tools_for_llama2'),\n",
    "    PaperTitle('LLM on Graph','Large Language Models using on Graph Structure','LLM_on_Graphs_A_Comprehensive_Survey.pdf', 'tools_for_GNN_or_recomendation_system')\n",
    "]\n",
    "\n",
    "paper_vector_index = {}\n",
    "for paper in papers_titles:\n",
    "    try:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=f\"./storage/cache/{paper.key}_vector\")\n",
    "        paper_vector_index[paper.key] = load_index_from_storage(storage_context)\n",
    "    except:\n",
    "        documents = loader.load_data(file=Path(f\"./assets/{paper.file}\"))\n",
    "        vector_index = VectorStoreIndex.from_documents(documents)\n",
    "        paper_vector_index[paper.key] = vector_index\n",
    "        vector_index.storage_context.persist(persist_dir=f\"./storage/cache/{paper.key}_vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tools_for_llama2': <llama_index.indices.vector_store.base.VectorStoreIndex at 0x1754fcd30>,\n",
       " 'tools_for_GNN_or_recomendation_system': <llama_index.indices.vector_store.base.VectorStoreIndex at 0x280d93b50>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_vector_index # 각 paper에 대해서 vector index가 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import KeywordNodePostprocessor\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "node_processor = KeywordNodePostprocessor(\n",
    "    exclude_keywords=[\"commecial\"]\n",
    ")\n",
    "\n",
    "query_engine_tools = []\n",
    "paper_vector_engines = {}\n",
    "\n",
    "for paper in papers_titles:\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index = paper_vector_index[paper.key],\n",
    "        similarity_top_k = 3,\n",
    "    )\n",
    "    paper_vector_engines[paper.key] = RetrieverQueryEngine(\n",
    "        retriever = retriever,\n",
    "        node_postprocessors = [node_processor]\n",
    "    )\n",
    "    new_tool = QueryEngineTool(\n",
    "        query_engine=paper_vector_engines[paper.key],\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"{paper.key}_vector_tool\",\n",
    "            description=f\"Useful for retrieving specific context from a paper {paper.name}. \"\n",
    "            f\"Use When you need information related to {paper.about}\"\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    query_engine_tools.append(new_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tools_for_llama2': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x2816f5f90>,\n",
       " 'tools_for_GNN_or_recomendation_system': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x2816f5de0>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_vector_engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "agent = OpenAIAgent.from_tools(query_engine_tools, llm=OpenAI(temperature=0, model='gpt-3.5-turbo-0613'), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about llama 2?\n",
      "=== Calling Function ===\n",
      "Calling function: tools_for_llama2_vector_tool with args: {\n",
      "  \"input\": \"What are the key features of llama 2?\"\n",
      "}\n",
      "Got output: Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) that range in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. These models have demonstrated competitiveness with existing open-source chat models and have competency equivalent to some proprietary models on evaluation sets. Llama 2 uses an optimized transformer architecture and employs supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. The models were trained between January 2023 and July 2023 and are intended for commercial and research use in English. The pretrained models can be adapted for a variety of natural language generation tasks, while the tuned models are specifically designed for assistant-like chat. Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources and includes fine-tuning data from publicly available instruction datasets and over one million new human-annotated examples. The model developers have emphasized the principles of helpfulness and safety in the development of Llama 2.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat('Tell me about llama 2?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed by metaAI. Here are some key features of LLAMA 2:\n",
      "\n",
      "1. Scale: LLAMA 2 consists of LLMs ranging in scale from 7 billion to 70 billion parameters. The larger models have more capacity to capture complex language patterns and generate high-quality text.\n",
      "\n",
      "2. Fine-tuned for Dialogue: LLAMA 2 includes fine-tuned LLMs called LLAMA 2-Chat, which are specifically optimized for dialogue-based use cases. These models have been trained to generate contextually relevant and coherent responses in conversations.\n",
      "\n",
      "3. Competitiveness: LLAMA 2-Chat models have demonstrated competitiveness with existing open-source chat models and are comparable to some proprietary models on evaluation sets. This means that LLAMA 2 can provide high-quality dialogue generation.\n",
      "\n",
      "4. Optimized Transformer Architecture: LLAMA 2 utilizes an optimized transformer architecture, which is a state-of-the-art neural network architecture for natural language processing tasks. This architecture allows LLAMA 2 to effectively model and generate text.\n",
      "\n",
      "5. Training Approach: LLAMA 2 models are trained using a combination of supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). This training approach helps align the models with human preferences for helpfulness and safety.\n",
      "\n",
      "6. Data Sources: LLAMA 2 was pretrained on a massive amount of text data, including 2 trillion tokens from publicly available sources. The fine-tuning data includes examples from publicly available instruction datasets and over one million new human-annotated examples.\n",
      "\n",
      "7. Commercial and Research Use: LLAMA 2 models are intended for commercial and research use in English. They can be adapted for a variety of natural language generation tasks, including chat-based applications.\n",
      "\n",
      "8. Emphasis on Helpfulness and Safety: The development of LLAMA 2 models has prioritized the principles of helpfulness and safety. This ensures that the generated text is not only accurate and coherent but also aligns with ethical considerations.\n",
      "\n",
      "Overall, LLAMA 2 is a powerful language model that can generate high-quality text and is suitable for a wide range of natural language processing tasks, particularly in dialogue-based applications.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
