{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 쿼리할 때 마다 인덱스를 다시 생성해야 함 -> 인덱스를 디스크에 저장하여 다음에 실행할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.42.post1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import llama_index\n",
    "llama_index.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\").encode,\n",
    "    verbose = True\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
    "set_global_service_context(service_context) # global context로 설정해야 index를 load할 때에도 tiktoken이 작동함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from disk\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "\n",
    "# 인덱스가 존재하는지 확인하고, 없을 때만 다시 빌드\n",
    "# 기본 저장소는 kvstore(Key - Value store)\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir='./storage/cache/papers/llama2/')\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    print('loading from disk')\n",
    "except:\n",
    "    documents = SimpleDirectoryReader('assets').load_data()\n",
    "    # 노드 파싱, 임베딩\n",
    "    index = VectorStoreIndex.from_documents(documents=documents)\n",
    "    # 인덱스를 디스크에 지속적으로 가지고 있음\n",
    "    index.storage_context.persist(persist_dir='./storage/cache/papers/llama2/')\n",
    "    print('persisting from disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug로 설정하면 OpenAI에 어떤 프롬프트가 보내졌는지 볼 수 있음\n",
    "# import openai\n",
    "# openai.log = 'debug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "text_qa_template_str = (\n",
    "    # default template\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------------\\n\"\n",
    "    \"Using both the context information and also using your own knowledge, \"\n",
    "    \"answer the question : {query_str}\\n\"\n",
    "    \"If the context isn't helpful, you can also answer the question on your own.\\n\"   \n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "add_system_qa_template_str = (\n",
    "    # system message 추가\n",
    "    \"You are author of llama2\"\n",
    "    \"Always answer the query only using the provided context information,\"\n",
    "    \"and not prior knowledge.\\n\"\n",
    "    \"Some rules to follow:\\n\"\n",
    "    \"1. Never directly reference the given context in your answer.\\n\"\n",
    "    \"2. Avoid statements like 'Based on the context, ...' or\"\n",
    "    \"'The context information ...' or anything along \"\n",
    "    \"those lines.\"\n",
    "    \n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------------\\n\"\n",
    "    \"Answer the question : {query_str}\\n\"\n",
    ")\n",
    "add_system_qa_template = PromptTemplate(text_qa_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 5\n",
      "LLM Prompt Token Usage: 1118\n",
      "LLM Completion Token Usage: 132\n",
      "Embedding Token Usage: 5\n",
      "LLM Prompt Token Usage: 1118\n",
      "LLM Completion Token Usage: 105\n"
     ]
    }
   ],
   "source": [
    "response_1 = index.as_query_engine(\n",
    "    text_qa_template=text_qa_template\n",
    "    ).query('What is llama2?')\n",
    "\n",
    "response_2 = index.as_query_engine(\n",
    "    text_qa_template=add_system_qa_template\n",
    "    ).query('What is llama2?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed by the authors of the paper. These models range in scale from 7 billion to 70 billion parameters. The authors specifically mention Llama 2-Chat, which is optimized for dialogue use cases. The models have demonstrated competitiveness with existing open-source chat models and are considered to be a suitable substitute for closed-source models based on evaluations for helpfulness and safety. The authors provide a detailed description of their approach to fine-tuning and safety improvements in order to enable the community to build on their work and contribute to the responsible development of LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. These models, called Llama 2-Chat, are optimized for dialogue use cases and have demonstrated competitiveness with existing open-source chat models. The authors of Llama 2 have provided a detailed description of their approach to fine-tuning and safety improvements, aiming to enable the community to build on their work and contribute to the responsible development of LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=4,\n",
    ")\n",
    "s_processor = SimilarityPostprocessor(similarity_cutoff=0.5) # 스코어 컷오프 지정\n",
    "k_processor = KeywordNodePostprocessor(\n",
    "    exclude_keywords=['cummecial'],\n",
    "    # required_keywords=['llama2']\n",
    ")\n",
    "# 디버깅을 위해서 llm으로 보내지 않음(토큰 사용 x) 쿼리 엔진을 실행하면 final answer가 나오지 않음.\n",
    "response_synthesize = get_response_synthesizer(\n",
    "    response_mode='no_text' \n",
    ")\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[k_processor, s_processor],\n",
    "    # response_synthesizer=response_synthesize,\n",
    "    callback_manager = callback_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 5\n",
      "LLM Prompt Token Usage: 2982\n",
      "LLM Completion Token Usage: 134\n",
      "Final Response: Llama 2 is a collection of pretrained and fine-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70\n",
      "billion parameters. These models, called Llama 2-Chat, are optimized\n",
      "for dialogue use cases and have demonstrated competitiveness with\n",
      "existing open-source chat models. They are considered to be a suitable\n",
      "substitute for closed-source models in terms of helpfulness and\n",
      "safety. Llama 2 is made available for both research and commercial\n",
      "use, and developers must comply with the terms of the provided license\n",
      "and the Acceptable Use Policy. The responsible release of Llama 2 aims\n",
      "to encourage responsible AI innovation and collaboration within the AI\n",
      "community.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 72090ff0-1d79-4c79-a087-8929920a6a24\n",
      "Similarity: 0.8383467706663337\n",
      "Text: (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs,\n",
      "with concerns ranging from privacy to misleading expertise claims.\n",
      "Deng et al. (2023) proposes a taxonomic framework to tackle these\n",
      "issues, and Bergman et al. (2022) delves into the balance between\n",
      "potential positive and negative impacts from releasing dialogue\n",
      "models. Investigati...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 7dea5a00-36cf-402c-863e-8d7d2b565efe\n",
      "Similarity: 0.8335203294602297\n",
      "Text: Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo\n",
      "Touvron∗Louis Martin†Kevin Stone† Peter Albert Amjad Almahairi Yasmine\n",
      "Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale\n",
      "Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem\n",
      "Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao ...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: adfc8491-d549-4224-8fe3-0075d48592e9\n",
      "Similarity: 0.8217909875886006\n",
      "Text: A.7 Model Card Table 52 presents a model card (Mitchell et al.,\n",
      "2018; Anil et al., 2023) that summarizes details of the models. Model\n",
      "Details Model Developers Meta AI Variations Llama 2 comes in a range\n",
      "of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-\n",
      "tuned variations. Input Models input text only. Output Models generate\n",
      "text o...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 0c81432d-6a17-4546-93a2-54c197bdb012\n",
      "Similarity: 0.8198384204643145\n",
      "Text: NoteveryonewhousesAImodelshasgoodintentions,andconversationalAIa\n",
      "gentscouldpotentiallybe usedfornefariouspurposessuchasgeneratingmisinf\n",
      "ormationorretrievinginformationabouttopicslike bioterrorism or\n",
      "cybercrime. We have, however, made efforts to tune the models to avoid\n",
      "these topics and diminish any capabilities they might have offered for\n",
      "those us...\n",
      "embedding tokens : 5 \n",
      " LLM prompts : 2982 \n",
      " LLM completions : 134 \n",
      " Total LLM token count : 3116 \n",
      "\n",
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. These models, called Llama 2-Chat, are optimized for dialogue use cases and have demonstrated competitiveness with existing open-source chat models. They are considered to be a suitable substitute for closed-source models in terms of helpfulness and safety. Llama 2 is made available for both research and commercial use, and developers must comply with the terms of the provided license and the Acceptable Use Policy. The responsible release of Llama 2 aims to encourage responsible AI innovation and collaboration within the AI community.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "\n",
    "token_counter.reset_counts()\n",
    "response = query_engine.query('what is llama2?')\n",
    "pprint_response(response, show_source=True) # 보여주기만 하는 기능\n",
    "\n",
    "print('embedding tokens :', token_counter.total_embedding_token_count, '\\n',\n",
    "      'LLM prompts :', token_counter.prompt_llm_token_count, '\\n',\n",
    "      'LLM completions :', token_counter.completion_llm_token_count, '\\n',\n",
    "      'Total LLM token count :', token_counter.total_llm_token_count, '\\n',\n",
    ")\n",
    "print(response) # load 했기 때문에 token을 0개 사용함..? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='ff89be49-96f5-4ad9-b2a4-0f07fed644cd', embedding=None, metadata={'page_label': '36', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6fa2b14b-261f-43bb-8201-1192e3dd2c8b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '36', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, hash='f9a40705e297dcf37fb06df092a9a1cd009326f95eaa7b8d645855e749cbb1db'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='38eb4902-6abd-4e3a-a465-bdb811fb5d19', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '36', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, hash='014593f135f68b04016b6d6967121dc7d160b16b8505d13b49df8aeb94c8db18'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0e3870e8-17b1-48b1-8c54-22beea968038', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2ceac0d433db98ab106ff338ec2d1e2c71f2bd5ba931567ad904ba98d73d1c1e')}, text='(2021)alsoilluminatesthedifficultiestiedtochatbot-oriented\\nLLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes\\na taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between\\npotential positive and negative impacts from releasing dialogue models.\\nInvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022)\\nand Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of\\nharmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also\\nraisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike\\nbiological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an\\nover-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu\\nandRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto\\ncontinuing our work engaging with the broader policy, academic, and industry community on these issues.\\n7 Conclusion\\nInthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales\\nof7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting\\nopen-source chat models, as well as competency that is equivalent to some proprietary models on evaluation\\nsetsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe\\nmethodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe\\nprinciplesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch,\\nwehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat . Aspartofourongoingcommitmentto\\ntransparency and safety, we plan to make further improvements to Llama 2-Chat in future work.\\n36', start_char_idx=2918, end_char_idx=4837, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8378943080728823), NodeWithScore(node=TextNode(id_='6a0192c9-f1cd-4b82-aa68-d2dcfef9ce19', embedding=None, metadata={'page_label': '1', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8488eff8-bcb2-4fd0-bf17-aadf59c00815', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, hash='9d32ba117cf648d83b0155c15a123b4f245fcbebe3b9405ef1f66c7b3772c837'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bb9c0922-b0ec-475c-8a1f-d2251dfd66a5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2943feba461bcc0a8827ff332f9eb64520303e271d9b6aa7269e9231d281aac6')}, text='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023', start_char_idx=0, end_char_idx=1880, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8332349740166617), NodeWithScore(node=TextNode(id_='6487475f-9271-40d1-82b6-8000f28cf355', embedding=None, metadata={'page_label': '77', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8a8df8f2-65c6-42d6-9e19-d1d0be825674', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '77', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, hash='61bb88e16b68c1b0712d48b74ccd63cb61d297319944074a09fed71ed39016f0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='273aa9d9-d346-4376-b8e4-c4d936fd32a1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '76', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, hash='69e992e9bc429bbf7ad098d9b34a64c7f5e18d0c01810563f5371409855a8863')}, text='A.7 Model Card\\nTable 52 presents a model card (Mitchell et al., 2018; Anil et al., 2023) that summarizes details of the models.\\nModel Details\\nModel Developers Meta AI\\nVariations Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as\\npretrained and fine-tuned variations.\\nInput Models input text only.\\nOutput Models generate text only.\\nModel Architecture Llama 2 isanauto-regressivelanguagemodelthatusesanoptimizedtransformer\\narchitecture. Thetunedversionsusesupervisedfine-tuning(SFT)andreinforce-\\nmentlearning withhuman feedback(RLHF)to aligntohuman preferencesfor\\nhelpfulness and safety.\\nModel Dates Llama 2 was trained between January 2023 and July 2023.\\nStatus This is a static model trained on an offline dataset. Future versions of the tuned\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send com-\\nmentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nLlama 2.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproduc-\\ntionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\\nperformed on third-party cloud compute.\\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware\\nof type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO 2eq, 100% of which were offset by Meta’s sustainability program.\\nTraining Data (Sections 2.1 and 3)\\nOverview Llama 2 was pretrained on 2 trillion tokens of data from publicly available\\nsources. The fine-tuning data includes publicly available instruction datasets, as\\nwellasoveronemillionnewhuman-annotatedexamples. Neitherthepretraining\\nnor the fine-tuning datasets include Meta user data.\\nData Freshness The pretraining data has a cutoff of September 2022, but some tuning data is\\nmore recent, up to July 2023.\\nEvaluation Results\\nSee evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations (Section 5.2)\\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\\napplications of Llama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card for Llama 2 .\\n77', start_char_idx=0, end_char_idx=3398, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8213534966769586), NodeWithScore(node=TextNode(id_='a5bb7a11-af2e-4565-8100-464a90700ca4', embedding=None, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='89a96d31-ab66-449f-abc0-f8261b105453', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '35', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, hash='5feddf161c9b4220ecd18b0166b74fd10541f4152dc2e82bb6fc16bfaeba8710'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e651ef79-e04d-4c52-90e3-826bc5e4b437', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '34', 'file_name': 'llama2.pdf', 'file_path': 'assets/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-02-04', 'last_modified_date': '2023-12-16', 'last_accessed_date': '2024-02-04'}, hash='fbebd9897b4e3a21e466c66fcddc8ca77f7be7141ece0c179793c7c0870f2aa3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='944615a7-c289-4688-a330-7207ece68d26', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f1251f1541dd8d13148d4dd1a25bfec55abdf2d480b0b5a596560c4202c9c435')}, text='NoteveryonewhousesAImodelshasgoodintentions,andconversationalAIagentscouldpotentiallybe\\nusedfornefariouspurposessuchasgeneratingmisinformationorretrievinginformationabouttopicslike\\nbioterrorism or cybercrime. We have, however, made efforts to tune the models to avoid these topics and\\ndiminish any capabilities they might have offered for those use cases.\\nWhile we attempted to reasonably balance safety with helpfulness, in some instances, our safety tuning goes\\ntoo far. Users of Llama 2-Chat may observe an overly cautious approach, with the model erring on the side\\nof declining certain requests or responding with too many safety details.\\nUsersofthepretrainedmodelsneedtobeparticularlycautious,andshouldtakeextrastepsintuningand\\ndeployment as described in our Responsible Use Guide.§§\\n5.3 Responsible Release Strategy\\nReleaseDetails. Wemake Llama 2 availableforbothresearchandcommercialuseat https://ai.meta.\\ncom/resources/models-and-libraries/llama/ . Thosewhouse Llama 2 mustcomplywiththetermsof\\nthe provided license and our Acceptable Use Policy , which prohibit any uses that would violate applicable\\npolicies, laws, rules, and regulations.\\nWealsoprovidecodeexamplestohelpdevelopersreplicateoursafegenerationswith Llama 2-Chat and\\napplybasicsafetytechniquesattheuserinputandmodeloutputlayers. Thesecodesamplesareavailable\\nhere: https://github.com/facebookresearch/llama . Finally,wearesharinga ResponsibleUseGuide ,which\\nprovides guidelines regarding safe development and deployment.\\nResponsibleRelease. WhilemanycompanieshaveoptedtobuildAIbehindcloseddoors,wearereleasing\\nLlama 2 openly to encourage responsible AI innovation. Based on our experience, an open approach draws\\nuponthecollectivewisdom,diversity,andingenuityoftheAI-practitionercommunitytorealizethebenefitsof\\nthistechnology. Collaborationwillmakethesemodelsbetterandsafer. TheentireAIcommunity—academic\\nresearchers, civil society, policymakers, and industry—must work together to rigorously analyze and expose\\nthe risks of current AI systems and to build solutions that address potentially problematic misuse. This\\napproachnotonlyfostersrealcollaborationwithdiversestakeholders—thosebeyondthewallsofbigtech\\ncompanies—but also serves as the cornerstone for democratizing access to foundational models. As argued\\nin Zellers et al. (2019b), open releases promote transparency and allow more people to access AI tools,\\ndemocratizingthetechnologyanddecentralizingAIexpertise. WebelievethatthedecentralizationofAI\\nexpertisedoesmorethansimplydistributeknowledge—itstimulatesinnovationandacceleratesprogress\\nin the industry. Lastly, openly releasing these models consolidates costs and eliminates barriers to entry,\\nallowingsmallbusinessestoleverageinnovationsinLLMstoexploreandbuildtext-generationusecases.\\nUltimately, we believe this will create a more level playing field for organizations of all sizes across the globe\\nto benefit from the economic growth promised by the advancement of AI.\\nWe know that not everyone who uses AI models has good intentions, and we acknowledge that there\\nare reasonable concerns regarding the ways that AI will impact our world. Toxic content generation and\\nproblematic associations are meaningful risks that the AI community has yet to fully mitigate. As this\\npaper illustrates, we have made strides in limiting the prevalence of these types of responses. While we\\nrecognize there is more work to be done, this realization only deepens our commitment to open science and\\ncollaboration with the AI community.\\n6 Related Work\\nLarge Language Models. The recent years have witnessed a substantial evolution in the field of LLMs.\\nFollowing the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B\\nparameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized\\nmodels, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al.,\\n2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in\\nthisprogressionistheriseofLlama,recognizedforitsfocusoncomputationalefficiencyduringinference\\n(Touvron et al., 2023).', start_char_idx=0, end_char_idx=4162, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8196817130317882)]\n"
     ]
    }
   ],
   "source": [
    "# 그냥 응답의 모든 정보를 보려면\n",
    "print(response.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('llamaindex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f13b51dec9893a665284acd155bab776a7e14c668dd25de27de89f99a4571a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
