# **Abstract**

> 본 연구에서는 LLaVA의 완전 연결된 Vision-Language Cross-Modal Connector가 매우 강력하고 데이터 효율적이라는 것을 보여줌. LLaVA에 간단한 수정, 즉 CLIP-ViT-L-336px와 MLP projection을 사용하고 간단한 응답 형식 프롬프트로 Academic-Task-Oriented VQA 데이터를 추가하면, 11개의 벤치마크에서 최첨단 성능을 달성하는 강력한 기준점을 설정할 수 있음. 최종 13B 체크포인트는 단 120만 개의 공개 데이터만을 사용하며, 단일 8-A100 노드에서 약 하루 만에 전체 훈련을 완료함.
> 

# **Instruction**

최근 다중모달 모델(LMM)은 일반 목적의 어시스턴트로서 큰 관심을 받고 있으며, 시각적 지시 조정(Visual Instruction Tuning)이 주요 개념으로 부각되고 있다. LLaVA 및 MiniGPT-4 같은 모델들이 자연스러운 지시 수행과 시각적 추론 능력에서 뛰어난 성과를 보이고 있다. 여러 연구는 사전 훈련 데이터, 지시 수행 데이터, 시각 인코더, 언어 모델을 확장하여 성능을 개선하고 있다.

LLaVA 아키텍처는 시각적 지시 조정에 있어 강력하고 데이터 효율적인 모델을 구축하는데 초점을 맞추고 있으며, 단순한 수정과 학문적 과제 지향 VQA 데이터를 포함하여 성능을 크게 향상시켰다. 이 연구는 InstructBLIP 및 Qwen-VL과 같은 모델들과 달리, LLaVA는 간단한 아키텍처 설계를 사용하여 훨씬 적은 데이터로 훈련이 가능함을 보여준다.

이 연구는 LLaVA의 아키텍처가 시각적 지시 조정에 강력하며, 단순한 MLP 교차 모달 커넥터와 학문적 과제 지향 데이터를 통합하는 것이 멀티모달 이해 능력을 향상시키는 데 효과적임을 보여준다. 이 모델은 단일 8-A100 기계에서 약 하루 만에 훈련을 완료하며, 다양한 벤치마크에서 최첨단 성능을 달성한다. 공개적으로 이용 가능한 데이터를 사용하여 연구가 재현 가능하고 접근 가능하도록 하는 것을 목표로 한다.

# **Background**

다중모달 지시 수행 모델(LMMs)은 시각적 기능을 인코딩하기 위한 사전 훈련된 시각적 백본, 사용자 지시를 이해하고 응답을 생성하는 사전 훈련된 대형 언어 모델(LLM), 그리고 시각적 인코더 출력을 언어 모델에 맞추는 시각-언어 교차 모달 커넥터로 구성된다. LLaVA는 이러한 모델 중 가장 간단한 아키텍처를 가지고 있으며, 시각적 패치 수를 줄이기 위해 Qformer와 같은 시각적 리샘플러를 사용하기도 한다.

지시 수행 LMM의 훈련은 두 단계로 진행된다. 첫째, 시각-언어 정렬 사전 훈련 단계에서는 이미지-텍스트 쌍을 사용하여 시각적 기능을 언어 모델의 단어 임베딩 공간에 맞춘다. 최근 연구는 더 많은 이미지-텍스트 쌍을 사용하여 성능을 극대화하고 있다. 둘째, 시각적 지시 조정 단계에서는 모델을 시각적 콘텐츠를 포함한 다양한 사용자 지시를 따를 수 있도록 조정한다.

LLaVA는 텍스트 전용 GPT-4를 활용하여 기존 COCO 데이터셋을 다중모달 지시 수행 데이터셋으로 확장했다. InstructBLIP 같은 모델은 학문적 과제 지향 VQA 데이터셋을 추가하여 모델의 시각적 능력을 향상시켰다. 그러나 이러한 데이터 병합은 모델이 VQA 데이터셋에 과적합되어 자연스러운 대화에 참여하는 능력이 저하될 수 있다. LLaVA 파이프라인은 VQA 데이터셋을 대화형 스타일로 변환하여 이 문제를 해결했지만, 데이터 확장에 추가적인 복잡성을 도입했다.

# **Improved Baselines of LLaVA**

**개요**

- LLaVA는 시각적 지시 조정에서 뛰어난 능력을 보이며, 간단한 수정으로 성능을 크게 향상시킬 수 있다.
- MLP 교차 모달 커넥터와 학문적 과제 관련 VQA 데이터를 추가하면 LLaVA의 멀티모달 이해 능력이 향상된다.
- 이 모델은 단순한 아키텍처와 적은 데이터로도 최첨단 성능을 달성하며, 재현 가능하고 접근 가능한 기준점을 제공한다.

**응답 형식 프롬프트**

- LLaVA의 VQA 데이터셋이 짧은 형태의 답변을 요구하는 이유는 응답 형식 프롬프트가 모호하기 때문이다.
- "질문: {Question} 응답: {Answer}"와 같은 형식은 명확한 출력 형식을 제공하지 않아, 자연스러운 대화에서 짧은 답변으로 과적합될 수 있다.
- 이를 해결하기 위해 단일 단어 또는 구로 답변하도록 지시하는 프롬프트를 추가하여, LLaVA가 사용자 지시에 따라 출력 형식을 조정할 수 있도록 했다.

**MLP 시각-언어 커넥터**

- 자기지도 학습에서 MLP를 사용하여 성능을 개선한 것처럼, LLaVA의 시각-언어 커넥터를 2계층 MLP로 개선하면 멀티모달 성능이 향상된다.

**학문적 과제 지향 데이터**

- 학문적 과제 지향 VQA 데이터셋을 추가하여 모델의 다양한 능력을 강화했다.
- OKVQA, OCR, 지역 수준 인식 데이터셋을 추가하여 LLaVA의 성능을 개선했다.

**추가 확장**

- 입력 이미지 해상도를 높여 모델이 이미지의 세부사항을 명확히 볼 수 있도록 했고, GQA 데이터셋을 추가하여 시각적 지식을 보강했다.
- ShareGPT 데이터를 통합하고 LLM을 13B로 확장했다.
- 최종 모델 LLaVA-1.5는 모든 수정 사항을 적용하여 성능을 크게 향상시켰다.

이 연구는 LLaVA 아키텍처의 강력함과 데이터 효율성을 보여주며, 간단한 수정으로 멀티모달 이해 능력을 크게 향상시킬 수 있음을 입증했다.

Discussion 요약해줘

ChatGPT

# **Discussion**

**최신 기술과의 비교**

- LLaVA-1.5는 12개의 벤치마크에서 테스트되었으며, 11개에서 최고 성능을 기록하고 나머지 하나에서도 2위를 차지했다.
- 다른 방법들에 비해 훨씬 적은 데이터와 컴퓨팅 리소스를 사용하면서도 뛰어난 성능을 보여준다.
- 이 연구는 시각적 지시 조정이 LMM의 성능 향상에 중요한 역할을 한다는 것을 시사하며, 대규모 사전 훈련이 필수적이라는 기존의 믿음에 의문을 제기한다.

**제로샷 형식 지시 일반화**

- LLaVA-1.5는 제한된 형식 지시만으로도 다른 형식 지시를 일반화할 수 있다.
- VizWiz 데이터셋에서는 주어진 콘텐츠로 답변할 수 없을 때 "답변 불가"를 출력하는 형식을 효과적으로 수행한다.

**제로샷 다국어 능력**

- LLaVA-1.5는 다국어 멀티모달 지시 수행을 위한 추가적인 미세 조정을 하지 않았음에도 불구하고 다국어 지시를 따를 수 있다.
- MMBench-CN에서 Qwen-VL-Chat보다 우수한 성능을 보였다.

**컴퓨팅 비용**

- LLaVA-1.5는 LLaVA와 동일한 사전 훈련 데이터셋을 사용하고, 훈련 반복과 배치 크기를 비슷하게 유지한다.
- 이미지 입력 해상도가 증가함에 따라 훈련 시간은 약 두 배가 되며, 8개의 A100을 사용하여 사전 훈련에 약 6시간, 시각적 지시 조정에 약 20시간이 소요된다.

**제한 사항**

- LLaVA는 전체 이미지 패치를 사용하여 각 훈련 반복이 길어질 수 있다.
- 다중 이미지 처리가 불가능하며, 복잡한 문제 해결 능력은 특정 도메인에서 제한될 수 있다.
- LLaVA-1.5는 환각(hallucination)을 줄였지만 여전히 발생할 수 있으므로 중요한 응용 분야에서 주의가 필요하다.

**감사의 글**

- 이 연구는 NSF CAREER IIS2150012와 한국 정부의 지원을 받았다.

LLaVA-1.5는 간단한 아키텍처와 적은 데이터로 최첨단 성능을 달성하며, 재현 가능하고 접근 가능한 기준점을 제공한다. 이는 시각적 지시 조정의 중요성을 강조하고, 대규모 사전 훈련이 필수적이지 않음을 시사한다.