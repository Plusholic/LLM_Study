
# 3 LINE SUMMARY

# Abstract

> Low-Rank Adaptation(LoRA)은 전통적인 파인튜닝 방법에서 요구되는 모든 매개변수를 재학습하는 필요성을 크게 줄임. 사전 훈련된 모델의 가중치를 freeze 하고 Transformer 계층에 학습 가능한 Low Rank Matrix를 주입함으로써, LoRA는 파인튜닝된 GPT-3 모델과 비교할 때 학습 가능한 매개변수의 수를 최대 10,000배, GPU 메모리 요구 사항을 세 배 감소시킴. 학습 가능한 매개변수가 적음에도 불구하고 LoRA는 추가적인 추론 지연 없이 여러 NLP 작업 및 모델(RoBERTa, DeBERTa, GPT-2, GPT-3)에서 비교적 우수하거나 더 나은 품질을 보임.
> 

https://github.com/microsoft/LoRA.

# Introduction

> 대규모 언어 모델들이 자연어 처리 분야에서 다양한 Downstream Task로의 적응이 일반적으로 파인튜닝을 통해 이루어짐. 이 과정에서 모든 매개변수를 업데이트하는 것이 주된 방법이었으나 이 방식은 모델이 커질수록 많은 리소스를 요구하고, GPT-3처럼 1750억 개의 매개변수를 가진 모델의 경우 매우 비실용적.
> 
- 연구자들은 모델의 일부 매개변수만을 조정하거나, 새로운 태스크에 맞춰 학습하는 외부 모듈을 사용하는 등의 방법을 모색함.
- 이는 배포시 필요한 저장 공간과 로드 시간을 줄이는데 도움을 줄 수 있으나, 기존 기술들은 대부분 모델의 품질을 저하시키거나 추론 지연을 야기하는 문제점.
- 학습된 over-parametrized models가 실제로는 낮은 내재적 차원에 존재한다는 것을 보여주는 Li 등(2018a)과 Aghajanyan 등(2020)에서 영감을 받음.
    - 모델 적응 중 가중치의 변화도 '내재적 순위'가 낮다는 가설을 세우고, 이에 따라 저순위 적응(LoRA) 접근법을 제안했습니다.
    - 내재적 순위가 낮다는 것은 신경망의 특정 부분이나 레이어에서 발생하는 변화가 비교적 적다는 의미
    - 신경망 전체의 복잡도나 크기에 비해 해당 부분의 변화에 필요한 정보의 양이 적다는 것을 나타냄
- 이에 저자들은 Low-Rank Adaptation, 즉 LoRA 방법을 제안. 이는 트랜스포머 아키텍처의 각 계층에 저랭크 분해 행렬을 주입하는 방식으로, 사전 훈련된 가중치는 고정하고, 변경될 가중치만을 학습하여 매개변수 수를 현저히 줄일 수 있음.
- LoRA를 사용하면 그림 1과 같이 미리 학습된 가중치를 고정된 상태로 유지하면서 대신 Adaptation 중 고밀도 계층의 변화에 대한 rank decomposition matrix를 최적화하여 신경망의 일부 고밀도 계층을 간접적으로 학습할 수 있음.
- GPT-3 175B를 예로 들면, 전체 랭크(즉, d)가 12,288에 달하는 경우에도 매우 낮은 랭크(그림 1의 r은 1~2개일 수 있음)로도 충분하므로 LoRA는 저장 및 컴퓨팅 효율이 모두 높음.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/eb380ee5-a7cf-4b9d-910e-4c7aa80a0428/Untitled.png)

# Problem Statement

> 주된 문제는 대규모 사전 훈련된 언어 모델, 특히 GPT-3와 같은 모델을 다양한 다운스트림 텍스트 생성 작업에 적응시킬 때, 각 작업에 대해 별도의 모델을 학습하고 배포하는 것은 저장 공간과 연산 리소스 면에서 비효율적임.
> 
- 전체 미세 조정 중에 모델은 사전 학습된 가중치 $\Phi_0$으로 초기화되고 조건부 언어 모델링 목표를 최대화하기 위해 기울기를 따라 반복적으로 $\Phi_0 + \Delta \Phi$로 업데이트

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/c01b510c-7bd3-43ac-8512-763cd6bc8e34/Untitled.png)

- Full Fine Tuning의 주요 단점 중 하나는 각 다운스트림 작업마다 $|\Delta\Phi|$ 차원이 $|\Phi_0|$인 다른 파라미터 세트 $\Delta\Phi$를 학습한다는 점.
- 따라서 사전 학습된 모델이 큰 경우(예: $|\Phi_0| \approx 175B$인 GPT-3), 미세 조정된 모델의 많은 독립적인 인스턴스를 저장하고 배포하는 것이 어려울 수 있음
- 작업별 파라미터 증분 $\Delta\Phi = \Delta\Phi(\Theta)$를 $|\Theta| \ll |\Phi_0|$인 훨씬 더 작은 크기의 파라미터 $\Theta$ 집합으로 인코딩하는 보다 파라미터 효율적인 접근 방식을 채택.
- 따라서 $\Delta\Phi$를 찾는 작업은 $\Theta$에 대한 최적화:

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/9fe12d70-be59-445c-919e-e0acbb60c579/Untitled.png)

# AREN’T EXISTING SOLUTIONS GOOD ENOUGH?

> 대규모 언어 모델을 다양한 작업에 적응시키기 위해 기존에 존재하는 여러 접근 방법들이 갖는 한계점. 특히, 언어 모델링과 같은 작업에서 사용되는 기존 방식들인 어댑터 레이어 추가와 입력 계층 활성화 최적화는 큰 규모와 지연 시간에 민감한 생산 환경에서 여러 문제가 있음.
> 
- 어댑터 레이어는 추론 시간에 지연을 유발하며, 하드웨어 병렬 처리를 통해 지연 시간을 낮추는 데에 한계가 있음
- 특히, 이러한 레이어는 처리를 순차적으로 해야 하기 때문에 온라인 추론 설정에서 배치 크기가 작을 때 눈에 띄게 지연 시간이 증가.
- 모델을 샤드(분할)해야 할 필요가 있을 때는 이러한 어댑터의 깊이가 추가적인 동기화 GPU 작업을 필요로 하며, 어댑터 매개변수를 중복 저장하지 않는 한 문제가 더욱 악화됩니다.
- 또한, 입력 프롬프트를 직접 최적화하는 접근법은 최적화가 어렵고, 성능이 매개변수에 따라 비모노토닉하게 변동한다는 점을 확인하였습니다. 이는 작업에 사용할 수 있는 시퀀스 길이를 줄임으로써, 프롬프트 튜닝이 기타 방법들보다 성능이 떨어질 가능성이 있다는 우려를 제기합니다.
- 이러한 한계점들을 감안할 때, 기존 솔루션들은 대규모 언어 모델을 효율적으로 적응시키는 데에 여러 문제를 지니고 있으며, 이는 LoRA 같은 새로운 접근법이 필요함을 시사합니다. LoRA는 기존 방법들의 문제점을 해결하면서, 학습 매개변수의 수를 대폭 줄이고, 추가적인 추론 지연 없이 모델 품질을 유지할 수 있는 장점을 제공합니다.

# OUR METHOD

논문의 "우리의 방법(OUR METHOD)" 섹션에서는 LoRA(Low-Rank Adaptation) 기법의 구체적인 설계 및 이점에 대해 설명합니다. 이 방법은 심층 학습 모델의 밀집층에 대한 새로운 저랭크 파라미터화된 업데이트 매트릭스를 도입함으로써, 모델 적응을 효율적으로 수행합니다.

### **설계와 원리**

- **저랭크 업데이트 매트릭스**: LoRA는 신경망의 밀집층을 저랭크 업데이트 매트릭스로 구성하여, 사전 훈련된 가중치 *W*0에 추가합니다. 즉, *W*0+Δ*W*=*W*0+*BA*로 표현되며, 여기서 *B*와 *A*는 각각 *Rd*×*r*과 *Rr*×*k*의 차원을 가진 행렬입니다. *r*은 저랭크로, 기존 차원보다 훨씬 작습니다.
- **훈련 과정의 단순화**: 사전 훈련된 가중치 *W*0는 고정되며, 변화는 *B*와 *A*를 통해 이루어집니다. 이로 인해 모델은 필요한 매개변수의 수를 줄이면서도 효과적인 학습이 가능합니다.

### **실제 적용과 이점**

- **추론 시 추가 지연 없음**: LoRA를 적용한 모델은 추론 시 *W*0+*BA*를 미리 계산하고 저장함으로써, 추론 시 추가적인 지연을 발생시키지 않습니다. 다른 작업으로 전환할 때는 *W*0에서 *BA*를 빼고 다른 *B*′*A*′를 더함으로써 빠르게 전환할 수 있습니다.
- **추가적 이점**: LoRA는 메모리 사용량을 줄이고, 복잡한 최적화 상태를 유지할 필요 없이 효율적인 계산이 가능합니다. 또한, 다른 매개변수 효율적 적응 기법과 결합될 수 있는 유연성을 제공합니다.

LoRA는 전통적인 파인튜닝 방법과 비교하여 획기적으로 매개변수 수를 줄이면서도 모델의 성능을 유지하거나 개선할 수 있는 강력한 방법을 제시합니다. 이는 특히 대규모 모델을 다양한 작업에 적용할 때의 비용과 효율성 문제를 해결하는 데 큰 도움이 됩니다.

논문의 "우리의 방법(OUR METHOD)" 섹션에서는 LoRA(Low-Rank Adaptation) 기법의 구체적인 설계 및 이점에 대해 설명합니다. 이 방법은 심층 학습 모델의 밀집층에 대한 새로운 저랭크 파라미터화된 업데이트 매트릭스를 도입함으로써, 모델 적응을 효율적으로 수행합니다.

### **설계와 원리**

- **저랭크 업데이트 매트릭스**: LoRA는 신경망의 밀집층을 저랭크 업데이트 매트릭스로 구성하여, 사전 훈련된 가중치 *W*0에 추가합니다. 즉, *W*0+Δ*W*=*W*0+*BA*로 표현되며, 여기서 *B*와 *A*는 각각 *Rd*×*r*과 *Rr*×*k*의 차원을 가진 행렬입니다. *r*은 저랭크로, 기존 차원보다 훨씬 작습니다.
- **훈련 과정의 단순화**: 사전 훈련된 가중치 *W*0는 고정되며, 변화는 *B*와 *A*를 통해 이루어집니다. 이로 인해 모델은 필요한 매개변수의 수를 줄이면서도 효과적인 학습이 가능합니다.

### **실제 적용과 이점**

- **추론 시 추가 지연 없음**: LoRA를 적용한 모델은 추론 시 *W*0+*BA*를 미리 계산하고 저장함으로써, 추론 시 추가적인 지연을 발생시키지 않습니다. 다른 작업으로 전환할 때는 *W*0에서 *BA*를 빼고 다른 *B*′*A*′를 더함으로써 빠르게 전환할 수 있습니다.
- **추가적 이점**: LoRA는 메모리 사용량을 줄이고, 복잡한 최적화 상태를 유지할 필요 없이 효율적인 계산이 가능합니다. 또한, 다른 매개변수 효율적 적응 기법과 결합될 수 있는 유연성을 제공합니다.

LoRA는 전통적인 파인튜닝 방법과 비교하여 획기적으로 매개변수 수를 줄이면서도 모델의 성능을 유지하거나 개선할 수 있는 강력한 방법을 제시합니다. 이는 특히 대규모 모델을 다양한 작업에 적용할 때의 비용과 효율성 문제를 해결하는 데 큰 도움이 됩니다.

# EMPIRICAL EXPERIMENTS

논문의 "경험적 실험(EMPIRICAL EXPERIMENTS)" 섹션에서는 LoRA(Low-Rank Adaptation)의 효과를 다양한 언어 모델과 벤치마크 작업을 통해 평가합니다. 이 실험은 RoBERTa, DeBERTa, GPT-2, 그리고 GPT-3 모델을 사용하여 다양한 자연어 이해(NLU) 및 생성(NLG) 작업에 대한 LoRA의 성능을 측정합니다.

### **실험 설정 및 모델**

- **RoBERTa 및 DeBERTa**: GLUE 벤치마크에서의 성능을 평가하여, LoRA가 파인튜닝과 비교했을 때 어떻게 성능이 유지되는지 확인합니다.
- **GPT-2 및 GPT-3**: 위키SQL과 SAMSum 데이터셋을 포함한 다양한 대규모 실험을 통해 자연어 생성 작업에서 LoRA의 성능을 평가합니다.

### **주요 결과**

- **성능 유지 및 개선**: LoRA는 전통적인 파인튜닝 방법과 비교하여 학습 가능한 매개변수의 수를 현저히 줄이면서도 동등하거나 더 나은 성능을 제공합니다. RoBERTa와 DeBERTa 모델에서 LoRA를 사용한 실험은 GLUE 벤치마크에서 파인튜닝과 유사하거나 더 높은 성능을 보여줍니다.
- **자원 사용 최적화**: LoRA는 훨씬 적은 수의 매개변수를 사용함으로써 메모리 및 저장 공간을 절약하면서도 효과적인 학습을 가능하게 합니다. 이는 특히 대규모 모델에서 중요한 이점입니다.
- **다운스트림 작업 적응성**: 다양한 다운스트림 작업에 대한 LoRA의 적응성을 검증하여, 저랭크 업데이트를 통한 적응이 얼마나 효과적인지를 입증합니다. 특히, GPT-3 실험에서는 기존 파인튜닝과 비교하여 더 나은 결과를 보여줍니다.

### **결론**

이 섹션에서 수행된 실험은 LoRA가 매개변수의 수를 대폭 줄이면서도 모델의 품질을 유지하거나 향상시킬 수 있음을 실증적으로 보여줍니다. 이는 특히 계산 자원이 제한적인 환경에서 모델의 배포 및 운영을 최적화하는 데 큰 도움이 됩니다. 또한, LoRA는 추론 시간에 추가적인 지연을 초래하지 않으므로, 실시간 시스템에서도 유용하게 사용될 수 있습니다.

# RELATED WORKS

논문의 "관련 연구(RELATED WORKS)" 섹션에서는 LoRA(Low-Rank Adaptation) 방법론과 관련된 여러 연구들을 소개하고, LoRA의 개발에 영향을 준 기존 기술과 방법론들을 설명합니다. 이 섹션은 어댑터 레이어, 프롬프트 튜닝, 및 기타 매개변수 효율적 접근법과 관련된 다양한 연구들을 포함합니다.

### **Transformer 언어 모델**

- **Transformer 아키텍처**: Vaswani et al. (2017)에 의해 소개된 Transformer는 자기 주의(self-attention)를 사용하는 시퀀스-투-시퀀스 아키텍처로, 이후 다양한 언어 모델에 채택되었습니다. Radford et al.의 GPT 시리즈와 BERT는 이 아키텍처를 기반으로 사전 훈련 후 특정 작업에 대해 파인튜닝을 통해 상당한 성능 향상을 보였습니다.

### **파인튜닝 및 프롬프트 엔지니어링**

- **Fine-Tuning과 Prompt Engineering**: BERT와 GPT-3와 같은 모델들은 사전 훈련 후 파인튜닝을 통해 특정 작업에 적응합니다. 특히 GPT-3는 몇 가지 추가 훈련 예시만으로도 입력 프롬프트에 따라 행동을 조정할 수 있으며, 이는 '프롬프트 엔지니어링'이라는 기술을 필요로 합니다.

### **매개변수 효율적 적응 방법**

- **Adapter Layers**: Houlsby et al. (2019)는 Transformer 블록 사이에 어댑터 레이어를 삽입하여 전이 학습을 위한 매개변수 효율적인 방법을 제안했습니다. 이와 유사하게, 다양한 연구에서는 모델의 기존 레이어 사이에 추가적인 어댑터 레이어를 삽입하여 새로운 작업을 학습할 수 있는 방법을 탐색했습니다.
- **Prompt Optimization**: 최근 연구들은 입력 단어 임베딩을 최적화하거나 조정하여, 파인튜닝 없이 모델의 성능을 개선하는 방법을 제안했습니다. 이는 프롬프트 기반 접근 방식의 연속적이고 미분 가능한 일반화로 볼 수 있습니다.

### **저랭크 구조 및 딥러닝**

- **Low-Rank Structures**: 많은 기계 학습 및 딥러닝 문제에서 저랭크 구조의 이점이 연구되어 왔습니다. 이는 과대 매개변수화된 신경망이 훈련 후 저랭크 속성을 가진다는 사실을 발견한 것에서 기인합니다. LoRA는 이러한 저랭크 구조를 활용하여 효율적으로 매개변수를 적응시키는 새로운 방법을 제시합니다.

이 섹션은 LoRA의 개발과 구현에 중요한 기존 연구들을 설명함으로써, LoRA가 어떻게 현재 기술을 확장하고 새로운 기능을 제공하는지에 대한 맥락을 제공합니다.

# UNDERSTANDING THE LOW-RANK UPDATES

논문의 "저랭크 업데이트 이해하기(UNDERSTANDING THE LOW-RANK UPDATES)" 섹션에서는 LoRA(Low-Rank Adaptation)의 저랭크 업데이트가 어떻게 효율적으로 작동하는지에 대한 심층적인 분석을 제공합니다. 이 섹션은 저랭크 업데이트가 실제로 언어 모델의 적응에서 어떤 역할을 하는지, 그리고 왜 이러한 방식이 효과적인지를 이해하기 위한 여러 실험적 연구를 포함합니다.

### **연구 목적**

- **저랭크 업데이트의 효율성 분석**: 저랭크 업데이트가 모델 적응에 어떤 영향을 미치는지, 그리고 어떻게 그 효율성이 실현되는지에 대한 명확한 이해를 돕기 위해 실험적 연구를 수행합니다.

### **주요 연구 내용**

1. **업데이트 매트릭스의 진정한 저랭크 특성 확인**: 저랭크 구조가 실제로 언어 모델 적응에 있어 중요한 역할을 하는지를 탐구합니다. 특히, 업데이트가 이루어지는 가중치 Δ*W*의 랭크를 조절하여 어떻게 모델의 성능에 영향을 미치는지 관찰합니다.
    
    Δ�
    
2. **효율적인 적응을 위한 최적의 랭크 탐색**: 다양한 랭크 설정에서 모델의 성능을 비교함으로써, 가장 효율적인 저랭크 값을 결정합니다. 이는 LoRA가 적절한 랭크 선택을 통해 모델 적응의 효율성을 최대화할 수 있음을 보여줍니다.
3. **업데이트 매트릭스와 사전 훈련된 가중치 간의 상호작용 분석**: Δ*W*와 사전 훈련된 가중치 *W* 간의 상호작용을 분석하여, Δ*W*가 *W*의 어떤 특성을 강화하거나 보완하는지 탐구합니다. 이는 저랭크 업데이트가 어떻게 특정 작업에 대한 모델의 민감도를 조절하는지에 대한 통찰을 제공합니다.

### **실험 결과 및 의미**

- **실질적인 결과**: 이 연구를 통해 LoRA의 저랭크 업데이트가 모델 적응에 있어서 중요한 부분이며, 효율적인 계산과 메모리 사용에 있어서 큰 이점이 있음을 확인할 수 있습니다.
- **이론적 기여**: 저랭크 업데이트의 메커니즘에 대한 깊은 이해를 통해, 언어 모델의 사전 훈련 및 적응 과정에서 중요한 특성을 명확히 할 수 있습니다.

이 섹션은 LoRA가 어떻게 기존의 파인튜닝 접근법에 비해 효율성을 크게 향상시키면서도 모델의 성능을 유지하거나 향상시킬 수 있는지에 대한 중요한 통찰을 제공합니다.