# Abstract

> Low-Rank Adaptation(LoRA)은 전통적인 파인튜닝 방법에서 요구되는 모든 매개변수를 재학습하는 필요성을 크게 줄임. 사전 훈련된 모델의 가중치를 freeze 하고 Transformer 계층에 학습 가능한 Low Rank Matrix를 주입함으로써, LoRA는 파인튜닝된 GPT-3 모델과 비교할 때 학습 가능한 매개변수의 수를 최대 10,000배, GPU 메모리 요구 사항을 세 배 감소시킴. 학습 가능한 매개변수가 적음에도 불구하고 LoRA는 추가적인 추론 지연 없이 여러 NLP 작업 및 모델(RoBERTa, DeBERTa, GPT-2, GPT-3)에서 비교적 우수하거나 더 나은 품질을 보임.
> 

https://github.com/microsoft/LoRA.

# Introduction

> 대규모 언어 모델들이 자연어 처리 분야에서 다양한 Downstream Task로의 적응이 일반적으로 파인튜닝을 통해 이루어짐. 이 과정에서 모든 매개변수를 업데이트하는 것이 주된 방법이었으나 이 방식은 모델이 커질수록 많은 리소스를 요구하고, GPT-3처럼 1750억 개의 매개변수를 가진 모델의 경우 매우 비실용적.
> 
- 연구자들은 모델의 일부 매개변수만을 조정하거나, 새로운 태스크에 맞춰 학습하는 외부 모듈을 사용하는 등의 방법을 모색함.
- 이는 배포시 필요한 저장 공간과 로드 시간을 줄이는데 도움을 줄 수 있으나, 기존 기술들은 대부분 모델의 품질을 저하시키거나 추론 지연을 야기하는 문제점.
- 학습된 over-parametrized models가 실제로는 낮은 내재적 차원에 존재한다는 것을 보여주는 Li 등(2018a)과 Aghajanyan 등(2020)에서 영감을 받음.
    - 모델 적응 중 가중치의 변화도 '내재적 순위'가 낮다는 가설을 세우고, 이에 따라 저순위 적응(LoRA) 접근법을 제안했습니다.
    - 내재적 순위가 낮다는 것은 신경망의 특정 부분이나 레이어에서 발생하는 변화가 비교적 적다는 의미
    - 신경망 전체의 복잡도나 크기에 비해 해당 부분의 변화에 필요한 정보의 양이 적다는 것을 나타냄
- 이에 저자들은 Low-Rank Adaptation, 즉 LoRA 방법을 제안. 이는 트랜스포머 아키텍처의 각 계층에 저랭크 분해 행렬을 주입하는 방식으로, 사전 훈련된 가중치는 고정하고, 변경될 가중치만을 학습하여 매개변수 수를 현저히 줄일 수 있음.
- LoRA를 사용하면 그림 1과 같이 미리 학습된 가중치를 고정된 상태로 유지하면서 대신 Adaptation 중 고밀도 계층의 변화에 대한 rank decomposition matrix를 최적화하여 신경망의 일부 고밀도 계층을 간접적으로 학습할 수 있음.
- GPT-3 175B를 예로 들면, 전체 랭크(즉, d)가 12,288에 달하는 경우에도 매우 낮은 랭크(그림 1의 r은 1~2개일 수 있음)로도 충분하므로 LoRA는 저장 및 컴퓨팅 효율이 모두 높음.

# Problem Statement

> 주된 문제는 대규모 사전 훈련된 언어 모델, 특히 GPT-3와 같은 모델을 다양한 다운스트림 텍스트 생성 작업에 적응시킬 때, 각 작업에 대해 별도의 모델을 학습하고 배포하는 것은 저장 공간과 연산 리소스 면에서 비효율적임.
> 
- 전체 미세 조정 중에 모델은 사전 학습된 가중치 $\Phi_0$으로 초기화되고 조건부 언어 모델링 목표를 최대화하기 위해 기울기를 따라 반복적으로 $\Phi_0 + \Delta \Phi$로 업데이트

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/c01b510c-7bd3-43ac-8512-763cd6bc8e34/Untitled.png)

- Full Fine Tuning의 주요 단점 중 하나는 각 다운스트림 작업마다 $|\Delta\Phi|$ 차원이 $|\Phi_0|$인 다른 파라미터 세트 $\Delta\Phi$를 학습한다는 점.
- 따라서 사전 학습된 모델이 큰 경우(예: $|\Phi_0| \approx 175B$인 GPT-3), 미세 조정된 모델의 많은 독립적인 인스턴스를 저장하고 배포하는 것이 어려울 수 있음
- 작업별 파라미터 증분 $\Delta\Phi = \Delta\Phi(\Theta)$를 $|\Theta| \ll |\Phi_0|$인 훨씬 더 작은 크기의 파라미터 $\Theta$ 집합으로 인코딩하는 보다 파라미터 효율적인 접근 방식을 채택.
- 따라서 $\Delta\Phi$를 찾는 작업은 $\Theta$에 대한 최적화:

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/9fe12d70-be59-445c-919e-e0acbb60c579/Untitled.png)

# AREN’T EXISTING SOLUTIONS GOOD ENOUGH?

> 대규모 언어 모델을 다양한 작업에 적응시키기 위해 기존에 존재하는 여러 접근 방법들이 갖는 한계점. 특히, 언어 모델링과 같은 작업에서 사용되는 기존 방식들인 어댑터 레이어 추가와 입력 계층 활성화 최적화는 큰 규모와 지연 시간에 민감한 생산 환경에서 여러 문제가 있음.
> 
- 기존의 어댑터 레이어는 Transformer block마다 두 개씩 존재함.
- 이 구성은 모델의 복잡도를 증가시키며, 순차적으로 처리되어야 하므로 병렬 처리 능력을 충분히 활용할 수 없음.
- 온라인 추론 상황에서는 일반적으로 배치 크기가 매우 작게 설정되며, 어댑터 레이어가 추가된 Transformer 모델은 작은 병목 차원에도 불구하고 순차적인 연산처리 때문에 느려지는 것을 확인할 수 있음(Table 1)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/6965beaf-4799-4ddf-b254-6dfe535713b0/Untitled.png)

- 입력 프롬프트를 직접 최적화하는 접근법은 최적화가 어렵고, 성능이 매개변수에 따라 비모노토닉하게 변동한다는 점을 확인.
- 이는 작업에 사용할 수 있는 시퀀스 길이를 줄임으로써, 프롬프트 튜닝이 기타 방법들보다 성능이 떨어질 가능성이 있다는 우려를 제기합니다.
- 이러한 점들을 감안할 때, 기존 솔루션들은 대규모 언어 모델을 효율적으로 적응시키는 데에 여러 문제를 지니고 있음

# OUR METHOD

> LoRA(Low-Rank Adaptation) 기법의 구체적인 설계 및 이점에 대해 설명. 이 방법은 심층 학습 모델의 dense layer에 대한 새로운 Low Rank Parametrized Update Matrix 를 도입함으로써, 모델 적응을 효율적으로 수행.
> 

### LOW-RANK-PARAMETRIZED UPDATE MATRICES

- Specific Task에 적응할 때 사전 학습된 언어 모델이 '내재적 차원'이 낮아 더 작은 하위 공간에 무작위로 투영하더라도 여전히 효율적으로 학습할 수 있음을 보여줌.
    - 이에 영감을 받아 가중치 업데이트의 크기 역시 적응 과정에서 '내재적 순위'가 낮다고 가정.
- 사전 훈련된 Weghted Matrix $W_0 \in \R^{d\times k}$ 의 경우, Low Rank Decomposition $W_0 + \Delta W = W_0 + BA$($B \in \R^{d \times r}, A \in \R^{r \times k}, rank \ll min(d, k$) 로 표현
- 훈련 중에 $W_0$는 고정되어 기울기 업데이트를 받지 않는 반면, $A$와 $B$는 훈련 가능한 파라미터를 포함.
- $W_0$과 $\Delta W = BA$는 모두 동일한 입력으로 곱해지며, 각각의 출력 벡터는 좌표 방향으로 합산. $h = W_0x$의 경우 수정된 포워드 패스가 산출

$$
h = W_0x + \Delta Wx = W_0x + BAx
$$

- $A$에는 Random Gaussian initialization를, $B$에는 0을 사용하므로 훈련 시작 시 $\Delta W=BA$는 0(그림 1)
- 그런 다음 $\Delta Wx$를 $\alpha \over r$로 조정. 여기서 $\alpha$는 $r$의 상수입니다.
- Adam으로 최적화할 때 초기화를 적절히 조정하면 $\alpha$를 조정하는 것은 학습 속도를 조정하는 것과 거의 동일합니다. 따라서 $\alpha$를 처음 시도하는 $r$로 설정하고 튜닝하지 않습니다.
- 이러한 스케일링은 $r$을 변경할 때 하이퍼파라미터를 다시 조정할 필요성을 줄이는 데 도움이 됩니다(Yang & Hu, 2021).
- 전체 미세 조정의 일반화. 보다 일반적인 형태의 미세 조정을 통해 미리 학습된 파라미터의 하위 집합을 학습할 수 있습니다.
- LoRA는 여기서 한 걸음 더 나아가 적응 과정에서 가중 행렬에 대한 누적 기울기 업데이트가 전체 순위를 갖도록 요구하지 않습니다.
- 즉, 모든 가중 행렬에 LoRA를 적용하고 모든 편향2을 학습할 때 LoRA 랭크 $r$을 사전 학습된 가중 행렬의 랭크로 설정함으로써 전체 미세 조정의 표현력을 대략적으로 회복할 수 있습니다.
- 즉, 훈련 가능한 파라미터의 수를 늘릴수록3 LoRA 훈련은 대략 원래 모델 훈련에 수렴하는 반면, 어댑터 기반 방법은 MLP에 수렴하고 접두사 기반 방법은 긴 입력 시퀀스를 사용할 수 없는 모델에 수렴합니다.
- **훈련 과정의 단순화** : 사전 훈련된 가중치 $W_0$는 고정되며, 변화는 $B$와 $A$를 통해 이루어집니다. 이로 인해 모델은 필요한 매개변수의 수를 줄이면서도 효과적인 학습이 가능합니다.

### APPLYING LORA TO TRANSFORMER

- 원칙적으로 신경망의 모든 가중치 행렬 하위 집합에 LoRA를 적용하여 학습 가능한 파라미터의 수를 줄일 수 있습니다. 트랜스포머 아키텍처에서는 자기 주의 모듈에 4개의 가중치 행렬(Wq , Wk , Wv , Wo )이 있고 MLP 모듈에 2개의 가중치 행렬이 있습니다. 출력 차원이 일반적으로 주의 헤드로 분할되더라도 Wq(또는 Wk , Wv )는 차원 d모델 × d모델의 단일 행렬로 취급합니다.
- 우리는 단순성과 매개변수 효율성을 위해 다운스트림 작업에 대한 주의 가중치만 조정하고 MLP 모듈을 동결(다운스트림 작업에서 훈련되지 않도록)하는 것으로 연구를 제한하며, 7.1절에서 트랜스포머에서 다양한 유형의 주의 가중치 행렬을 조정할 때의 효과를 추가로 연구합니다. MLP 레이어, 레이어노름 레이어, 편향의 적용에 대한 경험적 조사는 추후 작업으로 남겨둡니다.
- 실질적인 혜택과 한계. 가장 큰 이점은 메모리 및 스토리지 사용량 감소에서 비롯됩니다. Adam으로 훈련된 대형 트랜스포머의 경우, 고정된 파라미터에 대한 최적화 상태를 저장할 필요가 없기 때문에 r ≪ dmodel의 경우 VRAM 사용량을 최대 2/3까지 줄일 수 있습니다. GPT-3 175B에서는 훈련 중 VRAM 소비량을 1.2TB에서 350GB로 줄였습니다. r = 4이고 쿼리와 값 투영 행렬만 적용하면 체크포인트 크기가 약 10,000배(350GB에서 35MB로)4 감소.
- 따라서 훨씬 적은 수의 GPU로 훈련할 수 있고 I/O 병목 현상을 피할 수 있습니다. 또 다른 이점은 모든 파라미터가 아닌 LoRA 가중치만 교체함으로써 훨씬 저렴한 비용으로 배포하면서 작업 간에 전환할 수 있다는 것입니다.
- 이를 통해 사전 학습된 가중치를 VRAM에 저장하는 머신에서 즉시 교체할 수 있는 다양한 맞춤형 모델을 생성할 수 있습니다. 또한 대부분의 파라미터에 대한 기울기를 계산할 필요가 없기 때문에 전체 미세 조정5에 비해 GPT-3 175B에서 훈련하는 동안 25%의 속도 향상을 관찰할 수 있었습니다.
- LoRA에도 한계가 있습니다. 예를 들어, 추가 추론 대기 시간을 없애기 위해 A와 B를 W로 흡수하기로 선택한 경우, 하나의 포워드 패스에서 A와 B가 다른 여러 작업에 대한 입력을 일괄 처리하는 것은 간단하지 않습니다.
- 지연 시간이 중요하지 않은 시나리오에서는 가중치를 병합하지 않고 샘플에 사용할 LoRA 모듈을 일괄적으로 동적으로 선택할 수 있습니다.

- **추론 시 추가 지연 없음**: LoRA를 적용한 모델은 추론 시 *W*0+*BA*를 미리 계산하고 저장함으로써, 추론 시 추가적인 지연을 발생시키지 않습니다. 다른 작업으로 전환할 때는 *W*0에서 *BA*를 빼고 다른 *B*′*A*′를 더함으로써 빠르게 전환할 수 있습니다.
- **추가적 이점**: LoRA는 메모리 사용량을 줄이고, 복잡한 최적화 상태를 유지할 필요 없이 효율적인 계산이 가능합니다. 또한, 다른 매개변수 효율적 적응 기법과 결합될 수 있는 유연성을 제공합니다.
- LoRA는 전통적인 파인튜닝 방법과 비교하여 획기적으로 매개변수 수를 줄이면서도 모델의 성능을 유지하거나 개선할 수 있는 강력한 방법을 제시합니다. 이는 특히 대규모 모델을 다양한 작업에 적용할 때의 비용과 효율성 문제를 해결하는 데 큰 도움이 됩니다.
- 논문의 "우리의 방법(OUR METHOD)" 섹션에서는 LoRA(Low-Rank Adaptation) 기법의 구체적인 설계 및 이점에 대해 설명합니다. 이 방법은 심층 학습 모델의 밀집층에 대한 새로운 저랭크 파라미터화된 업데이트 매트릭스를 도입함으로써, 모델 적응을 효율적으로 수행합니다.

### **설계와 원리**

- **저랭크 업데이트 매트릭스**: LoRA는 신경망의 밀집층을 저랭크 업데이트 매트릭스로 구성하여, 사전 훈련된 가중치 *W*0에 추가합니다. 즉, *W*0+Δ*W*=*W*0+*BA*로 표현되며, 여기서 *B*와 *A*는 각각 *Rd*×*r*과 *Rr*×*k*의 차원을 가진 행렬입니다. *r*은 저랭크로, 기존 차원보다 훨씬 작습니다.
- **훈련 과정의 단순화**: 사전 훈련된 가중치 *W*0는 고정되며, 변화는 *B*와 *A*를 통해 이루어집니다. 이로 인해 모델은 필요한 매개변수의 수를 줄이면서도 효과적인 학습이 가능합니다.

### **실제 적용과 이점**

- **추론 시 추가 지연 없음**: LoRA를 적용한 모델은 추론 시 *W*0+*BA*를 미리 계산하고 저장함으로써, 추론 시 추가적인 지연을 발생시키지 않습니다. 다른 작업으로 전환할 때는 *W*0에서 *BA*를 빼고 다른 *B*′*A*′를 더함으로써 빠르게 전환할 수 있습니다.
- **추가적 이점**: LoRA는 메모리 사용량을 줄이고, 복잡한 최적화 상태를 유지할 필요 없이 효율적인 계산이 가능합니다. 또한, 다른 매개변수 효율적 적응 기법과 결합될 수 있는 유연성을 제공합니다.

# EMPIRICAL EXPERIMENTS

> LoRA(Low-Rank Adaptation)의 효과를 다양한 언어 모델과 벤치마크 작업을 통해 평가합니다. 이 실험은 RoBERTa, DeBERTa, GPT-2, 그리고 GPT-3 모델을 사용하여 다양한 자연어 이해(NLU) 및 생성(NLG) 작업에 대한 LoRA의 성능을 측정.
> 

### **실험 설정 및 모델**

- **RoBERTa 및 DeBERTa**: GLUE 벤치마크에서의 성능을 평가하여, LoRA가 파인튜닝과 비교했을 때 어떻게 성능이 유지되는지 확인합니다.
- **GPT-2 및 GPT-3**: 위키SQL과 SAMSum 데이터셋을 포함한 다양한 대규모 실험을 통해 자연어 생성 작업에서 LoRA의 성능을 평가합니다.

### **주요 결과**

- **성능 유지 및 개선**: LoRA는 전통적인 파인튜닝 방법과 비교하여 학습 가능한 매개변수의 수를 현저히 줄이면서도 동등하거나 더 나은 성능을 제공합니다. RoBERTa와 DeBERTa 모델에서 LoRA를 사용한 실험은 GLUE 벤치마크에서 파인튜닝과 유사하거나 더 높은 성능을 보여줍니다.
- **자원 사용 최적화**: LoRA는 훨씬 적은 수의 매개변수를 사용함으로써 메모리 및 저장 공간을 절약하면서도 효과적인 학습을 가능하게 합니다. 이는 특히 대규모 모델에서 중요한 이점입니다.
- **다운스트림 작업 적응성**: 다양한 다운스트림 작업에 대한 LoRA의 적응성을 검증하여, 저랭크 업데이트를 통한 적응이 얼마나 효과적인지를 입증합니다. 특히, GPT-3 실험에서는 기존 파인튜닝과 비교하여 더 나은 결과를 보여줍니다.

### **결론**

이 섹션에서 수행된 실험은 LoRA가 매개변수의 수를 대폭 줄이면서도 모델의 품질을 유지하거나 향상시킬 수 있음을 실증적으로 보여줍니다. 이는 특히 계산 자원이 제한적인 환경에서 모델의 배포 및 운영을 최적화하는 데 큰 도움이 됩니다. 또한, LoRA는 추론 시간에 추가적인 지연을 초래하지 않으므로, 실시간 시스템에서도 유용하게 사용될 수 있습니다.

# RELATED WORKS

> LoRA(Low-Rank Adaptation) 방법론과 관련된 여러 연구들을 소개하고, LoRA의 개발에 영향을 준 기존 기술과 방법론들을 설명합니다. 이 섹션은 어댑터 레이어, 프롬프트 튜닝, 및 기타 매개변수 효율적 접근법과 관련된 다양한 연구들을 포함.
> 

### **Transformer 언어 모델**

- **Transformer 아키텍처**: Vaswani et al. (2017)에 의해 소개된 Transformer는 자기 주의(self-attention)를 사용하는 시퀀스-투-시퀀스 아키텍처로, 이후 다양한 언어 모델에 채택되었습니다. Radford et al.의 GPT 시리즈와 BERT는 이 아키텍처를 기반으로 사전 훈련 후 특정 작업에 대해 파인튜닝을 통해 상당한 성능 향상을 보였습니다.

### **파인튜닝 및 프롬프트 엔지니어링**

- **Fine-Tuning과 Prompt Engineering**: BERT와 GPT-3와 같은 모델들은 사전 훈련 후 파인튜닝을 통해 특정 작업에 적응합니다. 특히 GPT-3는 몇 가지 추가 훈련 예시만으로도 입력 프롬프트에 따라 행동을 조정할 수 있으며, 이는 '프롬프트 엔지니어링'이라는 기술을 필요로 합니다.

### **매개변수 효율적 적응 방법**

- **Adapter Layers**: Houlsby et al. (2019)는 Transformer 블록 사이에 어댑터 레이어를 삽입하여 전이 학습을 위한 매개변수 효율적인 방법을 제안했습니다. 이와 유사하게, 다양한 연구에서는 모델의 기존 레이어 사이에 추가적인 어댑터 레이어를 삽입하여 새로운 작업을 학습할 수 있는 방법을 탐색했습니다.
- **Prompt Optimization**: 최근 연구들은 입력 단어 임베딩을 최적화하거나 조정하여, 파인튜닝 없이 모델의 성능을 개선하는 방법을 제안했습니다. 이는 프롬프트 기반 접근 방식의 연속적이고 미분 가능한 일반화로 볼 수 있습니다.

### **저랭크 구조 및 딥러닝**

- **Low-Rank Structures**: 많은 기계 학습 및 딥러닝 문제에서 저랭크 구조의 이점이 연구되어 왔습니다. 이는 과대 매개변수화된 신경망이 훈련 후 저랭크 속성을 가진다는 사실을 발견한 것에서 기인합니다. LoRA는 이러한 저랭크 구조를 활용하여 효율적으로 매개변수를 적응시키는 새로운 방법을 제시합니다.

이 섹션은 LoRA의 개발과 구현에 중요한 기존 연구들을 설명함으로써, LoRA가 어떻게 현재 기술을 확장하고 새로운 기능을 제공하는지에 대한 맥락을 제공합니다.

# UNDERSTANDING THE LOW-RANK UPDATES

> LoRA(Low-Rank Adaptation)의 저랭크 업데이트가 어떻게 효율적으로 작동하는지에 대한 심층적인 분석을 제공합니다. 이 섹션은 저랭크 업데이트가 실제로 언어 모델의 적응에서 어떤 역할을 하는지, 그리고 왜 이러한 방식이 효과적인지를 이해하기 위한 여러 실험적 연구를 포함합니다.
> 

### **연구 목적**

- **저랭크 업데이트의 효율성 분석**: 저랭크 업데이트가 모델 적응에 어떤 영향을 미치는지, 그리고 어떻게 그 효율성이 실현되는지에 대한 명확한 이해를 돕기 위해 실험적 연구를 수행합니다.

### **주요 연구 내용**

1. **업데이트 매트릭스의 진정한 저랭크 특성 확인**: 저랭크 구조가 실제로 언어 모델 적응에 있어 중요한 역할을 하는지를 탐구합니다. 특히, 업데이트가 이루어지는 가중치 Δ*W*의 랭크를 조절하여 어떻게 모델의 성능에 영향을 미치는지 관찰합니다.
    
    Δ�
    
2. **효율적인 적응을 위한 최적의 랭크 탐색**: 다양한 랭크 설정에서 모델의 성능을 비교함으로써, 가장 효율적인 저랭크 값을 결정합니다. 이는 LoRA가 적절한 랭크 선택을 통해 모델 적응의 효율성을 최대화할 수 있음을 보여줍니다.
3. **업데이트 매트릭스와 사전 훈련된 가중치 간의 상호작용 분석**: Δ*W*와 사전 훈련된 가중치 *W* 간의 상호작용을 분석하여, Δ*W*가 *W*의 어떤 특성을 강화하거나 보완하는지 탐구합니다. 이는 저랭크 업데이트가 어떻게 특정 작업에 대한 모델의 민감도를 조절하는지에 대한 통찰을 제공합니다.

### **실험 결과 및 의미**

- **실질적인 결과**: 이 연구를 통해 LoRA의 저랭크 업데이트가 모델 적응에 있어서 중요한 부분이며, 효율적인 계산과 메모리 사용에 있어서 큰 이점이 있음을 확인할 수 있습니다.
- **이론적 기여**: 저랭크 업데이트의 메커니즘에 대한 깊은 이해를 통해, 언어 모델의 사전 훈련 및 적응 과정에서 중요한 특성을 명확히 할 수 있습니다.