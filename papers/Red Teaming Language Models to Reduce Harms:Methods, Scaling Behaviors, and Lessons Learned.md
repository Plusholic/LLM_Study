

# Abatract

언어 모델들이 잠재적으로 해로운 출력을 발견하고, 측정하며, 시도하여 줄이기 위해 진행된 초기 레드팀 활동을 설명합니다. 주요 기여는 세 가지입니다. 첫째, 3가지 모델 크기(2.7B, 13B, 52B 매개변수)와 4가지 모델 유형에 대한 레드팀 활동의 스케일링 행동을 조사했습니다. 둘째, 38,961개의 레드팀 공격 데이터셋을 공개하여 다른 연구자들이 분석하고 학습할 수 있도록 했습니다. 셋째, 레드팀 과정에 대한 지침, 절차, 통계적 방법론 및 불확실성을 상세하게 설명했습니다.

# Introduction

대규모 언어 모델이 나타낼 수 있는 다양한 해로운 행동을 조명하고 있습니다. 이러한 해로운 행동에는 사회적 편견 강화, 공격적 또는 유해한 출력 생성, 훈련 데이터에서 개인 식별 정보 유출, 정보 조작 캠페인 지원, 극단적 텍스트 생성, 거짓 정보 전파 등이 포함됩니다. AI 시스템이 발전함에 따라 이러한 해로움의 범위는 확장될 가능성이 높습니다. 이 문제에 대처하기 위해 다양한 전략이 개발되었으며, 그 중 하나가 레드팀을 사용하는 것입니다. 레드팀은 언어 모델을 적대적으로 조사하여 유해한 출력을 찾고 모델을 업데이트하여 이러한 출력을 방지하는 수동 또는 자동화된 방법을 사용합니다. 이 논문은 수동 레드팀을 구현하여 모델의 안전성을 향상시키고 측정하는 초기 노력에 대해 설명하고 있습니다.

# Methods

이 섹션에서는 레드팀 작업에 사용된 방법론을 상세히 설명합니다. 레드팀 멤버들은 AI 보조자와 개방형 대화를 통해 AI가 부적절하고 해로운 것들을 말하도록 유도하는 역할을 맡았습니다. 이를 위해, 다음과 같은 네 가지 다이얼로그 모델이 사용되었습니다:

1. **Plain language models (Plain LM):** 이는 기본 언어 모델로, 일반 대화 모델로 사용되기 위해 1-shot 학습을 사용합니다.
2. **Prompted language models (Prompted LM):** 도움이 되고, 정직하며, 해가 없도록 프롬프트된 언어 모델입니다. 14-shot 학습을 통해 구현되었습니다.
3. **Rejection sampling (RS):** 프롬프트된 언어 모델로부터 생성된 16개의 샘플 중 해로움이 가장 적은 샘플을 선택합니다.
4. **Reinforcement learning from human feedback (RLHF):** 인간의 피드백으로부터 강화 학습을 통해 훈련된 모델로, 해로움 점수를 최대화하도록 설계되었습니다.

레드팀 멤버들은 다양한 대화를 통해 AI의 부적절한 반응을 유도하도록 도전하고, 각 대화 턴에서 AI가 생성한 두 가지 응답 중 더 해로운 응답을 선택하게 합니다. 이러한 프로세스는 AI 시스템의 취약점을 식별하고 빠르게 파악하는 데 도움을 주며, 또한 모델 응답 쌍 데이터셋을 생성하여 해로움 선호 모델을 훈련시키는 데 사용됩니다.

# Results

이 섹션에서는 레드팀 프로젝트의 결과를 제시하고 있습니다. 연구팀은 크게 세 가지 주요 결과를 얻었습니다:

1. **모델 크기 및 유형에 따른 공격 성공률의 차이:**
    - **RLHF 모델:** 크기가 증가함에 따라 레드팀에 의해 공격받기 어려워지는 경향이 있습니다.
    - **기타 모델들 (Plain LM, Prompted LM, RS):** 크기에 따른 뚜렷한 공격 성공률의 변화가 없으며, RS 모델이 어떤 크기에서도 공격하기 가장 어려운 것으로 나타났습니다.
2. **공격 유형의 분석:**
    - 연구팀은 38,961건의 레드팀 공격 데이터를 분석하여 다양한 유형의 해로운 출력을 확인했습니다. 이러한 출력은 공격적인 언어 사용에서부터 비폭력적이지만 비윤리적인 행동에 이르기까지 다양했습니다.
3. **데이터셋 공개 및 분석:**
    - 연구팀은 자체 분석 결과와 함께 레드팀 공격 데이터셋을 공개하여, 다른 연구자들이 이 데이터를 사용하여 유해한 출력을 이해하고, (반)자동 레드팀 기법을 개발하며, 언어 모델에서의 해로움을 측정하고 완화하는 전략을 프로토타이핑할 수 있도록 했습니다.

이 연구는 또한 레드팀 공격이 언어 모델에 미치는 영향을 보여주는 여러 그래프와 통계적 분석을 포함하고 있으며, 이는 안전한 AI 시스템을 개발하는 데 중요한 통찰을 제공합니다.

# Discussion

이 섹션에서는 레드팀 프로젝트의 결과와 그에 따른 함의를 심층적으로 논의하고 있습니다. 주요 논의 포인트는 다음과 같습니다:

1. **레드팀의 효과성과 한계:**
    - 연구팀은 레드팀 활동이 언어 모델의 안전성을 측정하고 개선하는 데 유용하다는 것을 발견했습니다. 그러나 모든 유형의 해로움을 완벽하게 방지하는 것은 여전히 도전적입니다. 특히, 더 큰 모델들이 더 정교하고 미묘한 해로운 출력을 생성할 가능성이 있음을 발견했습니다.
2. **데이터셋의 활용:**
    - 공개된 데이터셋은 다른 연구자들이 언어 모델을 레드팀하는 데 사용될 수 있으며, 이를 통해 모델의 안전성을 향상시킬 수 있는 새로운 접근법을 개발할 수 있습니다. 데이터셋은 또한 해로운 출력의 패턴을 이해하고, 이에 대응하기 위한 자동화된 시스템을 개발하는 데 도움을 줄 수 있습니다.
3. **정책 개입 제안:**
    - 연구팀은 AI 시스템의 안전성을 향상시키기 위해 커뮤니티가 공동으로 노력해야 한다고 강조합니다. 이를 위해 레드팀 활동과 결과를 공유하고, 이를 통해 얻은 지식을 바탕으로 표준과 관행을 개발하는 것이 중요하다고 제안합니다.
4. **향후 연구 방향:**
    - 레드팀 작업을 통해 얻은 데이터와 경험을 기반으로, 더 효과적인 안전 중재 방법을 개발하고, AI 시스템의 다양한 적용 사례에 대해 더 깊이 이해할 필요가 있습니다. 또한, 레드팀 기법을 자동화하여 대규모로 적용 가능하도록 하는 방안에 대한 연구가 필요합니다.

# Conclusion

연구팀이 수행한 레드팀 활동의 중요성과 그 결과의 의미에 대해 요약하고 있습니다. 주요 결론은 다음과 같습니다:

1. **레드팀의 중요성:** 언어 모델이 생성할 수 있는 다양한 해로운 출력을 식별, 측정, 완화하는 데 있어 레드팀의 역할은 매우 중요합니다. 이 연구를 통해 레드팀이 언어 모델의 안전성을 향상시키는 데 기여할 수 있는 방법을 구체적으로 보여줍니다.
2. **공개 데이터셋의 가치:** 연구팀이 공개한 레드팀 공격 데이터셋은 이 분야의 다른 연구자들에게 중요한 자원을 제공합니다. 이 데이터를 통해 연구자들은 해로운 출력의 패턴을 분석하고, 언어 모델의 안전성을 향상시키기 위한 새로운 기술을 개발할 수 있습니다.
3. **투명성의 중요성:** 연구팀은 레드팀 과정과 결과에 대한 투명성을 유지하는 것이 커뮤니티 전체의 안전 기준과 관행 개발에 도움이 된다고 강조합니다. 이는 공공의 이익을 위한 노력의 일환으로, 이러한 투명성이 연구 커뮤니티에 긍정적인 영향을 미칠 수 있습니다.
4. **지속적인 노력의 필요성:** AI 기술의 발전에 따라 새로운 종류의 위험이 계속해서 발생할 가능성이 있습니다. 따라서 레드팀 활동은 지속적으로 필요하며, 이를 통해 AI 시스템의 안전성을 지속적으로 검증하고 개선해야 합니다.

이 연구는 레드팀이 어떻게 AI 시스템의 안전성을 평가하고 향상시킬 수 있는지에 대한 실질적인 예를 제공합니다. 또한, 연구 커뮤니티가 이러한 활동을 지원하고 확장하기 위한 구체적인 방안을 모색해야 함을 강조합니다.