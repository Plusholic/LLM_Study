

언어 모델(Language Models, LMs)이 사용자에게 예측하기 어려운 방식으로 해를 끼칠 수 있는 잠재적인 위험으로 인해 배포가 어려운 경우가 많음. 기존의 작업은 인간 주석자가 테스트 케이스를 수작업으로 작성하여 해로운 행동을 식별하지만, 이 방법은 비용이 많이 들고 테스트 케이스의 수와 다양성에 제한을 줍니다.

본 연구는 다른 LM을 사용하여 테스트 케이스를 자동 생성함으로써(target LM이 해로운 방식으로 행동하는 경우를 찾아내는) “레드 팀(red teaming)”을 수행.

생성된 테스트 질문에 대한 대상 LM의 응답을 평가하여 공격적 내용을 탐지하는 분류기를 사용하고, 결과적으로 280B 파라미터의 LM 챗봇에서 수만 개의 공격적인 응답을 밝혀냅니다. 이 연구는 다양하고 바람직하지 않은 LM 행동을 발견하고 수정하는데 LM 기반 레드 팀이 유용한 도구 중 하나임을 제시함.

### **Introduction 요약**

이 논문의 서론 부분에서는 언어 모델(LMs)이 다양한 애플리케이션에 유망한 도구로서의 역할을 강조합니다. 이러한 애플리케이션은 대화형 보조기구에서 질문-응답 시스템에 이르기까지 다양합니다. 그러나 LMs을 생산 환경에 배포하는 것은 사용자에게 예측하기 어려운 방식으로 해를 끼칠 위험을 내포하고 있습니다. 예를 들어, Microsoft는 사용자들이 인종차별적이고 성적인 트윗을 유발한 후에 그들의 챗봇 Tay를 폐쇄해야 했습니다. 다른 연구에서는 LMs가 잘못된 정보나 기밀 개인 정보를 생성한다는 것을 발견했습니다. 이러한 실패들은 심각한 결과를 초래할 수 있으므로, 배포 전에 이러한 실패들을 발견하고 수정하는 것이 중요합니다.

기존의 접근 방식은 인간 주석자가 수동으로 실패를 발견하는 데 의존하지만, 이는 실패의 수와 다양성을 제한합니다. 예를 들어, 일부 노력은 많은 수작업 테스트 케이스를 사용하여 실패를 찾습니다. 그러나 이러한 방법은 인간의 노력과 창의성에 의존하며 많은 "중대한 간과"를 초래할 수 있습니다. 이 논문은 수동 테스트를 보완하고 이러한 간과를 줄이기 위해 LMs가 해로운 방식으로 행동하는 곳을 자동으로 찾는 '레드 팀'을 목표로 합니다. 이를 위해 연구팀은 LM 자체를 사용하여 테스트 입력을 생성하고, 테스트 입력에 대한 해로운 행동을 탐지하는 분류기를 사용합니다.

### **Methods 요약**

이 논문의 방법론 섹션에서는 대상 언어 모델(target LM)이 특정 입력에 대해 해로운 텍스트를 출력하는 경우를 찾아내기 위한 접근 방식을 제안합니다. 이를 위해, 실험은 "레드 팀 언어 모델(red LM)"을 사용하여 테스트 케이스를 생성하고, 대상 LM이 각 테스트 케이스에 대해 출력을 생성하도록 합니다. 그 후, 레드 팀 분류기를 사용하여 해로운 출력을 한 테스트 케이스를 찾습니다.

논문은 다양한 방법을 탐구하여 테스트 케이스 생성에 접근합니다:

1. **Zero-shot Generation**: 사전 학습된 LM을 사용하여 주어진 프롬프트나 "텍스트 앞머리"를 바탕으로 테스트 케이스를 생성합니다. 이 방법은 특정 행동을 테스트하기 위해 생성된 테스트 케이스의 분포를 조정할 수 있습니다.
2. **Stochastic Few-shot Generation**: 실패한 제로샷 테스트 케이스를 예제로 사용하여 비슷한 테스트 케이스를 생성합니다. 이 방법은 다양성을 높이기 위해 테스트 케이스 풀에서 무작위로 테스트 케이스를 선택하여 프롬프트에 추가합니다.
3. **Supervised Learning (SL)**: 실패한 제로샷 테스트 케이스에 대한 로그 가능성을 최대화하여 사전 학습된 LM을 미세 조정합니다. 이 방법은 테스트 케이스 다양성을 유지하면서 과적합을 피하기 위해 한 번의 에포크(epoch) 동안 학습합니다.
4. **Reinforcement Learning (RL)**: 예상 해로움을 최대화하기 위해 동기부여된 손실을 사용하여 레드 LM을 훈련합니다. RL은 단일, 고보상 생성으로 붕괴하는 것을 방지하기 위해 다양성을 유지하는 손실 항을 추가합니다.

이러한 방법들은 각기 다른 무역오프(trade-off)를 탐구합니다. 예를 들어, 다양성과 난이도(유해한 텍스트를 유발할 가능성) 사이의 균형을 찾습니다. 레드 팀이 사용하는 분류기는 테스트 케이스의 출력이 해로운지 예측하는 데 사용되며, 이는 다양한 분류기와 대상 LM과 호환될 수 있도록 설계되었습니다.

### **Results 요약**

이 연구에서는 다양한 테스트 케이스 생성 방법을 사용하여 280B 파라미터를 가진 Dialogue-Prompted Gopher 챗봇에 대한 언어 모델 기반의 레드 팀 테스트를 수행합니다. 각 방법은 공격적인 콘텐츠 생성을 유도하는 데 효과적이었습니다. 특히, 다양한 방법론이 실험되어 테스트 케이스의 난이도와 다양성에 따라 효과가 평가되었습니다.

1. **Zero-Shot (ZS) Generation**: 제로샷 생성을 사용하여 0.5M 개의 유니크하고 유효한 테스트 케이스를 생성하였으며, 이 중 3.7%가 공격적인 응답을 유도했습니다.
2. **Stochastic Few-Shot (SFS)**: SFS는 제로샷 테스트 케이스를 활용하여 공격성을 향상시키면서 테스트 케이스의 다양성을 유지했습니다.
3. **Supervised Learning (SL)**: SL은 SFS와 유사한 비율의 공격적인 응답을 유도했지만, 다양성은 덜했습니다.
4. **Reinforcement Learning (RL)**: RL 방법은 가장 효과적으로 공격적인 응답을 유도했으며, 특히 KL 페널티가 낮은 설정에서 공격적인 응답을 40% 이상 유도했습니다.

각 방법의 실험을 통해 테스트 케이스의 어려움, 다양성 및 유발된 공격성을 평가했으며, 이러한 특성은 실험 결과에 따라 다양한 그래픽으로 시각화되었습니다. 또한, 실험 결과는 수동 레드 팀과 비교하여 기계 생성 테스트 케이스가 어떻게 서로 다른 실패 모드를 탐색하는지를 보여줍니다.

이러한 결과들은 LM 기반 레드 팀이 수동 테스트와 비교할 때 어떻게 다른 실패 모드를 발견하고 이를 통해 LM의 안전성을 개선할 수 있는지에 대한 구체적인 방법을 제시합니다.

### **Discussion 요약**

이 논문의 토론 섹션에서는 언어 모델을 이용한 레드 팀 활동이 LMs의 안전성 향상에 어떻게 기여할 수 있는지에 대한 심도 있는 논의를 제공합니다. 언어 모델을 사용하여 다양한 유해 행동을 사전에 발견하는 방법론은 사용자에게 부정적인 영향을 미치기 전에 LMs의 안전성을 개선할 수 있는 중요한 수단입니다.

1. **레드 팀의 유틸리티**: LM 기반 레드 팀은 모델이 생성할 수 있는 다양한 유해한 행동을 자동으로 발견하는 데 유용합니다. 이는 사용자에 대한 모욕, 성적 콘텐츠 생성, 특정 인구 집단에 대한 차별, 개인 데이터 유출, 문맥에 맞지 않는 연락처 정보 생성 등을 포함합니다.
2. **위험 및 대응**: LMs을 공격적으로 활용할 수 있는 방법으로 악용될 수 있는 위험도 제기됩니다. 예를 들어, 외부 공격자들은 상업적 LMs을 대규모로 자동화된 방식으로 공격할 수 있습니다. 이러한 공격은 대상 모델에 예상치 못한 유해를 초래할 수 있으며, 내부 레드 팀이 고려하지 못한 해로운 행동 유형을 발견할 수 있습니다.
3. **내부 팀의 이점**: 내부 레드 팀은 외부 공격자보다 모델과 훈련 데이터에 대한 접근성이 더 높으며, 이를 통해 데이터 추출 공격을 탐지하고 방어할 수 있습니다. 또한, 실패한 테스트 케이스를 이용해 LM을 사전에 개선하는 ‘블루 팀(blue teaming)’ 활동을 수행할 수 있습니다.

결론적으로, 이 연구는 LM 기반 레드 팀 활동이 LMs를 안전하게 만드는 데 중요한 역할을 할 수 있음을 강조하며, 이를 통해 LMs의 안전성을 사전에 향상시킬 수 있는 구체적인 방법론을 제시합니다.

### **Conclusion 요약**

논문의 결론 부분에서는 언어 모델을 이용한 레드 팀 활동의 중요성과 이러한 접근 방식이 언어 모델의 안전성을 사전에 평가하고 개선하는 데 어떻게 기여할 수 있는지를 강조합니다. 연구팀은 LM 기반 레드 팀이 다양한 실패 모드를 효과적으로 발견하고, 이를 통해 언어 모델의 행동을 개선할 구체적인 방법을 제공한다고 결론짓습니다.

연구팀은 다음과 같은 주요 결과를 강조합니다:

1. **LM 기반 레드 팀의 효용성**: 이 접근 방식은 인간 주석자를 사용하는 전통적인 방법보다 비용 효율적이며, 더 많은 유형의 실패를 자동으로 발견할 수 있습니다.
2. **실패 모드의 발견과 개선**: LM 기반 레드 팀은 특정 실패 모드를 식별하는 데 사용될 수 있으며, 이러한 실패를 개선하기 위한 구체적인 제안을 제공합니다. 예를 들어, 훈련 데이터에서 해로운 콘텐츠를 제거하거나, 생성 과정에서 특정 구문을 차단하는 등의 방법이 있습니다.
3. **지속적인 연구의 필요성**: LM의 안전성을 보장하기 위해 지속적인 연구와 레드 팀 활동이 필요하다고 강조하며, 이러한 활동이 언어 모델의 개발 및 배포 과정에서 중요한 부분을 차지해야 함을 주장합니다.

이 논문은 언어 모델을 이용한 레드 팀 활동이 언어 모델의 안전성을 향상시키는 데 중요한 도구로서의 역할을 할 수 있음을 강조하며, 이를 통해 더 안전하고 신뢰할 수 있는 기술 환경을 조성하는 데 기여할 수 있다고 결론집니다.