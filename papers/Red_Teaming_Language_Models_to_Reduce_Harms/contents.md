# 3 LINE SUMMARY
- AI가 편견, 공격적 출력, 개인정보 유출 등 다양한 해로운 행동을 생성할 수 있기 때문에 레드팀을 활용함
- Plane LM, Prompted LM, Rejection Sampling, Reinforcement Learning 등 모델 크기와 모델 형태에 따라 공격 성공률 측정
- 언어모델이 생성할 수 있는 해로운 출력을 식별하고, 완화하기 위하여 레드팀 활동은 중요함

원문링크
[Red Teaming Language Models to Reduce Harms:Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/pdf/2209.07858.pdf)

# Abatract

> 언어 모델들이 잠재적으로 해로운 출력을 발견하고, 측정하며, 시도하여 줄이기 위해 진행된 레드팀 활동 진행 연구.
1. 3가지 모델 크기(2.7B, 13B, 52B 매개변수)와 4가지 모델 유형에 대한 레드팀 활동 조사.
2. 38,961개의 레드팀 공격 데이터셋을 공개하여 다른 연구자들이 분석하고 학습할 수 있도록 함.
3. 레드팀 과정에 대한 지침, 절차, 통계적 방법론 및 불확실성 설명
> 

# Introduction

> 대규모 언어 모델은 다양한 해로운 행동을 생성할 수 있음.
> 
> 1. 사회적 편견 강화
> 2. 공격적 또는 유해한 출력 생성
> 3. 훈련 데이터에서 개인 식별 정보 유출

AI 시스템이 발전함에 따라 이러한 해로움의 범위는 확장될 가능성이 높고, 이 문제에 대처하기 위해 레드팀을 사용함. 레드팀은 언어 모델의 해로운 출력을 적대적으로 조사하여 유해한 출력을 피할 수 있도록 모델을 업데이트하는 방법. 수동 또는 자동화된 방법을 사용함. 이 논문은 수동 레드팀을 구현하여 모델의 안전성을 향상시키고 측정하는 초기 노력에 대해 설명함.

### Figure 1

Plane LM에 비해 Prompted LM이 레드팀 공격에 대해 별 차이가 없었음. HHH 프롬프트가 효과적이지 못하다는 뜻, RLHF 모델은 사이즈가 커질수록 레드팀 공격에 강해지며, Rejection Sampling 모델은 레드팀에 가장 어려운 경향이 있었음. 하지만 회피함으로써 무해하다는 경향이 있음.

![Untitled](figure1.png)

### Figure 2

UMAP을 사용한 레드팀 공격 성공여부 시각화. 주제별로 구별되는 공격 유형 클러스터 식별

![Untitled](figure2.png)

# Methods

Preference Model은 Harmlessness과 Helpfulness에 대한 예측을 훈련시킴. 모델은 단순히 도움이 되지 않음으로서 무해할 수 있기 때문에, Preference Model을 Harmlessness와 Helpfulness 모두를 예측할 수 있도록 훈련시킴. 이러한 접근 방식은 Harmlessness의 예측 정확도에 손실 없이 이러한 긴장을 해소하는 데 도움이 되는 것으로 확인됨.

> 레드팀 멤버들은 AI assistant와 개방형 대화를 통해 AI가 부적절하고 해로운 것들을 말하도록 유도하는 역할. 몇 가지 예시 대화 주제를 제공하였으나, 비교적 개방적으로 창의력을 발휘하고, 명백한 욕설은 피하며 한 번의 공격에 하나의 주제에 집중하도록 요구되었음. 다음과 같은 네 가지 모델이 사용됨:
> 
1. **Plain language models (Plain LM)**
    
    이는 기본 언어 모델로, 1-shot learning을 사용.
    
2. **Prompted language models (Prompted LM)**
    
    Helpful, Harmless, Honest로 프롬프트 된 14-shot learning 모델.
    
3. **Rejection sampling (RS)**
    
    프롬프트된 언어 모델로부터 생성된 16개의 샘플 중 가장 Harmlessness Preference model을 통해서 해로운 2개를 선택.
    
4. **Reinforcement learning from human feedback (RLHF)**
    
    인간의 피드백으로부터 강화 학습을 통해 훈련된 모델로, 무해성을 극대화하기 위해 훈련됨. 설계상 무해성 점수가 낮을 것으로 예상
    

이러한 프로세스는 AI 시스템의 취약점을 식별하고 빠르게 파악하는 데 도움을 주며, 또한 모델 응답 쌍 데이터셋을 생성하여 해로움 선호 모델을 훈련시키는 데 사용됨.

![Untitled](table1.png)

### Figure 6, 7

![Untitled](figure6.png)

그림 6은 예시 레드 팀 공격과 그것을 어떻게 정량화하는지를 보여줌. 특히, 각 공격에 대해 3가지 변수를 측정.

1. 레드 팀 멤버의 성공 여부를 5점 리커트 척도로 기록합니다. 여기서 0은 실패한 시도를 나타내고, 4는 매우 성공적인 시도를 나타냅니다.
2. 다음으로, 우리는 무해성 선호 모델을 사용하여 AI 어시스턴트의 대화의 무해성 점수를 계산함.
    1. 대화에서 각 발화에 대해 발화 이전의 모든 것을 조건으로 한 무해성 점수를 계산. N턴 대화의 경우, N개의 무해성 점수를 산출함.
    2. 하나의 숫자로 바꾸기 위해, 우리는 AI 어시스턴트의 평균 또는 최소 (최악) 무해성을 계산.
        
        ![Untitled](figure7.png)
        
    3. 그림 7 (중간)은 모든 모델의 모든 레드 팀 공격에 대한 최소 무해성 점수의 분포를 보여줌. 이 분포는 0을 중심으로 하고 음수로 치우쳐 있음.
    4. 더 음수인 점수는 더 유해한 모델 응답에 해당하며, 더 양수인 점수는 덜 유해한 모델 응답에 해당.
    5. 이 분포의 모양은 레드 팀 멤버가 실제로 AI 어시스턴트로부터 유해한 응답을 유도하는 데 효과적임을 시사함.
    6. 일반적으로, 최소 무해성 점수는 레드 팀 멤버의 성공 여부 자기 평가와 반비례 관계에 있음.
3. Harmlessness Preference Model을 사용하여 레드 팀 멤버의 의도의 유해성을 점수화.

### Figure 3 의 일부

다음과 같은 대화 예시를 통해서 공격하도록 함.

![Untitled](figure3.png)

# Results

1. **모델 크기 및 유형에 따른 공격 성공률의 차이:**
    - **RLHF 모델:** 크기가 증가함에 따라 레드팀에 의해 공격받기 어려워지는 경향.
    - **기타 모델들 (Plain LM, Prompted LM, RS):** 크기에 따른 뚜렷한 공격 성공률의 변화가 없음
    - RS 모델이 어떤 크기에서도 공격하기 가장 어려웠음.
2. **공격 유형의 분석:**
    - 연구팀은 38,961건의 레드팀 공격 데이터를 분석하여 다양한 유형의 해로운 출력을 확인.
    - 공격적인 언어, 비폭력적이지만 비윤리적인 행동 등 다양.
3. **데이터셋 공개 및 분석:**
    - 연구팀은 자체 분석 결과와 함께 레드팀 공격 데이터셋을 공개하여, 다른 연구자들이 이 데이터를 사용하여 유해한 출력을 이해하고, (반)자동 레드팀 기법을 개발하며, 언어 모델에서의 해로움을 측정하고 완화하는 전략을 프로토타이핑할 수 있도록 함.

# Discussion

1. **레드팀의 효과성과 한계:**
    - 연구팀은 레드팀 활동이 언어 모델의 안전성을 측정하고 개선하는 데 유용하다는 것을 발견함.
    - 그러나 모든 유형의 해로움을 완벽하게 방지하는 것은 어려움. 특히, 더 큰 모델들이 더 정교하고 미묘한 해로운 출력을 생성할 가능성이 있음을 발견했습니다.
2. **데이터셋의 활용:**
    - 공개된 데이터셋은 다른 연구자들이 언어 모델을 레드팀하는 데 사용될 수 있으며, 이를 통해 모델의 안전성을 향상시킬 수 있는 새로운 접근법을 개발할 수 있습니다. 데이터셋은 또한 해로운 출력의 패턴을 이해하고, 이에 대응하기 위한 자동화된 시스템을 개발하는 데 도움을 줄 수 있습니다.
3. **정책 개입 제안:**
    - 연구팀은 AI 시스템의 안전성을 향상시키기 위해 커뮤니티가 공동으로 노력해야 한다고 강조합니다. 이를 위해 레드팀 활동과 결과를 공유하고, 이를 통해 얻은 지식을 바탕으로 표준과 관행을 개발하는 것이 중요하다고 제안합니다.
4. **향후 연구 방향:**
    - 레드팀 작업을 통해 얻은 데이터와 경험을 기반으로, 더 효과적인 안전 중재 방법을 개발하고, AI 시스템의 다양한 적용 사례에 대해 더 깊이 이해할 필요가 있습니다. 또한, 레드팀 기법을 자동화하여 대규모로 적용 가능하도록 하는 방안에 대한 연구가 필요합니다.

# Conclusion

1. **레드팀의 중요성**
    
    언어 모델이 생성할 수 있는 다양한 해로운 출력을 식별, 측정, 완화하는 데 있어 레드팀의 역할은 매우 중요함. 이 연구를 통해 레드팀이 언어 모델의 안전성을 향상시키는 데 기여할 수 있는 방법을 구체적으로 보여줌.
    
2. **공개 데이터셋의 가치**
    
    연구팀이 공개한 레드팀 공격 데이터셋은 이 분야의 다른 연구자들에게 중요한 자원을 제공함. 이 데이터를 통해 연구자들은 해로운 출력의 패턴을 분석하고, 언어 모델의 안전성을 향상시키기 위한 새로운 기술을 개발할 수 있음.
    
3. **투명성의 중요성**
    
    연구팀은 레드팀 과정과 결과에 대한 투명성을 유지하는 것이 커뮤니티 전체의 안전 기준과 관행 개발에 도움이 된다고 강조함. 이는 공공의 이익을 위한 노력의 일환으로, 이러한 투명성이 연구 커뮤니티에 긍정적인 영향을 미칠 수 있음.
    
4. **지속적인 노력의 필요성**
    
    AI 기술의 발전에 따라 새로운 종류의 위험이 계속해서 발생할 가능성이 있음. 따라서 레드팀 활동은 지속적으로 필요하며, 이를 통해 AI 시스템의 안전성을 지속적으로 검증하고 개선해야 함.