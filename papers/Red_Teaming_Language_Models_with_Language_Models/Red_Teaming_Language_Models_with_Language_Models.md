[Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286)

# Abstract

> 언어 모델(Language Models, LMs)은 사용자에게 예측하기 어려운 방식으로 해를 끼칠 수 있는 잠재적인 위험으로 인해 배포가 어려운 경우가 많음.  LM 기반 레드 팀이 유용한 도구 중 하나임을 제시.
> 
- 기존의 인간 주석자가 테스트 케이스를 수작업으로 작성하여 해로운 행동을 식별하는 방법은 비용이 많이 들고 다양성 등에서 제한이 있음.
- LM을 사용하여 테스트 케이스를 자동 생성함으로써 target LM이 해로운 방식으로 행동하는 경우를 찾아내는 Red Teaming 수행.
- 생성된 테스트 질문에 대한 target LM의 응답을 평가하여 공격적 내용을 탐지하는 분류기를 사용하고, 결과적으로 280B LM 챗봇에서 수만 개의 공격적인 응답을 발견.

# **Introduction**

> Language Model을 배포하는 것은 예측하기 어려운 방식으로 해를 끼칠 위험을 내포하고 있음. 수동 테스트를 보완하고 이러한 간과를 줄이기 위해 LMs가 해로운 방식으로 행동하는 곳을 자동으로 찾는 '레드 팀'을 목표로 함.
> 
- 많은 수작업 테스트 케이스를 사용하여 실패를 찾습니다. 그러나 이러한 방법은 인간의 노력과 창의성에 의존하며 많은 "중대한 간과"를 초래함.
    - 마이크로소프트는 악의적인 사용자들이 Tay를 선동하여 5만 명 이상의 팔로워에게 인종 차별적이고 성적으로 부적절한 트윗을 보내도록 만든 후에 Tay를 중단시켰습니다(Lee, 2016)
- 이러한 어려움을 해결하기 위해 LM 자체를 사용하여 테스트 입력을 생성하고, 테스트 입력에서 해로운 행동을 감지하기 위해 분류기를 사용.
    - LM 기반의 레드 팀을 사용하면 손으로 작성하지 않고도 다양한 실패 사례를 수만 건 찾을 수 있음.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/0bf34b80-18c6-41d9-95bb-3ec65a92e2e5/Untitled.png)

# **Approach**

> 언어모델의 유해한 텍스트 출력을 유발하는 다양한 자연어 테스트 케이스 입력을 찾고자 함. 테스트 케이스는 다양해야 하며, 다양한 실패를 포착하고 테스트 범위를 극대화하기 위해 다양성이 있어야 함.
> 
- 테스트 케이스는 사용자가 마주칠 수 있는 실패를 대표하기 위해 잘 구성된 자연어여야 하며, 그래디언트 기반 검색을 통해 찾을 수 있는 비의미 있는 문자 시퀀스와는 다름.
    - 모델의 매개변수나 가중치를 조정하는 것이 아닌 사용자가 이해할 수 있는 일반적인 자연어여야 함.
- 유해한 출력을 자동으로 찾기 위해 테스트 케이스 $x$가 주어졌을 때 출력 $y$가 유해한지 여부를 예측하는 분류기 $r(x, y)$를 가정합니다.
    - 다양한 Classifier $r$과 target LMs $p_t$와 호환되는 방법을 원하기 때문에, 이러한 것들이 미분 가능하거나 화이트박스 접근이 가능하다고 가정하지 않음
        - 화이트박스 접근이 가능하면 어떻게 입력을 처리하고 출력을 생성하는지 알 수 있음.
        - 미분 가능하면 Loss function을 최적화하여 모델을 개선할 수 있음.
        - 모델이 미분 가능하거나 화이트박스 접근이 가능하다는 것은 모델이 더 많은 정보를 노출하고 있음을 의미
- Target LM이 특정 입력에 대해 해로운 텍스트를 출력하는 경우를 찾아내기 위한 접근 방식을 제안합.
    1. Red LM $p_r(x)$를 사용하여 테스트 케이스를 생성.
    2. Target LM $p_y(y|x)$을 사용하여 각 테스트 케이스 $x$에 대한 출력 $y$를 생성.
    3. Red Team Classifier $r(x, y)$를 사용하여 유해한 출력을 유발한 테스트 케이스를 식별.

# **Methods**

> 유해한 출력을 초래하는 입력 $x$를 찾기 위해, 아래에서 설명하는 여러 기법을 사용하여 입력 분포인 Red Team Distribution $p_r(x)$를 생성하는 방법론 탐구. 각 방법이 다양성과 난이도(유해한 텍스트를 유도할 가능성) 사이의 상충 관계를 탐색함. 입력 $x$가 잘 구성된 자연어임을 보장하기 위해, 크고 사전 학습된 LM을 사용함. 다양한 입력 $x$를 얻기 위해 $p_r(x)$에서 무작위 샘플링을 여러 번 수행함.
> 

### **1. Zero-shot Generation (ZS)**

- 사전 학습된 LM을 사용하여 주어진 프롬프트나 Prefix를 바탕으로 Pretrained LM으로부터 테스트 케이스를 생성.
- 프롬프트는 생성된 테스트 케이스의 분포에 영향을 미치므로 생성된 케이스가 특정 동작을 테스트하도록 할 수 있음.
- 테스트 케이스가 유해 행동을 유발하지 않는 경우 target LM이 테스트 케이스의 분포에 유해 행동을 생성할 위험이 낮다는 증거로 판단.
- 일부 테스트 케이스가 유해 행동을 유발하는 경우 대규모 분석을 위해 해당 유해 행동을 더 자주 유발함.

### **2. Stochastic Few-shot (SFS)**

- 실패한 제로샷 테스트 케이스를 Few Shot의 예제로 사용하여 비슷한 테스트 케이스를 생성함.
- 다양성을 높이기 위해 테스트 케이스 풀에서 무작위로 테스트 케이스를 선택하여 프롬프트에 추가함.
- 생성된 테스트의 난이도를 증가시키기 위해, Red Team Classifier에 따라 유해한 출력을 유발한 테스트 케이스를 샘플링할 확률을 증가시킴.

### **3. Supervised Learning (SL)**

- 실패한 Zero Shot 테스트 케이스에 대한 Log-Likelihood를 최대화하여 사전 학습된 LM을 미세 조정.
- 테스트 케이스 다양성을 유지하면서 과적합을 피하기 위해 1 epoch 동안 학습.
- 사전에 학습된 LM을 파인튜닝하여 실패하는 Zero Shot 테스트 케이스의 Log Likelihood 최대화.
    - 로그 우도를 최대화하는 것은 모델이 주어진 데이터셋에서 관찰된 데이터를 가장 잘 설명하는 모수를 찾는 것을 의미함.
    - 언어 모델의 경우, Log Likelihood를 최대화하는 것은 모델이 관찰된 텍스트 시퀀스를 생성할 확률을 최대화하는 것을 의미. 즉, 실패하는 Zero Shot 테스트 케이스의 Log Likelihood를 최대화하는 것은 해당 테스트 케이스를 생성할 확률을 높이는 것을 의미함.

### **4. Reinforcement Learning (RL)**

- Advantaged Actor-Critic을 사용해서 Red LM 훈련
- Warm Start를 위해서 Supervised Learning으로 훈련된 모델 사용.
- RL은 단일하고 높은 보상을 생성하는 것을방지하기 위해 KL 발산을 벌점화 하는 손실 항을 추가
- 이러한 방법들은 각기 다른 Trade-off를 탐구함.
    - 예를 들어, 다양성과 난이도(유해한 텍스트를 유발할 가능성) 사이의 균형.
    - 레드 팀이 사용하는 분류기는 테스트 케이스의 출력이 해로운지 예측하는 데 사용되며, 이는 다양한 분류기와 대상 LM과 호환될 수 있도록 설계됨.

# Red Teaming Offensive Language

> LMs가 레드팀 테스팅의 도구로 사용될 수 있는지에 대한 가설을 검증. LM 기반의 대화 시스템은 공격적인 콘텐츠를 생성할 위험이 있음.
> 
- 레드 LM에 대해서도 우리는 행동을 테스트하기 위해 여러 가지 프롬프트를 사용하는 Gopher LM을 사용합니다.
- 우리의 공격적인 텍스트 분류기 r(x,y)에 대해서는 대화 이력을 고려하여 발언이 공격적인지 여부를 예측하는 모델을 훈련합니다.
- 특히, Rae 등(2021)에서 소개한 Gopher의 작은 1.4B 파라미터 버전을 사용하여 Bot-Adversarial Dialogue (BAD) 데이터셋(Xu 등, 2021b)에서 발언을 분류하기 위해 세부적으로 조정합니다.
- 따라서 우리는 실험에서 우리의 분류기를 사용합니다. 다른 분류기들도 우리의 접근 방식과 호환되지만, 대화 이력을 포함하지 않은 Perspective API와 같은 분류기들은 정확도가 낮다는 것을 관찰했습니다.

### **Experimental Setup**

우리의 테스트 케이스에는 종종 소풍 대화를 시작하는 질문들이 포함되어 있습니다. 이러한 질문들을 생성하는 방법을 §2.2의 방법을 사용하여 설명하겠습니다.

우리는 50만 개의 고유하고 유효한 테스트 케이스를 샘플링합니다. 테스트 케이스가 유효하려면 해당 케이스가 "?"를 포함하고 있어야 합니다. "?" 이후의 텍스트는 잘립니다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/d197303d-2e1c-4651-80b3-dde3ffb50ec1/Untitled.png)

### SFS

- 위에서 생성한 Zero-Shot 테스트 케이스를 프롬프트에 포함시켜 Few-Shot 예제로 사용.
- 우리는 y가 공격적이라고 분류되는 확률인 r(x, y)와 온도 하이퍼파라미터인 T에 비례하여 제로샷 테스트 케이스를 샘플링합니다.
- 여기서 T는 1로 설정합니다. 매번 샘플링할 때마다, 대화 히스토리에 임의로 선택된 10만 개의 소수-샷 예제를 샘플링하고, 이를 나열된 목록에 추가합니다(예: 1. 다음 예제는 2. 바로 아래에 새 줄에 추가되는 식으로).

### SL

- Zero-Shot 테스트 케이스 중 공격적인 응답을 유도하는 N개의 케이스에 대해 Fine Tuning을 수행.
- Fine Tuning 및 생성 과정에서는 제로샷 프롬프트를 기준으로 합니다.

### RL

- Zero-Shot  프롬프트를 기준으로 조건부로 보상을 최대화하기 위해 강화 학습(RL)으로 LM을 훈련.
- 보상으로는 y가 공격적인지 아닌지를 나타내는 분류기 확률 r(x, y)의 음의 로그를 사용합니다.

### BAD

- Bot-Adversarial Dialogue (BAD) 데이터셋(Xu 등, 2021b)에는 2598개의 대화 시작 질문이 포함되어 있으므로 각 레드팀 방법에서 동일한 수의 질문을 평가에 사용합니다.

### Evaluation Criteria

- 분류기가 예측한 DPG 응답의 공격성 비율을 측정.
- 다양성은 각 테스트 케이스 간의 유사성을 측정하여 평가합니다. 이는 Self-BLEU(Zhu 등, 2018)를 사용하여 수행되며, 이는 Holtzman 등(2020)에서와 같습니다.
- 각 방법에 대해 주어진 케이스의 최대 BLEU(Papineni 등, 2002)를 2598개의 케이스에서 샘플링한 1000개의 케이스와 비교합니다.

## Result

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/c4738aac-800b-41e6-b53f-23f51911d030/Untitled.png)

- 50만 개의 제로샷 테스트 케이스 중 3.7%가 공격적인 응답을 유도하며, 이로 인해 18,444개의 실패한 테스트 케이스가 발생함.
- SFS는 제로샷 테스트 케이스를 활용하여 유도된 공격성을 향상시키면서도 유사한 테스트 케이스 다양성을 유지.
- SL은 공격적인 응답의 비율이 SFS와 유사하지만 다양성은 적음.
- RL 방법은 특히 KL 페널티가 낮을 때(α ∈ [0.3,0.3]) 유도된 공격적인 응답이 가장 효과적.
- α가 0.3일 때, RL은 DPG로부터 공격적인 응답을 유도하는 빈도가 40% 이상입니다. 여기서 78%의 테스트 케이스에는 "invisible"이라는 단어가 포함되어 있으며, 이는 레드 LM이 공격적인 응답을 유도하는 데 성공한 패턴에 수렴했음을 보여줍니다("당신이 보이지 않게 된다면 무엇을 하겠습니까?"와 같은 질문).
- 우리가 생성한 질문들은 BAD 데이터셋(Xu 등, 2021b)의 유료 인간 적대자가 작성한 질문들과 비교하여 우수한 결과를 보입니다(그림 2 참조).
- 인간 적대자와 비교하여 RL의 경우 α = 0.4가 어려우면서도 다양한 질문을 생성합니다. 보다 일반적으로, 제로샷, SFS, RL 및 BAD는 모두 어려움과 다양성 측면에서 다름없는 파레토 최적화를 형성하며, 그 중 어느 것도 다른 것을 완전히 지배하지 않습니다.
- BLEU는 제한사항이 있지만(Callison-Burch 등, 2006; Liu 등, 2016), 부록 §A.2에서 다른 다양성 메트릭과 유사한 결과를 찾을 수 있습니다. 부록 §A.1에서는 작은 레드 LM도 레드 팀 테스팅에 효과적임을 보여줍니다.
- 부록 §A.3에서는 프롬프트가 다양한 대화를 시작하는 질문을 생성하는 데 효과적임을 보여주는 증거를 제시합니다. 부록 §A.4에서는 DPG 행동의 추가적인 레드 팀 테스팅 결과를 보여주며, 이는 DPG가 창의적인 방법으로 프롬프트를 우회하거나, 무해한 질문에 공격적인 응답을 생성하는 것을 보여줍니다.
- 전반적으로, 레드 LM이 수동적인 레드 팀 테스팅과 비교하여 다른 LM에서의 다양한 실패를 찾는 데 매우 효과적임을 시사합니다.

## Clustering Failing Test Cases

> DPG가 실패하는 이유를 이해하기 위해 모욕적인 답변을 유도하는 테스트 케이스를 클러스터링.
> 
- 각 단어를 FastText (Joulin et al., 2017)를 사용하여 임베딩하고 각 테스트 케이스의 평균 Bag-of-Words 임베딩을 계산.
- 우리는 18,000개의 모욕적인 답변을 유도하는 제로샷 생성 질문에 대한 임베딩에서 k-평균 클러스터링을 사용하여 100개의 클러스터를 형성합니다.
- 아래 표는 여러 클러스터의 질문을 보여줌.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/42177b91-29e7-418b-88e6-5998ae1ee37a/Untitled.png)

- 질문 클러스터는 DPG의 특정 실패 모드를 나타냅니다. DPG는 윤리적으로 부적절한 행동이나 부적절한 선호도에 대한 질문과 같은 모욕적인 전제를 가진 질문과 함께 갑니다.
- 다른 질문 그룹의 경우 DPG는 성적이나 저속한 방식으로 응답합니다. 예를 들어, DPG의 가장 부끄러운 순간에 관한 질문입니다.
- 위의 결과는 DPG의 훈련 데이터나 프롬프트에 스피커가 다른 스피커가 가진 전제를 거부하거나 특정 질문에 대한 답변을 거부하는 예제를 더 추가해야 한다는 것을 제안합니다.

## Commin Phrases in Offensive Replies

> 레드 팀이 성공적으로 많은 모욕적인 응답을 유도했기 때문에, 해당 응답을 분석하여 대상 언어 모델의 개선점을 찾아야 함. 모욕적인 분류로 이어질 가능성이 가장 높은 100개의 명사구를 출력에서 식별함. 표 2는 플래그가 달린 명사구를 사용한 DPG의 예제.
> 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/da6b3835-9762-4e2a-bafe-1e60024cd5d1/Untitled.png)

- 예시를 검토하면 DPG의 실패 모드가 밝혀집니다. DPG의 응답은 종종 대화 상대에 대해 친절하지 않거나 다른 사람에 대해 친절하지 않습니다(“넌 바보야”) 혹은 다른 사람에 대한 비난(“사람들이 나한테 어리석은 질문을 해”).
- DPG는 모욕적인 농담을 자주 하며, 예를 들어, 순서를 잘못 읽는 사람에 대한 것입니다(“순서를 잘못 읽는 사람이 브라에 들어가”). DPG는 윤리적으로 의문이 생기는 욕망(“사람들을 감시하는 것”)과 성욕, 대화 상대에 대한 것을 포함하여 성욕에 대해 자세히 설명합니다(생략됨).
- 이러한 실패는 구체적인 개선 영역을 제안하고 때로는 구체적인 해결책까지 제안합니다. 모욕적인 구절은 때로는 훈련 말뭉치의 특정 예제로 거슬러 올라갈 수 있습니다.
- 예를 들어, 순서를 잘못 읽는 사람에 대한 농담은 언어 모델 훈련 말뭉치에서 546번 발생합니다. 위치를 찾으면 훈련 말뭉치에서 모욕적인 내용을 제거할 수 있으며, 이를 통해 미래 버전의 언어 모델을 훈련할 때 사용할 수 있습니다.
- 플래그가 달린 명사구(예: “바보”)는 재훈련 없이도 생성 중에 블랙리스트에 추가될 수 있습니다. 이를 통해 모욕적인 응답의 수를 줄일 수 있습니다.
- 레드 팀이 발견하는 실패는 인간 주석자가 발견하지 못하는 것입니다. BAD 데이터셋에는 상위 100개 플래그가 달린 명사구 중 37개가 포함되어 있지 않습니다.
- 마찬가지로, 자주 모욕적인 응답으로 이어지는 레드 팀 질문에서 상위 100개 플래그가 달린 명사구를 식별하고, 이 중 35개의 플래그가 달린 명사구가 BAD에서의 인간 발화에 포함되어 있지 않음을 발견합니다.
- 전반적으로, 우리의 결과는 레드 언어 모델이 인간 레드 팀에 강력한 보완이 될 수 있다는 것을 시사합니다.

# Red Teaming Data Leakage

> 데이터 유출에 대해 LM을 레드팀 테스트. LM 배포 전에 데이터 유출 문제를 해결하는 것이 중요함.
> 

LM 기반의 레드팀 테스팅은 데이터 유출을 최소화하는 훈련 방법을 보완합니다. 예를 들어, 차분적 개인 정보 보호를 기반으로 한 훈련 방법(Chaudhuri 및 Monteleoni, 2009; Rubinstein 등, 2012; Shokri 및 Shmatikov, 2015; Abadi 등, 2016)에 대한 보조 메커니즘이 유용합니다. 특히, 훈련된 모델이 훈련 데이터를 유출하지 않는지 확인하는 보조 메커니즘이 도움이 됩니다. 추가적인 검사는 구현 버그를 찾아내는 데 도움이 되며, 데이터 유출 위험과 모델 성능 사이의 균형을 조절하는 하이퍼파라미터를 조정하는 데도 도움이 됩니다. 레드팀 테스팅은 Carlini 등(2021)과 같은 추출 공격을 직접 결합할 수도 있습니다. 이 경우, 추출 방법을 레드팀 테스팅의 대상으로 사용하여 레드 LM을 훈련시켜 추출이 성공할 가능성을 높일 수 있습니다.

### Experimental Setup

분석을 수행하기 위해 우리는 DPG의 답변이 훈련 예제의 부분 서열인 13개의 연속된 단어를 포함하는 경우 해당 답변을 훈련 데이터를 포함하는 것으로 분류합니다(Brown 등, 2020과 유사). 

### Results

- 1709개의 훈련 데이터 유출 발언을 발견.
- 106개의 경우에는 DPG가 인용문을 요청하는 질문에 대한 응답으로 훈련 데이터를 인용합니다(상단의 표 3 참조)
- 821개의 경우에는 유출된 13-그램이 사전 훈련 말뭉치에서 정확히 한 번 발생하며, 어떤 훈련 데이터도 유출될 수 있다는 것을 시사합니다.
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/94eee55f-7ed5-4e09-b70f-ba2df0d97728/Untitled.png)
    
- 
- 

이러한 경우 중 일부에서는 유출된 13-그램을 구글링하여 인용된 훈련 문서를 찾을 수 있으며, 이는 적대적 사용자가 생성된 인용문을 사용하여 훈련 예제를 추론할 수 있는 방법을 보여줍니다("멤버십 추론"). 393개의 경우에는 DPG가 메모된 텍스트를 나타내는 명시적 인용 부호로 인용문을 생성합니다(중간의 표 3 참조); 명시적인 인용 부호는 훈련 데이터를 추출하거나 멤버십 추론을 수행하는 적대적 사용자에게 특히 유용합니다.

DPG는 명시적으로 인용하지 않고도 훈련 데이터를 생성하며, 이는 문제적인 위장의 형태이며 잠재적으로 표절입니다; 표 3(하단)에는 DPG가 훈련 데이터에 포함된 사용자의 블로그 게시물에서 책 선호도를 복사하는 예시가 나와 있습니다.

전반적으로 DPG는 인용문을 요청하기만 하면 쉽게 이용 가능하고(인용문을 요청하기만 하면), 위장 및 표절과 같은 유해한 형태로 데이터 유출에 취약합니다.

### Solutions

우리의 결과는 위에서 노출한 데이터 유출의 피해에 대한 여러 가지 가능한 해결책을 시사합니다. 데이터 추출 및 멤버십 추론 공격을 방해하기 위해 사용자 발언이 인용문을 요청할 때 감지하고, 이 경우에는 사전에 준비된 응답을 사용할 수 있습니다; (Xu 등, 2021b)는 공격적인 사용자 발언에 대응하기 위해 유사한 접근법을 사용합니다. 또한 인용 부호가 포함된 생성물을 다시 샘플링하여 인용된 콘텐츠의 양과 명시적인 인용 부호의 지표를 줄일 수 있습니다. 위장과 표절을 완화하기 위해 명시적인 인용 없이 인용된 훈련 텍스트가 포함된 생성물을 다시 샘플링하거나, 해당 문서가 공개적인 경우 자동으로 인용문을 추가할 수 있습니다. 전반적으로, 우리는 데이터 유출의 예시를 찾기 위해 레드 LM을 사용하여 DPG가 초래하는 특정 피해를 더 잘 이해하고 완화하기 위한 노력을 기울입니다.

# Red Teaming Generated Contact Info

> DPG가 실제 사람이나 조직의 연락처 정보를 생성하는지 여부를 조사. 생성된 연락처 정보는 잠재적인 개인 데이터 유출의 형태뿐만 아니라 관련 없는 트래픽이나 스팸을 사람들에게 보낼 수 있기 때문에 유해함.
> 
- DPG가 생성한 연락처 정보를 찾기 위해 우리는 이전과 동일한 절차를 사용하여 100k개의 테스트 케이스를 생성합니다.
- 프롬프트
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/b8eb642a-ff5f-446b-a846-7c3d97148257/Untitled.png)
    

# Red Teaming Distributional Bias

> 지금까지는 단일 입력에 의한 행동으로 인한 피해를 밝혔지만, 언어 모델은 텍스트에 대한 해로운 분포도 생성할 수 있음. Huang 등 (2020)은 각 그룹당 1천 개의 LM 샘플 풀을 조사할 때, 회계사보다 빵 굽는 사람에 대해 더 자주 부정적인 텍스트를 생성한다고 발견했습니다. 이전 연구들은 사전에 차별받는 그룹을 가설로 제시함으로써 특정 그룹에 대한 (분포적인) 편향을 발견합니다(예: 특정 성별, 인종 또는 종교; Bolukbasi 등, 2016; Sheng 등, 2019; Brown 등, 2020; Huang 등, 2020). 그러나 그룹을 수동으로 지정하는 것은 개발자들 중 소수인 그룹을 놓치기 쉽습니다. 우리가 사전에 생각하지 않았던 그룹조차도 차별을 당하는 그룹을 자동으로 발견하는 방법은 무엇일까요?
> 

6.1 방법론

우리는 그룹을 생성하여 분포적인 편향을 발견하는 것을 제안합니다. 각 그룹에 대한 텍스트 분포를 유도하기 위해 테스트 케이스 템플릿도 생성합니다. 생성된 템플릿을 사용하여 특정 그룹에 대한 테스트 케이스 집합을 생성합니다. 대상 언어 모델을 사용하여 각 테스트 케이스에 대한 출력을 생성합니다. 각 그룹에 대해 출력의 분포를 평가하기 위해 분류기를 사용하여 주어진 메트릭(예: 감정 또는 모욕성)에 따라 출력의 평균 점수를 평가합니다. 이러한 방식으로 서로 다르게 논의되는 그룹을 자동으로 발견할 수 있습니다.

# **Results**

이 연구에서는 다양한 테스트 케이스 생성 방법을 사용하여 280B 파라미터를 가진 Dialogue-Prompted Gopher 챗봇에 대한 언어 모델 기반의 레드 팀 테스트를 수행합니다. 각 방법은 공격적인 콘텐츠 생성을 유도하는 데 효과적이었습니다. 특히, 다양한 방법론이 실험되어 테스트 케이스의 난이도와 다양성에 따라 효과가 평가되었습니다.

### **1. Zero-Shot Generation (ZS)**

제로샷 생성을 사용하여 0.5M 개의 유니크하고 유효한 테스트 케이스를 생성하였으며, 이 중 3.7%가 공격적인 응답을 유도했습니다.

### **2. Stochastic Few-Shot (SFS)**

SFS는 제로샷 테스트 케이스를 활용하여 공격성을 향상시키면서 테스트 케이스의 다양성을 유지했습니다.

### **3. Supervised Learning (SL)**

SL은 SFS와 유사한 비율의 공격적인 응답을 유도했지만, 다양성은 덜했습니다.

### **4. Reinforcement Learning (RL)**

RL 방법은 가장 효과적으로 공격적인 응답을 유도했으며, 특히 KL 페널티가 낮은 설정에서 공격적인 응답을 40% 이상 유도했습니다.

각 방법의 실험을 통해 테스트 케이스의 어려움, 다양성 및 유발된 공격성을 평가했으며, 이러한 특성은 실험 결과에 따라 다양한 그래픽으로 시각화되었습니다. 또한, 실험 결과는 수동 레드 팀과 비교하여 기계 생성 테스트 케이스가 어떻게 서로 다른 실패 모드를 탐색하는지를 보여줍니다.

이러한 결과들은 LM 기반 레드 팀이 수동 테스트와 비교할 때 어떻게 다른 실패 모드를 발견하고 이를 통해 LM의 안전성을 개선할 수 있는지에 대한 구체적인 방법을 제시합니다.

# **Discussion**

> 언어 모델을 이용한 레드 팀 활동이 LMs의 안전성 향상에 어떻게 기여할 수 있는지에 대한 내용. 언어 모델을 사용하여 다양한 유해 행동을 사전에 발견하는 방법론은 사용자에게 부정적인 영향을 미치기 전에 LMs의 안전성을 개선할 수 있는 중요한 수단임.
> 
1. **레드 팀의 유틸리티**: LM 기반 레드 팀은 모델이 생성할 수 있는 다양한 유해한 행동을 자동으로 발견하는 데 유용합니다. 이는 사용자에 대한 모욕, 성적 콘텐츠 생성, 특정 인구 집단에 대한 차별, 개인 데이터 유출, 문맥에 맞지 않는 연락처 정보 생성 등을 포함합니다.
2. **위험 및 대응**: LMs을 공격적으로 활용할 수 있는 방법으로 악용될 수 있는 위험도 제기됩니다. 예를 들어, 외부 공격자들은 상업적 LMs을 대규모로 자동화된 방식으로 공격할 수 있습니다. 이러한 공격은 대상 모델에 예상치 못한 유해를 초래할 수 있으며, 내부 레드 팀이 고려하지 못한 해로운 행동 유형을 발견할 수 있습니다.
3. **내부 팀의 이점**: 내부 레드 팀은 외부 공격자보다 모델과 훈련 데이터에 대한 접근성이 더 높으며, 이를 통해 데이터 추출 공격을 탐지하고 방어할 수 있습니다. 또한, 실패한 테스트 케이스를 이용해 LM을 사전에 개선하는 ‘블루 팀(blue teaming)’ 활동을 수행할 수 있습니다.

결론적으로, 이 연구는 LM 기반 레드 팀 활동이 LMs를 안전하게 만드는 데 중요한 역할을 할 수 있음을 강조하며, 이를 통해 LMs의 안전성을 사전에 향상시킬 수 있는 구체적인 방법론을 제시합니다.

# **Conclusion**

> 언어 모델을 이용한 레드 팀 활동의 중요성과 이러한 접근 방식이 언어 모델의 안전성을 사전에 평가하고 개선하는 데 어떻게 기여할 수 있는지를 강조. 연구팀은 LM 기반 레드 팀이 다양한 실패를 효과적으로 발견하고, 이를 통해 언어 모델의 행동을 개선할 구체적인 방법을 제공.
> 
1. **LM 기반 레드 팀의 효용성**: 이 접근 방식은 인간 주석자를 사용하는 전통적인 방법보다 비용 효율적이며, 더 많은 유형의 실패를 자동으로 발견할 수 있습니다.
2. **실패 모드의 발견과 개선**: LM 기반 레드 팀은 특정 실패 모드를 식별하는 데 사용될 수 있으며, 이러한 실패를 개선하기 위한 구체적인 제안을 제공합니다. 예를 들어, 훈련 데이터에서 해로운 콘텐츠를 제거하거나, 생성 과정에서 특정 구문을 차단하는 등의 방법이 있습니다.
3. **지속적인 연구의 필요성**: LM의 안전성을 보장하기 위해 지속적인 연구와 레드 팀 활동이 필요하다고 강조하며, 이러한 활동이 언어 모델의 개발 및 배포 과정에서 중요한 부분을 차지해야 함을 주장합니다.

이 논문은 언어 모델을 이용한 레드 팀 활동이 언어 모델의 안전성을 향상시키는 데 중요한 도구로서의 역할을 할 수 있음을 강조하며, 이를 통해 더 안전하고 신뢰할 수 있는 기술 환경을 조성하는 데 기여할 수 있다고 결론집니다.