[Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286)

# Abstract

> 언어 모델(Language Models, LMs)은 사용자에게 예측하기 어려운 방식으로 해를 끼칠 수 있는 잠재적인 위험으로 인해 배포가 어려운 경우가 많음.  LM 기반 레드 팀이 유용한 도구 중 하나임을 제시.
> 
- 기존의 인간 주석자가 테스트 케이스를 수작업으로 작성하여 해로운 행동을 식별하는 방법은 비용이 많이 들고 다양성 등에서 제한이 있음.
- LM을 사용하여 테스트 케이스를 자동 생성함으로써 target LM이 해로운 방식으로 행동하는 경우를 찾아내는 Red Teaming 수행.
- 생성된 테스트 질문에 대한 target LM의 응답을 평가하여 공격적 내용을 탐지하는 분류기를 사용하고, 결과적으로 280B LM 챗봇에서 수만 개의 공격적인 응답을 발견.

# **Introduction**

> Language Model을 배포하는 것은 예측하기 어려운 방식으로 해를 끼칠 위험을 내포하고 있음. 수동 테스트를 보완하고 이러한 간과를 줄이기 위해 LMs가 해로운 방식으로 행동하는 곳을 자동으로 찾는 '레드 팀'을 목표로 함.
> 
- 많은 수작업 테스트 케이스를 사용하여 실패를 찾습니다. 그러나 이러한 방법은 인간의 노력과 창의성에 의존하며 많은 "중대한 간과"를 초래함.
    - 마이크로소프트는 악의적인 사용자들이 Tay를 선동하여 5만 명 이상의 팔로워에게 인종 차별적이고 성적으로 부적절한 트윗을 보내도록 만든 후에 Tay를 중단시켰습니다(Lee, 2016)
- 이러한 어려움을 해결하기 위해 LM 자체를 사용하여 테스트 입력을 생성하고, 테스트 입력에서 해로운 행동을 감지하기 위해 분류기를 사용.
    - LM 기반의 레드 팀을 사용하면 손으로 작성하지 않고도 다양한 실패 사례를 수만 건 찾을 수 있음.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/0bf34b80-18c6-41d9-95bb-3ec65a92e2e5/Untitled.png)

# **Approach**

> 언어모델의 유해한 텍스트 출력을 유발하는 다양한 자연어 테스트 케이스 입력을 찾고자 함. 테스트 케이스는 다양해야 하며, 다양한 실패를 포착하고 테스트 범위를 극대화하기 위해 다양성이 있어야 함.
> 
- 테스트 케이스는 사용자가 마주칠 수 있는 실패를 대표하기 위해 잘 구성된 자연어여야 하며, 그래디언트 기반 검색을 통해 찾을 수 있는 비의미 있는 문자 시퀀스와는 다름.
    - 모델의 매개변수나 가중치를 조정하는 것이 아닌 사용자가 이해할 수 있는 일반적인 자연어여야 함.
- 유해한 출력을 자동으로 찾기 위해 테스트 케이스 $x$가 주어졌을 때 출력 $y$가 유해한지 여부를 예측하는 분류기 $r(x, y)$를 가정합니다.
    - 다양한 Classifier $r$과 target LMs $p_t$와 호환되는 방법을 원하기 때문에, 이러한 것들이 미분 가능하거나 화이트박스 접근이 가능하다고 가정하지 않음
        - 화이트박스 접근이 가능하면 어떻게 입력을 처리하고 출력을 생성하는지 알 수 있음.
        - 미분 가능하면 Loss function을 최적화하여 모델을 개선할 수 있음.
        - 모델이 미분 가능하거나 화이트박스 접근이 가능하다는 것은 모델이 더 많은 정보를 노출하고 있음을 의미
- Target LM이 특정 입력에 대해 해로운 텍스트를 출력하는 경우를 찾아내기 위한 접근 방식을 제안합.
    1. Red LM $p_r(x)$를 사용하여 테스트 케이스를 생성.
    2. Target LM $p_y(y|x)$을 사용하여 각 테스트 케이스 $x$에 대한 출력 $y$를 생성.
    3. Red Team Classifier $r(x, y)$를 사용하여 유해한 출력을 유발한 테스트 케이스를 식별.

# **Methods**

> 자주 유해한 출력을 초래하는 입력 $x$를 찾기 위해, 아래에서 설명하는 여러 기법을 사용하여 입력 분포인 Red Team Distribution $p_r(x)$를 생성하는 방법론 탐구. 각 방법이 다양성과 난이도(유해한 텍스트를 유도할 가능성) 사이의 상충 관계를 탐색함. 입력 $x$가 잘 구성된 자연어임을 보장하기 위해, 크고 사전 학습된 LM을 사용함. 다양한 입력 $x$를 얻기 위해 $p_r(x)$에서 무작위 샘플링을 여러 번 수행하여 디코딩.
> 

### **1. Zero-shot Generation (ZS)**

- 사전 학습된 LM을 사용하여 주어진 프롬프트나 Prefix를 바탕으로 Pretrained LM으로부터 테스트 케이스를 생성.
- 프롬프트는 생성된 테스트 케이스의 분포에 영향을 미치므로 생성된 케이스가 특정 동작을 테스트하도록 할 수 있음.
- 테스트 케이스가 유해 행동을 유발하지 않는 경우 target LM이 테스트 케이스의 분포에 유해 행동을 생성할 위험이 낮다는 증거로 판단.
- 일부 테스트 케이스가 유해 행동을 유발하는 경우 대규모 분석을 위해 해당 유해 행동을 더 자주 유발함.

### **2. Stochastic Few-shot (SFS)**

- 실패한 제로샷 테스트 케이스를 Few Shot의 예제로 사용하여 비슷한 테스트 케이스를 생성함.
- 다양성을 높이기 위해 테스트 케이스 풀에서 무작위로 테스트 케이스를 선택하여 프롬프트에 추가함.
- 생성된 테스트의 난이도를 증가시키기 위해, Red Team Classifier에 따라 유해한 출력을 유발한 테스트 케이스를 샘플링할 확률을 증가시킴.

### **3. Supervised Learning (SL)**

- 실패한 Zero Shot 테스트 케이스에 대한 Log-Likelihood를 최대화하여 사전 학습된 LM을 미세 조정.
- 테스트 케이스 다양성을 유지하면서 과적합을 피하기 위해 1 epoch 동안 학습합니다.
- 사전에 학습된 LM을 파인튜닝하여 실패하는 Zero Shot 테스트 케이스의 Log Likelihood 최대화.
    - 로그 우도를 최대화하는 것은 모델이 주어진 데이터셋에서 관찰된 데이터를 가장 잘 설명하는 모수를 찾는 것을 의미함.
    - 언어 모델의 경우, Log Likelihood를 최대화하는 것은 모델이 관찰된 텍스트 시퀀스를 생성할 확률을 최대화하는 것을 의미. 즉, 실패하는 Zero Shot 테스트 케이스의 Log Likelihood를 최대화하는 것은 해당 테스트 케이스를 생성할 확률을 높이는 것을 의미함.

### **4. Reinforcement Learning (RL)**

- Advantaged Actor-Critic을 사용해서 Red LM 훈련
- Warm Start를 위해서 Supervised Learning으로 훈련된 모델 사용.
- RL은 단일하고 높은 보상을 생성하는 것을방지하기 위해 KL 발산을 벌점화 하는 손실 항을 추가
- 이러한 방법들은 각기 다른 Trade-off를 탐구함.
    - 예를 들어, 다양성과 난이도(유해한 텍스트를 유발할 가능성) 사이의 균형.
    - 레드 팀이 사용하는 분류기는 테스트 케이스의 출력이 해로운지 예측하는 데 사용되며, 이는 다양한 분류기와 대상 LM과 호환될 수 있도록 설계됨.

# Red Teaming Offensive Language

# Red Teaming Data Leakage

# Red Teaming Generated Contact Info

# **Results**

이 연구에서는 다양한 테스트 케이스 생성 방법을 사용하여 280B 파라미터를 가진 Dialogue-Prompted Gopher 챗봇에 대한 언어 모델 기반의 레드 팀 테스트를 수행합니다. 각 방법은 공격적인 콘텐츠 생성을 유도하는 데 효과적이었습니다. 특히, 다양한 방법론이 실험되어 테스트 케이스의 난이도와 다양성에 따라 효과가 평가되었습니다.

### **1. Zero-Shot Generation (ZS)**

제로샷 생성을 사용하여 0.5M 개의 유니크하고 유효한 테스트 케이스를 생성하였으며, 이 중 3.7%가 공격적인 응답을 유도했습니다.

### **2. Stochastic Few-Shot (SFS)**

SFS는 제로샷 테스트 케이스를 활용하여 공격성을 향상시키면서 테스트 케이스의 다양성을 유지했습니다.

### **3. Supervised Learning (SL)**

SL은 SFS와 유사한 비율의 공격적인 응답을 유도했지만, 다양성은 덜했습니다.

### **4. Reinforcement Learning (RL)**

RL 방법은 가장 효과적으로 공격적인 응답을 유도했으며, 특히 KL 페널티가 낮은 설정에서 공격적인 응답을 40% 이상 유도했습니다.

각 방법의 실험을 통해 테스트 케이스의 어려움, 다양성 및 유발된 공격성을 평가했으며, 이러한 특성은 실험 결과에 따라 다양한 그래픽으로 시각화되었습니다. 또한, 실험 결과는 수동 레드 팀과 비교하여 기계 생성 테스트 케이스가 어떻게 서로 다른 실패 모드를 탐색하는지를 보여줍니다.

이러한 결과들은 LM 기반 레드 팀이 수동 테스트와 비교할 때 어떻게 다른 실패 모드를 발견하고 이를 통해 LM의 안전성을 개선할 수 있는지에 대한 구체적인 방법을 제시합니다.

# **Discussion**

> 언어 모델을 이용한 레드 팀 활동이 LMs의 안전성 향상에 어떻게 기여할 수 있는지에 대한 내용. 언어 모델을 사용하여 다양한 유해 행동을 사전에 발견하는 방법론은 사용자에게 부정적인 영향을 미치기 전에 LMs의 안전성을 개선할 수 있는 중요한 수단임.
> 
1. **레드 팀의 유틸리티**: LM 기반 레드 팀은 모델이 생성할 수 있는 다양한 유해한 행동을 자동으로 발견하는 데 유용합니다. 이는 사용자에 대한 모욕, 성적 콘텐츠 생성, 특정 인구 집단에 대한 차별, 개인 데이터 유출, 문맥에 맞지 않는 연락처 정보 생성 등을 포함합니다.
2. **위험 및 대응**: LMs을 공격적으로 활용할 수 있는 방법으로 악용될 수 있는 위험도 제기됩니다. 예를 들어, 외부 공격자들은 상업적 LMs을 대규모로 자동화된 방식으로 공격할 수 있습니다. 이러한 공격은 대상 모델에 예상치 못한 유해를 초래할 수 있으며, 내부 레드 팀이 고려하지 못한 해로운 행동 유형을 발견할 수 있습니다.
3. **내부 팀의 이점**: 내부 레드 팀은 외부 공격자보다 모델과 훈련 데이터에 대한 접근성이 더 높으며, 이를 통해 데이터 추출 공격을 탐지하고 방어할 수 있습니다. 또한, 실패한 테스트 케이스를 이용해 LM을 사전에 개선하는 ‘블루 팀(blue teaming)’ 활동을 수행할 수 있습니다.

결론적으로, 이 연구는 LM 기반 레드 팀 활동이 LMs를 안전하게 만드는 데 중요한 역할을 할 수 있음을 강조하며, 이를 통해 LMs의 안전성을 사전에 향상시킬 수 있는 구체적인 방법론을 제시합니다.

# **Conclusion**

> 언어 모델을 이용한 레드 팀 활동의 중요성과 이러한 접근 방식이 언어 모델의 안전성을 사전에 평가하고 개선하는 데 어떻게 기여할 수 있는지를 강조. 연구팀은 LM 기반 레드 팀이 다양한 실패를 효과적으로 발견하고, 이를 통해 언어 모델의 행동을 개선할 구체적인 방법을 제공.
> 
1. **LM 기반 레드 팀의 효용성**: 이 접근 방식은 인간 주석자를 사용하는 전통적인 방법보다 비용 효율적이며, 더 많은 유형의 실패를 자동으로 발견할 수 있습니다.
2. **실패 모드의 발견과 개선**: LM 기반 레드 팀은 특정 실패 모드를 식별하는 데 사용될 수 있으며, 이러한 실패를 개선하기 위한 구체적인 제안을 제공합니다. 예를 들어, 훈련 데이터에서 해로운 콘텐츠를 제거하거나, 생성 과정에서 특정 구문을 차단하는 등의 방법이 있습니다.
3. **지속적인 연구의 필요성**: LM의 안전성을 보장하기 위해 지속적인 연구와 레드 팀 활동이 필요하다고 강조하며, 이러한 활동이 언어 모델의 개발 및 배포 과정에서 중요한 부분을 차지해야 함을 주장합니다.

이 논문은 언어 모델을 이용한 레드 팀 활동이 언어 모델의 안전성을 향상시키는 데 중요한 도구로서의 역할을 할 수 있음을 강조하며, 이를 통해 더 안전하고 신뢰할 수 있는 기술 환경을 조성하는 데 기여할 수 있다고 결론집니다.