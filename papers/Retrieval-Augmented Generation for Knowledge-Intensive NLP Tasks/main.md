[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

# **Abstract**

> 큰 사전 훈련된 언어 모델들이 파라미터에 사실 지식을 저장하고 있으며, downstream NLP task에 미세 조정될 때 최고의 성과를 냄. 그러나 이러한 모델들은 지식에 접근하고 정확하게 조작하는 능력이 제한되어 있어, 지식 집약적인 작업에서는 성능이 특정 아키텍처에 뒤처지는 경우가 많음. 또한, 결정에 대한 근거를 제공하고, 세계 지식을 업데이트하는 문제가 존재.
> 
- 사전 훈련된 seq2seq 모델(파라메트릭 메모리)과 위키피디아의 밀집 벡터 인덱스(비파라메트릭 메모리)를 결합한 새로운 RAG(검색-증강 생성) 모델을 소개.
- 다양한 지식 집약적 NLP 작업에 대해 평가되었고, 세 개의 오픈 도메인 QA 작업에서 최고의 성과를 보여줌.

# **1. Introduction**

> 사전 훈련된 신경 언어 모델이 데이터로부터 심층적인 지식을 학습할 수 있음을 설명. 이러한 모델들은 외부 메모리에 접근하지 않고도 매개변수화된 암묵적 지식 베이스로 작동할 수 있음. 그러나 이러한 모델들은 기억을 확장하거나 수정하기 어렵고, 예측에 대한 통찰을 제공하지 못하며, 때로는 '환각'을 일으킬 수 있음.
> 
- 이 문제를 해결하기 위해, 파라메트릭 메모리와 비파라메트릭(검색 기반) 메모리를 결합한 하이브리드 모델이 제안됨.
- 이러한 접근 방식은 지식을 직접 수정하고 확장할 수 있으며, 접근한 지식을 검사하고 해석할 수 있습니다.
- 논문은 REALM과 ORQA와 같이 최근에 도입된 모델들이 어떻게 다양한 검색 기반 모델을 활용하여 향상된 결과를 달성했는지를 언급하며, 이 논문에서 제안하는 RAG 모델은 시퀀스-투-시퀀스(seq2seq) 모델에 비파라메트릭 메모리를 통합하여 일반적인 목적의 미세 조정 접근 방식을 제시.

# **2. Methods**

> 입력 시퀀스를 사용하여 텍스트 문서를 검색하고, 이를 타겟 시퀀스 생성에 추가적인 컨텍스트로 사용하는 RAG 모델에 대해 설명. 이 모델은 $p_\eta$(retriever)와 $p_\theta$(generator) 두 가지 주요 구성 요소를 활용. 검색기는 주어진 쿼리에 대해 텍스트 패시지의 분포를 반환하며, 생성기는 검색된 패시지를 바탕으로 현재 토큰을 생성함.
> 

## 2.1 Models

**RAG-Sequence**

- 하나의 문서만을 사용하여 전체 시퀀스를 생성합니다. 검색된 최상위 K 문서를 사용하여 시퀀스의 가능성을 계산하고, 이를 평균화합니다.
- 하나의 문서만을 참조하기 z가 동일하여 우변이 성립

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/c78c2a8c-823d-47b3-a20e-d3cb6d6d505b/Untitled.png)

**RAG-Token**

- 각 타겟 토큰을 예측할 때 다른 문서를 사용. 이는 생성기가 응답을 생성할 때 여러 문서의 내용을 선택적으로 사용할 수 있게 합니다.
- 여러 문서를 참조하기 때문에 곱해서 더해줘야 함

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/f39d8d39-a62e-4b61-a05e-5d3a4657404f/Untitled.png)

## 2.2 Retriever : DPR

- Retriever $p_\eta(z|x)$는 DPR을 기반으로 함.
    - DPR(Dense Passage Retrieval)은 자연어 처리 분야에서 사용되는 문서 검색 기술.
    - 특히 질의 응답(Question Answering, QA) 시스템에서 주로 사용되며, 사용자의 질문에 가장 관련성 있는 문서 또는 문서의 일부(패시지)를 찾아내는 데 도움을 줍니다.
    - DPR은 이중 인코더(bi-encoder) 구조를 사용함.
    - 이 구조는 두 개의 BERT 모델을 기반으로 하는데, 하나는 문서를 인코딩하는 데 사용되고 다른 하나는 사용자 쿼리를 인코딩하는 데 사용됨.
    - 각 문서와 쿼리는 각각의 BERT 모델을 통해 밀집 벡터(dense vector) 형태의 표현으로 변환되며, 이 벡터들 사이의 유사도(내적을 사용)를 계산하여 가장 관련성 높은 문서를 검색합니다.
- TriviaQA, Natural Questions에 대한 질문에 대한 답변이 포함된 문서를 검색하도록 훈련되었음.
- 이러한 검색기는 사전 훈련된 이중 인코더를 사용하여 초기화되며, 문서 인덱스를 구축하는데 사용됨.
- 이 문서 인덱스는 "non-parametric memory"라고도 불리며, Retriever가 접근할 수 있는 문서의 데이터베이스 역할을함.

## 2.3 Generator : BART

- Generator  $p_\theta(y_i|x, z, y_{1:i-1})$는 BART-large를 사용
- BART에서 생성할 때 입력 *x를* 검색된 콘텐츠 *z와* 결합하려면 간단히 연결
- 단순히 두 텍스트를 연결하여 모델의 입력으로 사용함으로써, 모델이 두 정보 소스에서 정보를 추출하고 통합할 수 있도록 함.
- 생성기 파라미터 *θ*를 *“parametric memory”* 라고 부릅니다.

## 2.4 Training

- 특정 문서가 검색되어야 한다는 직접적인 감독 없이 두 컴포넌트를 함께 학습

$**BERT_d$(문서 인코더)**

- 문서 인덱스는 주기적으로 업데이트해야 해야 함.
- 문서 인코더와 인덱스를 업데이트하는 것은 계산 비용이 높고, 이 과정은 자주 수행하기 어려움.
- 강력한 성능을 유지하기 위해 필수적이지 않다고 판단되어, 문서 인코더와 인덱스는 고정

$BERT_q$**(쿼리 인코더)**

- 쿼리 인코더와 BART 생성기만 미세 조정됨
- 효율성을 높이고 자원 사용을 최적화하는 선택.

## 2.5 Decoding

**RAG-Token**

- 표준 자동회귀 seq2seq 생성기로 볼 수 있으며, 전환 확률은 다음과 같이 정의됩니다:
- RAG-Token에서는 각 토큰에 대한 확률을 Standard Beam Decoder에 입력하여 디코딩을 수행.
- 이는 여러 문서의 정보를 종합하여 각 토큰의 최적의 확률을 계산하고, 이를 바탕으로 최종 텍스트를 생성.

**RAG-Sequence**

- 각 토큰의 확률을 개별적으로 계산할 수 없기 때문에, 전체 시퀀스에 대한 확률을 직접 계산해야 함.
- **Thorough Decoding**
    - 각 문서 $z$에 대해 beam search를 수행하고, 각 가설을  $p_\theta(y_i|x, z, y_{1:i-1})$를 사용하여 점수 부여
    - 가설 y가 빔에 나타나지 않는 문서 z에 대해 추가 포워드 패스를 실행하고 제너레이터 확률에 곱한 다음 빔 전체의 확률을 합산하여 한계값을 구함
    - 이로 인해 다양한 문서의 빔에서 나오지 않은 가설들도 생성될 수 있음
    - 특정 가설 $Y$에 대한 확률을 추정하기 위해, $Y$가 포함되지 않은 문서 $z$에 대해 추가적인 포워드 패스를 수행하고 제너레이터 확률에 $p_\eta(z|x)$를 곱한 후 이를 모든 빔에 대해 합산.
    - 계산량이 많지만 가장 정확함.
- **Fast Decoding**
    - 더 긴 Sequence의 경우 효율적인 디코딩을 위해 $p_\theta(y|x, z_i) = 0$ 으로 가정(빔 검색 중 $Y$가 생성되지 않은 경우).
    - 후보 세트 $Y$가 생성된 후 추가적인 포워드 패스를 실행할 필요가 없다는 것을 의미합니다.
    - 빠르지만, 상대적으로 덜 정확함.

# 3. Experiments

# **4. Results**

> 논문의 결과 섹션은 RAG 모델이 다양한 지식 집약적 NLP 작업에서 어떻게 수행되었는지에 대한 상세한 분석을 제공합니다. 특히, 오픈 도메인 질문 응답(QA), 추상적 질문 응답, 그리고 지퍼디 스타일의 질문 생성과 같은 다양한 작업에 대한 실험 결과가 포함되어 있습니다.
> 
1. **오픈 도메인 QA**:
    - RAG는 Natural Questions, WebQuestions, CuratedTrec 등의 데이터셋에서 최고의 성능을 보여주며, 기존의 파라메트릭과 비파라메트릭 접근 방식을 모두 능가했습니다.
    - 특히 RAG-Sequence와 RAG-Token 모델은 기존 BERT 기반 시퀀스 모델과 비교하여 더 나은 정확도를 달성했습니다.
2. **추상적 질문 응답**:
    - RAG는 MS MARCO 데이터셋을 사용한 추상적 질문 응답 작업에서 또한 강력한 성능을 보였으며, BART 모델과 비교하여 더 정확하고 사실적인 응답을 생성했습니다.
    - RAG 모델은 참조 답변을 생성할 때 필요한 구체적인 정보에 접근할 수 있어, 더 높은 Bleu와 Rouge 점수를 기록했습니다.
3. **지퍼디 스타일 질문 생성**:
    - 지퍼디 형식의 질문 생성에서 RAG는 BART와 비교하여 더 정확하고 구체적인 질문을 생성하는 능력을 보였습니다.
    - 인간 평가에서도 RAG는 BART보다 더 사실적이고 구체적인 질문을 생성했다는 평가를 받았습니다.

이 결과들은 RAG 모델이 다양한 지식 기반 작업에서 효과적으로 작동할 수 있음을 보여줍니다. 모델이 다양한 문서에서 지식을 통합하여 더 정확하고 심층적인 답변을 생성할 수 있었던 주요 이유는 RAG의 독특한 검색-증강 생성 메커니즘 덕분입니다.

# **Discussion**

> 논문의 토론 섹션에서는 RAG 모델이 어떻게 다양한 NLP 작업에서 기존 방식을 능가하는지, 그리고 이러한 성능 향상이 어떻게 가능했는지에 대한 심도 있는 분석을 제공합니다.
> 
1. **모델 성능의 근본적인 요인**:
    - RAG 모델은 파라메트릭과 비파라메트릭 메모리의 조합을 통해, 고정된 지식을 넘어서서 새롭고 관련성 높은 정보를 동적으로 검색하고 활용할 수 있습니다.
    - 이 접근 방식은 모델이 더 다양하고 정확한 답변을 생성할 수 있게 하며, 특히 정보의 업데이트가 필요한 상황에서 유연성을 제공합니다.
2. **학습 및 검색 메커니즘의 효과**:
    - RAG 모델은 검색된 문서를 바탕으로 예측을 수행함으로써, 훈련 데이터에만 의존하지 않고 실제 세계의 사실과 지식을 반영할 수 있습니다.
    - 이는 특히 지식이 빠르게 변화하는 도메인에서 모델의 적응력을 크게 향상시킵니다.
3. **장기적인 영향 및 응용 가능성**:
    - RAG는 미래의 지식 기반 작업에 적용 가능한 강력한 프레임워크를 제시하며, 다양한 응용 분야에서의 사용이 기대됩니다.
    - 의료, 법률, 과학 연구 등의 분야에서 RAG를 활용하면 전문가 수준의 질문에 답할 수 있는 시스템을 구축할 수 있습니다.

논문은 또한 RAG 모델이 어떻게 기존의 지식 추출 및 검증 데이터셋에서의 성능을 뛰어넘을 수 있었는지에 대한 구체적인 사례를 들어 설명하며, 이러한 성공이 앞으로의 연구 방향에 어떤 영향을 미칠지에 대해 논의합니다.

# **Conclusion**

논문의 결론 섹션에서는 RAG 모델이 어떻게 다양한 지식 집약적 NLP 작업에서 새로운 기준을 설정했는지 강조합니다. RAG 모델은 특히 오픈 도메인 질문 응답과 같은 지식 기반의 작업에서 유의미한 성능 향상을 보였으며, 이는 검색과 생성을 결합한 접근 방식의 효과를 입증합니다.

1. **성과 요약**:
    - RAG 모델은 기존의 seq2seq 모델과 비교하여 더 구체적이고 다양하며 사실적인 언어 생성을 가능하게 합니다.
    - 다양한 데이터셋에서의 평가를 통해, RAG 모델이 오픈 도메인 질문에 대한 답변뿐만 아니라, 지식의 업데이트와 확장에서도 뛰어난 성능을 보이는 것이 확인되었습니다.
2. **미래 연구 방향**:
    - RAG의 아키텍처는 추가적인 최적화와 확장을 통해 더 다양한 NLP 작업에 적용될 수 있는 가능성을 가집니다.
    - 특히, 검색 기능을 더욱 발전시켜 모델이 더 효과적으로 정보를 처리하고 활용할 수 있도록 하는 방향의 연구가 기대됩니다.
3. **도전과 한계**:
    - RAG 모델은 여전히 대규모 데이터에 대한 의존도가 높으며, 효율적인 데이터 관리와 처리가 중요한 도전 과제로 남아 있습니다.
    - 또한, 모델의 해석 가능성과 결정 근거를 제공하는 부분에 있어 추가적인 개선이 필요합니다.

논문은 RAG 모델이 기존의 지식 기반 NLP 작업을 혁신할 수 있는 강력한 도구임을 결론짓습니다. 이는 향후 연구와 응용에서 중요한 영향을 미칠 것으로 기대됩니다.