


# Abstract

> Sequence transduction 모델들이 복잡한 순환 신경망(RNN) 또는 합성곱 신경망(CNN) 기반으로 구성되어 있으며, 이러한 모델들은 인코더와 디코더를 포함하고 있음. 이 모델들 중 최고 성능을 보이는 모델들은 주로 인코더와 디코더를 연결하는 어텐션 메커니즘을 사용. 이 논문에서는 어텐션 메커니즘만을 사용하는 새로운 간단한 네트워크 아키텍처인 "Transformer"를 제안.
> 
- sequence transduction은 입력 시퀀스를 받아서 다른 시퀀스로 변환하는 작업
- RNN과 CNN을 완전히 배제하고 있으며, 두 가지 기계 번역 과제에서 이 모델이 더 높은 품질을 제공하면서도 병렬화가 더 잘 되고 훈련 시간이 크게 단축된다는 것을 보여줌
- WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU 점수를 기록하여 기존 최고 결과보다 2 BLEU 이상 개선된 성과를 보였고, 영어-프랑스어 번역 작업에서도 41.0의 BLEU 점수를 달성하여 단일 모델 상태에서 최고 기록을 세움

# Introduction

> 시퀀스 모델링과 변환 문제에서는 RNN, LSTM, 그리고 GRU가 대표적인 접근법. 이러한 모델들은 언어 모델링 및 기계 번역과 같은 문제에 특히 효과적임. 하지만, RNN 기반 모델들은 이전 hidden state $h_{t-1}$을 통해 hidden state $h_t$를 생성함. 각 위치에 대한 계산은 이전 위치의 상태와 현재 위치의 입력에 따라 결정되며 이러한 순차적인 계산 방식은 특히 긴 시퀀스의 경우에는 병렬화가 어려움.
> 
- Attentioin 메커니즘은 input 또는 output sequence 내의 거리와 관계없이 종속성을 모델링할 수 있게 해주며, 여러 시퀀스 모델링 및 변환 과제에서 중요한 부분이 되었습니다.
- 전적으로 Attention 메커니즘에 의존하여 입력 및 출력 간의 전역 종속성을 생성한다는 점을 강조합니다.

# Model Architecture

> Transformer는 대부분의 경쟁력 있는 시퀀스 변환 모델과 마찬가지로 Encoder-Decoder 구조를 따릅니다. 인코더는 심볼 표현의 입력 시퀀스를 연속적인 표현의 시퀀스로 매핑하며, 디코더는 이를 바탕으로 하나씩 심볼로 이루어진 출력 시퀀스를 생성함. Transformer는 인코더와 디코더 모두에 스택된 Self-Attention Point-Wise Fully Connected Layer를 사용함.
> 

## 3.1 Encoder and Decoder Stacks

- Encoder는 동일한 N = 6 레이어로 구성된 스택으로 이루어져 있음.
- 각 레이어는 두 개의 서브 레이어로 구성되어 있음
    - 첫 번째는 Multi-Head Self-Attention 메커니즘이고
    - 두 번째는 간단한 Position-Wise Fully Connected Feed-Forward Network
- 각 서브 레이어 주위에는 residual connection이 있고, Layer Normalization이 이어집니다.
- Decoder도 동일한 N = 6 레이어로 구성된 스택으로 이루어져 있음
    - 추가적으로 인코더 출력에 대한 멀티 헤드 어텐션 서브 레이어가 있음.
    - Decoder의 Self-Attention 서브 레이어에서는 각 위치가 이후 위치에 접근하지 못하도록 마스킹을 적용하여, 각 위치 i에 대한 예측이 이전 위치에만 의존하도록 함.

## **3.2 Attention**

- Attention은 Query와 Key - Value 쌍을 출력에 매핑하는 것.
- 여기서 Query, Key, Value, Output은 모두 벡터. 출력은 Value의 Weighted Sum으로 계산되며, 각 Value에 할당된 가중치는 Query와 해당 Key의 호환성 함수에 의해 계산됩니다.

### **3.2.1 Scaled Dot-Product Attention**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/19fd67b1-0aa7-45f9-9b6d-a8b441f60733/3b940c33-c194-4ad6-afd8-9aa414cf8c7a/Untitled.png)

### **3.2.2 Multi-Head Attention**

### **3.2.3 Applications of Attention in our Model**

- 어텐션 기능은 주어진 쿼리, 키, 값에 대한 가중합을 수행합니다. 이 논문에서는 "스케일된 도트-프로덕트 어텐션"을 사용합니다. 쿼리, 키, 값 모두 행렬로 취급하며, 행렬 간의 도트 프로덕트를 계산해 가중치를 생성하고, 이를 바탕으로 가중합을 구합니다.
- Transformer에서는 "멀티 헤드 어텐션"을 사용하여 서로 다른 표현 하위 공간에 동시에 집중할 수 있도록 합니다. 이는 쿼리, 키, 값에 대한 다양한 학습된 선형 투영을 통해 각각을 병렬로 어텐션에 적용하고, 최종적으로 결합된 출력을 만듭니다. 이는 자기 어텐션을 인코더-디코더 어텐션 레이어, 인코더의 자기 어텐션 레이어, 디코더의 자기 어텐션 레이어에 적용합니다.

## **3.3 Position-wise Feed-Forward Networks**

- 포인트-와이즈 완전 연결 네트워크는 두 개의 선형 변환과 ReLU 활성화를 사용합니다. 이는 시퀀스의 각 위치에 동일하게 적용되며, 각 위치의 표현을 수정합니다.

## **3.4 Embeddings and Softmax**

## **3.5 Positional Encoding**

- 입력 토큰 및 출력 토큰은 512차원의 벡터로 임베딩되며, 디코더의 출력은 선형 변환과 소프트맥스 함수로 다음 예측 토큰을 생성합니다. 임베딩 레이어에서는 dmodel의 제곱근을 곱하여 사용합니다.
- Transformer는 반복과 합성곱을 사용하지 않기 때문에 시퀀스의 순서를 고려하기 위해 위치 인코딩을 추가로 사용합니다. 위치 인코딩은 사인과 코사인 함수를 기반으로 하며, 이는 모델이 상대적 위치를 학습하는 데 유리합니다.

# Results

> Transformer 모델의 성능은 WMT 2014 영어-독일어 번역 작업과 WMT 2014 영어-프랑스어 번역 작업에서 평가되었습니다. 큰 Transformer 모델(Transformer (big))은 영어-독일어 번역 작업에서 BLEU 점수 28.4를 기록해 기존에 보고된 최고 모델들을 2.0 이상 능가하며 새로운 최고 기록을 세웠습니다. 이 모델은 8개의 P100 GPU를 사용하여 3.5일 동안 훈련되었습니다. 기본 모델인 Transformer (base) 역시 기존에 발표된 모든 모델과 앙상블을 능가하며, 다른 경쟁 모델에 비해 훈련 비용이 훨씬 낮았습니다.
> 

영어-프랑스어 번역 작업에서는 Transformer (big) 모델이 BLEU 점수 41.0을 기록하여 이전에 발표된 단일 모델을 모두 능가했습니다. 이 성과는 이전 최고 기록 모델의 1/4 미만의 훈련 비용으로 달성되었습니다. 이 큰 모델은 드롭아웃 비율 0.3을 사용했습니다.

Transformer 모델들은 개발 셋에서 선택한 하이퍼파라미터를 사용해 평가되었으며, 최종 출력 길이는 입력 길이 + 50으로 설정되어 조기 종료가 가능하게 되었습니다.

다음 섹션을 요약하고 싶으시면, 섹션 번호를 알려주세요.

# Discussion

이 논문에서는 자기 어텐션 레이어를 순환 신경망(RNN) 및 합성곱 레이어와 비교하고 있습니다. 자기 어텐션 레이어는 상수 수의 순차적으로 실행된 연산으로 모든 위치를 연결하는 반면, 순환 레이어는 O(n) 순차적 연산이 필요합니다. 계산 복잡성 측면에서 자기 어텐션 레이어는 시퀀스 길이 n이 표현 차원 d보다 작을 때 순환 레이어보다 빠릅니다. 이것은 언어 모델링에서 사용하는 시퀀스의 대표적 특징을 고려할 때 자주 발생합니다.

자기 어텐션을 활용하면 더 많은 병렬화가 가능하고, 장거리 종속성에 대한 경로 길이도 짧게 유지됩니다. 따라서 자기 어텐션은 장거리 종속성을 학습하는 데 유리합니다. 반면, 단일 컨볼루션 레이어는 모든 위치 쌍을 연결하지 않으며, 이 때문에 여러 컨볼루션 레이어를 스택해야만 모든 위치 쌍을 연결할 수 있습니다.

자기 어텐션의 또 다른 이점은 모델의 해석 가능성이 향상될 수 있다는 점입니다. 논문에서는 자기 어텐션을 활용한 다양한 모델의 어텐션 분포를 분석하며, 개별 어텐션 헤드가 각기 다른 작업을 수행하고 구문 및 의미 구조와 관련된 행동을 보이는 것을 확인했습니다.

다음 섹션을 요약하고 싶으시면, 섹션 번호를 알려주세요.

# Conclusion

이 논문은 주목의 개념만을 활용하는 Transformer라는 새로운 시퀀스 변환 모델을 소개합니다. Transformer는 인코더-디코더 구조에서 자주 사용되는 순환 레이어를 멀티 헤드 자기 어텐션으로 대체했습니다. 번역 작업에 있어서 Transformer는 순환 레이어 또는 합성곱 레이어 기반 아키텍처보다 훈련 속도가 상당히 빠르며, 새로운 최고 수준의 번역 품질을 달성했습니다.

Transformer는 텍스트 이외의 입력 및 출력 양식을 포함하는 문제에도 적용할 수 있는 가능성을 가지고 있으며, 큰 입력 및 출력을 효율적으로 처리하기 위해 국지적, 제한된 어텐션 메커니즘을 탐구할 계획입니다. 또한, 생성 과정을 덜 순차적으로 만드는 것도 연구 목표 중 하나입니다.