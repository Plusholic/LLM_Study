{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "# setup prompts - specific to StableLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "# taken from https://huggingface.co/Writer/camel-5b-hf\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index import ServiceContext, set_global_service_context\n",
    "\n",
    "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "# optionally set a global service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0,\n",
    "             api_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# documents = SimpleDirectoryReader('C:/Users/user/Desktop/업무/경상업무/녹음파일/베타테스트_인터뷰/인터뷰STT/').load_data()\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"/Users/jeonjunhwi/문서/Projects/GNN_Covid/refference/GNN논문/A GNN RNN Approach for Harnessing Geospatial and Temporal Information Application to Crop Yeild Prediction.pdf\"]\n",
    ").load_data()\n",
    "    \n",
    "service_context = ServiceContext.from_defaults(chunk_size=256, llm=llm)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context\n",
    ")\n",
    "index.storage_context.persist(persist_dir=\"./storage/march\")\n",
    "\n",
    "query_engine = index.as_query_engine(service_context=service_context,\n",
    "                                     similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'VectorStoreIndex' has no attribute 'from_persist_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# load index from disk\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_persist_dir\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./storage/march/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m      9\u001b[0m     vector_store\u001b[38;5;241m=\u001b[39mvector_store,\n\u001b[1;32m     10\u001b[0m     persist_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./storage/march/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m index \u001b[38;5;241m=\u001b[39m load_index_from_storage(storage_context\u001b[38;5;241m=\u001b[39mstorage_context)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'VectorStoreIndex' has no attribute 'from_persist_dir'"
     ]
    }
   ],
   "source": [
    "from llama_index import StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "# dimensions of text-ada-embedding-002\n",
    "d = 384\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, storage_context=storage_context\n",
    ")\n",
    "index.storage_context.persist(persist_dir=\"../Retrieval_Database/test_pdf_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"../Retrieval_Database/test_pdf_faiss\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    persist_dir=\"../Retrieval_Database/test_pdf_faiss\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metadata length (70) is longer than chunk size (64). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_node \u001b[38;5;129;01min\u001b[39;00m base_nodes:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m sub_node_parsers:\n\u001b[0;32m---> 15\u001b[0m         sub_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbase_node\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         sub_inodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m             IndexNode\u001b[38;5;241m.\u001b[39mfrom_text_node(sn, base_node\u001b[38;5;241m.\u001b[39mnode_id) \u001b[38;5;28;01mfor\u001b[39;00m sn \u001b[38;5;129;01min\u001b[39;00m sub_nodes\n\u001b[1;32m     18\u001b[0m         ]\n\u001b[1;32m     19\u001b[0m         all_nodes\u001b[38;5;241m.\u001b[39mextend(sub_inodes)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/node_parser/interface.py:61\u001b[0m, in \u001b[0;36mNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m doc_id_to_document \u001b[39m=\u001b[39m {doc\u001b[39m.\u001b[39mid_: doc \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents}\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m     59\u001b[0m     CBEventType\u001b[39m.\u001b[39mNODE_PARSING, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mDOCUMENTS: documents}\n\u001b[1;32m     60\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m---> 61\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_nodes(documents, show_progress\u001b[39m=\u001b[39;49mshow_progress, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m i, node \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(nodes):\n\u001b[1;32m     64\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m             node\u001b[39m.\u001b[39mref_doc_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m             \u001b[39mand\u001b[39;00m node\u001b[39m.\u001b[39mref_doc_id \u001b[39min\u001b[39;00m doc_id_to_document\n\u001b[1;32m     67\u001b[0m         ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/node_parser/interface.py:163\u001b[0m, in \u001b[0;36mMetadataAwareTextSplitter._parse_nodes\u001b[0;34m(self, nodes, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m nodes_with_progress:\n\u001b[1;32m    162\u001b[0m     metadata_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_metadata_str(node)\n\u001b[0;32m--> 163\u001b[0m     splits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_text_metadata_aware(\n\u001b[1;32m    164\u001b[0m         node\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mNONE),\n\u001b[1;32m    165\u001b[0m         metadata_str\u001b[39m=\u001b[39;49mmetadata_str,\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m     all_nodes\u001b[39m.\u001b[39mextend(build_nodes_from_splits(splits, node))\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m all_nodes\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/node_parser/text/sentence.py:147\u001b[0m, in \u001b[0;36mSentenceSplitter.split_text_metadata_aware\u001b[0;34m(self, text, metadata_str)\u001b[0m\n\u001b[1;32m    145\u001b[0m effective_chunk_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size \u001b[39m-\u001b[39m metadata_len\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m effective_chunk_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMetadata length (\u001b[39m\u001b[39m{\u001b[39;00mmetadata_len\u001b[39m}\u001b[39;00m\u001b[39m) is longer than chunk size \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size\u001b[39m}\u001b[39;00m\u001b[39m). Consider increasing the chunk size or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdecreasing the size of your metadata to avoid this.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m \u001b[39melif\u001b[39;00m effective_chunk_size \u001b[39m<\u001b[39m \u001b[39m50\u001b[39m:\n\u001b[1;32m    153\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    154\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMetadata length (\u001b[39m\u001b[39m{\u001b[39;00mmetadata_len\u001b[39m}\u001b[39;00m\u001b[39m) is close to chunk size \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size\u001b[39m}\u001b[39;00m\u001b[39m). Resulting chunks are less than 50 tokens. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Metadata length (70) is longer than chunk size (64). Consider increasing the chunk size or decreasing the size of your metadata to avoid this."
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import IndexNode\n",
    "\n",
    "node_parser=SimpleNodeParser.from_defaults(chunk_size=256)\n",
    "base_nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "sub_chunk_sizes = [64, 128]\n",
    "sub_node_parsers = [\n",
    "    SimpleNodeParser.from_defaults(chunk_size=c, chunk_overlap=0) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "len(all_nodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_chunk = VectorStoreIndex(\n",
    "    all_nodes, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "# docs = [Document(text=d.get_content(), metadata=d.metadata.pop()) for d in docs]\n",
    "docs = []\n",
    "for d in all_nodes:\n",
    "    metadatas_={}\n",
    "    metadatas_['page_label'] = d.metadata['page_label']\n",
    "    metadatas_['file_path'] = d.metadata['file_path']\n",
    "    docs.append(Document(text=d.get_content(), metadata=metadatas_))#, id=d.id_, index_id=d.index_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "# dimensions of text-ada-embedding-002\n",
    "d = 384\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, storage_context=storage_context\n",
    ")\n",
    "index.storage_context.persist(persist_dir=\"../Retrieval_Database/test_pdf_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever_chunk = index.as_retriever(similarity_top_k=10)\n",
    "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=10)\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"../Retrieval_Database/test_pdf_faiss\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    persist_dir=\"../Retrieval_Database/test_pdf_faiss\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query('what is dropedge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DropEdge is not mentioned in the provided context information.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# small to big retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "from llama_index.indices.vector_store.retrievers.retriever import (\n",
    "    VectorIndexRetriever,\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
    "retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c090ab70-2815-40ff-a52f-7cb34361fd4d<br>**Similarity:** 3.7285354557593466<br>**Text:** Soil Survey Staff. 2020. Gridded Soil Survey Geographic\n",
       "(gSSURGO) Database for the Conterminous U...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** fb0ed3ff-66b4-451e-81c1-60e626698b98<br>**Similarity:** 2.976186845261263<br>**Text:** yield prediction. Computers and Electronics in Agriculture,\n",
       "127: 467–474.\n",
       "Nevavuori, P.; Narra, N...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "# will retrieve context from specific companies\n",
    "nodes = retriever.retrieve(\"What happened at Viaweb and Interleaf?\")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 35125356-2ad1-41d1-b7de-e1e60a6a2741<br>**Similarity:** 9.748103849921955<br>**Text:** Figure 1:\n",
       "Left: The CNN model used for per-year embedding extraction. Right: Our overall GNN-RNN ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** fb0ed3ff-66b4-451e-81c1-60e626698b98<br>**Similarity:** 3.9001567486205326<br>**Text:** yield prediction. Computers and Electronics in Agriculture,\n",
       "127: 467–474.\n",
       "Nevavuori, P.; Narra, N...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"What is Graph Neural Network?\")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node id 커스터마이징 할 수 있어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43madding_data_to_GPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36madding_data_to_GPT\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(persist_dir):\n\u001b[0;32m---> 21\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mread_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     index \u001b[38;5;241m=\u001b[39m build_storage(data_dir, persist_dir)\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mread_from_storage\u001b[0;34m(persist_dir)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_from_storage\u001b[39m(persist_dir):\n\u001b[0;32m---> 13\u001b[0m     storage_context \u001b[38;5;241m=\u001b[39m \u001b[43mStorageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_index_from_storage(storage_context)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/storage/storage_context.py:88\u001b[0m, in \u001b[0;36mStorageContext.from_defaults\u001b[0;34m(cls, docstore, index_store, vector_store, vector_stores, graph_store, persist_dir, fs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         vector_stores \u001b[39m=\u001b[39m vector_stores \u001b[39mor\u001b[39;00m {\n\u001b[1;32m     85\u001b[0m             DEFAULT_VECTOR_STORE: SimpleVectorStore()\n\u001b[1;32m     86\u001b[0m         }\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     docstore \u001b[39m=\u001b[39m docstore \u001b[39mor\u001b[39;00m SimpleDocumentStore\u001b[39m.\u001b[39;49mfrom_persist_dir(\n\u001b[1;32m     89\u001b[0m         persist_dir, fs\u001b[39m=\u001b[39;49mfs\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     index_store \u001b[39m=\u001b[39m index_store \u001b[39mor\u001b[39;00m SimpleIndexStore\u001b[39m.\u001b[39mfrom_persist_dir(\n\u001b[1;32m     92\u001b[0m         persist_dir, fs\u001b[39m=\u001b[39mfs\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m     graph_store \u001b[39m=\u001b[39m graph_store \u001b[39mor\u001b[39;00m SimpleGraphStore\u001b[39m.\u001b[39mfrom_persist_dir(\n\u001b[1;32m     95\u001b[0m         persist_dir, fs\u001b[39m=\u001b[39mfs\n\u001b[1;32m     96\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/storage/docstore/simple_docstore.py:56\u001b[0m, in \u001b[0;36mSimpleDocumentStore.from_persist_dir\u001b[0;34m(cls, persist_dir, namespace, fs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     persist_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(persist_dir, DEFAULT_PERSIST_FNAME)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_persist_path(persist_path, namespace\u001b[39m=\u001b[39;49mnamespace, fs\u001b[39m=\u001b[39;49mfs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/storage/docstore/simple_docstore.py:73\u001b[0m, in \u001b[0;36mSimpleDocumentStore.from_persist_path\u001b[0;34m(cls, persist_path, namespace, fs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_persist_path\u001b[39m(\n\u001b[1;32m     60\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     fs: Optional[fsspec\u001b[39m.\u001b[39mAbstractFileSystem] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSimpleDocumentStore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     65\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a SimpleDocumentStore from a persist path.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     simple_kvstore \u001b[39m=\u001b[39m SimpleKVStore\u001b[39m.\u001b[39;49mfrom_persist_path(persist_path, fs\u001b[39m=\u001b[39;49mfs)\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(simple_kvstore, namespace)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/site-packages/llama_index/storage/kvstore/simple_kvstore.py:76\u001b[0m, in \u001b[0;36mSimpleKVStore.from_persist_path\u001b[0;34m(cls, persist_path, fs)\u001b[0m\n\u001b[1;32m     74\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mpersist_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39mopen(persist_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 76\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # It must be before llama_index import\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "\n",
    "def build_storage(data_dir, persist_dir):\n",
    "    documents = SimpleDirectoryReader(data_dir).load_data()\n",
    "    index = GPTVectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir)\n",
    "    return index\n",
    "\n",
    "def read_from_storage(persist_dir):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    return load_index_from_storage(storage_context)\n",
    "\n",
    "def adding_data_to_GPT():\n",
    "    persist_dir = \"./storage\"\n",
    "    data_dir = \"./data\"\n",
    "    index = None\n",
    "    if os.path.exists(persist_dir):\n",
    "        index = read_from_storage(persist_dir)\n",
    "    else:\n",
    "        index = build_storage(data_dir, persist_dir)\n",
    "        query_engine = index.as_query_engine()\n",
    "\n",
    "    response = query_engine.query(\n",
    "        \"When did Ran Bar-Zik create his first pull request in CyberArk?\"\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    adding_data_to_GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('LLM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a3a50cfa768e55079b834244adbc23d62a4b24bbd2f8b443e30bd8c79d3f7ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
